# 面试-OS

## 操作系统内核

### 什么是内核？

内核就是将与硬件紧密相关的模块等，安排在紧靠硬件的软件层中，将其常驻内存，即是称为OS内核



### 内核的功能

- 基本功能	

- - 中断处理

    此功能是内核最基本的功能，是整个操作系统活动的基础。OS中许多重要的活动无不依赖于中断，比如系统调用、IO操作、进程调度、设备驱动等

  - 时钟管理

- - 原语操作：原语由若干指令组成，执行过程中不允许被中断

- 资源管理功能

- - 进程管理

    进程的调度与分派、创建与撤销等

  - 存储器管理

    如将逻辑地址转换为物理地址、内存的分配与回收等

- - 设备管理

    各类设备的驱动程序等



### 内核态与用户态

#### 划分目的

对操作系统进行保护，防止其遭受其它应用程序的破坏，提高OS的运行效率



#### 具体内容

- 内核态与用户态是操作系统的两种运行级别

- 大部分用户直接面对的程序都是运行在用户态，运行在用户态下的程序是不能直接访问操作系统内核数据结构和程序的，当程序执行到用户态没有权限做的工作时就会切换到内核态
- 处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的
- 处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的



#### 转换原因

1. **系统调用**：这是用户态进程**主动**要求转换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序来完成工作

   > 系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。
   >
   > 用户程序通常调用库函数，由库函数再调用系统调用，因此有的库函数会使用户程序进入内核态（只要库函数中某处调用了系统调用），有的则不会。

2. **异常**：当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。

3. **外围设备的中断**：当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序。如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。



#### 用户级线程ULT

1. 用户空间运行线程库，任何应用程序都可以通过使用线程库来实现多线程。

   > 线程库是用于用户级线程管理的一个例程包，它提供多线程应用程序的开发和运行的支撑环境，包含：用于创建和销毁线程的代码、在线程间传递数据和消息的代码、调度线程执行的代码以及保存和恢复线程上下文的代码

2. 所以线程的创建、消息传递、线程调度、保存/恢复上下文这些操作都由线程库来完成。操作系统感知不到用户级线程的存在，也就是**用户级线程对操作系统来说是透明的**，内核以进程为调度单位。

3. 特点：

   - 用户级线程切换不需要转换成内核态，大大节省了系统开销

   - 允许进程按照特定的需要选择不同的调度算法来调度线程

   - 由于其不需要内核支持，所以可以跨OS运行

   - 不能利用多核处理器的优势，操作系统调度的是进程，每个进程仅有一个用户级线程能执行

   - 用户级多线程对于处理逻辑并行性问题有很好的效果。不擅长于解决物理并行问题。

   - 与内核协作成本高，当需要进行I/O的时候，需要进行用户态到内核态的切换

   - 一个用户级线程阻塞，将导致整个进程的阻塞（因为操作系统并不能发现用户级线程阻塞而更换执行其他线程）

     > jacketing技术可以解决这个问题，jacketing的目标是把一个产生阻塞的系统调用转化成一个非阻塞的系统调用。例如,当进程中的一个线程调用IO中断前，先调用一个应用级的I/O jacket例程，而不是直接调用一个系统I/O。让这个jacket例程检查并确定I/O设备是否忙。如果忙，则jacketing将控制权交给该进程的线程调度程序，决定该线程进入阻塞状态并将控制权传送给另一个线程（若无就绪态线程就可能执行进程切换）。
     >
     > 这种在系统调用周围从事检查的这类代码称为包装器（jacket 或 wrapper）。这种处理方法需要重写部分系统调用库，所以效率不高也不优雅，不过没有其他的可选方案了。



#### 内核级线程KLT

1. 内核级线程的所有工作（创建、撤销、切换等）都由操作系统内核完成
2. 操作系统内核提供一个应用程序设计接口API，供开发者使用内核级线程
3. 特点
   - 创建成本、切换成本高：创建和切换的时候都需要切换到内核态
   - 进程中的一个线程阻塞，内核能调度同一进程的其他就绪态线程占有处理器运行
   - 多处理器环境中，内核能同时调度同一进程的多个线程，并把这些线程映射到不同的处理器核心上，实现并行，从而提高进程的执行效率
   - 应用程序线程在用户态执行时，线程调度和管理在内核实现。线程调度时，控制权从一个线程改变到另一个线程，需要进行模式切换，系统开销较大
   - 内核级多线程适用于解决物理并行性问题。



#### 线程实现的组合策略

由操作系统内核支持内核级多线程，由操作系统的程序库来支持用户级多线程，线程创建完全在用户空间创建，线程的调度也在应用程序内部进行，然后把用户级多线程映射到（或者说是绑定到）一些内核级多线程。**编程人员可以针对不同的应用特点调节内核级线程的数目来达到物理并行性和逻辑并行性的最佳方案。**

##### 多对一

多个用户线程对应到同一个内核线程上，线程的创建、调度、同步的所有细节全部由进程的用户空间线程库来处理。这样，极大地减少了创建内核态线程的成本，但是线程不可以并行。因此，这种模型现在基本上用的很少。

优点：

- 用户线程的很多操作对内核来说都是透明的，不需要用户态和内核态的频繁切换。使线程的创建、调度、同步等非常快。

缺点：

- 由于多个用户线程对应到同一个内核线程，如果其中一个用户线程阻塞，那么该其他用户线程也无法执行。
- 内核并不知道用户态有哪些线程，无法像内核线程一样实现较完整的调度、优先级等



##### 一对一

该模型为每个用户态的线程分配一个单独的内核态线程，在这种情况下，每个用户态都需要通过系统调用创建一个绑定的内核线程，并附加在上面执行。 这种模型允许所有线程并行执行，能够充分利用多核优势。

> 目前 Linux 中的线程、OpenJDK Java 线程等采用的都是一对一线程模型。每一个JVM线程，都有一个对应的内核线程。

优点：

- 实现起来较为简单

该模型的缺点是：

- 每创建一个用户线程，相应地就需要创建一个内核线程，开销较大，因此需要限制整个系统的线程数量。
- 对用户线程的大部分操作都会映射到内核线程上，引起用户态和内核态的频繁切换。



##### 多对多

这种模式下会为 n 个用户态线程分配 m 个内核态线程。m 通常小于 n。一种可行的策略是将 m 设置为核数。这种多对多的关系，减少了内核线程，同时也保证了多核心并行。多对多模型中线程的调度需要由内核态和用户态一起来实现，例如线程间同步需要用户态和内核态共同实现。用户态和内核态的分工合作导致实现该模型非常复杂。

> Linux多线程模型曾经也想使用该模型，但它太复杂，要对内核进行大范围改动，所以还是采用了一对一的模型！

该模型的缺点是：

- 实现起来非常复杂



## 进程

### 进程实体组成

- ==程序段==：存放程序代码
- ==数据段==：存放程序运行时使用、产生的运算数据。如全局变量、局部变量、宏定义的常量就存放在数据段内
- ==进程控制块PCB==：存放操作系统对进程实体进行管理所需的各种信息
  - 作为独立运行基本单位的标志，PCB是进程存在于系统中的唯一标志
  - 能实现间断性运行方式，当进程被中断时，会把CPU的现场信息存储在PCB中，便于进程再次被调度时可以恢复现场
  - 提供进程管理所需要的信息
  - 实现与其他进程的同步和通信


> 进程运行即进程实体的运行，创建进程就是创建进程实体中的PCB，撤销进程就是撤销进程实体中的PCB



### PCB组织方式

- ==线性组织方式==

  即PCB存放在一个数组中，使用时到数组中查询

  - 查询快，增删难

- ==链式组织方式==

  按照进程状态把PCB分到多个队列中，同一个队列中的PCB用指针相连，操作系统持有指向各个队列的指针

  - 查询慢，增删快，可容纳大量PCB

- ==索引组织方式==

  根据进程状态的不同，建立几张索引表，索引表中有指向PCB的指针，操作系统持有指向索引表的指针



### 进程的特征

- ==动态性==：程序是静态的，进程是动态的
- ==并发性==：多个进程实体同存于内存中，并且在同一段时间内并发运行
- ==独立性==：进程实体是能独立运行、获得资源、接受调度的基本单位
- ==异步性==：进程按照各自独立的不可预知的速度推进，也因此导致了结果的不可再现性



### 进程状态

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gynp8r5a3qj312w0igabm.jpg" alt="image-20220123164517242" style="zoom: 33%;" />

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gynpap9qgnj30gw0d8t9u.jpg" alt="image-20220123164712259" style="zoom:50%;" />

<img src="https://img-blog.csdnimg.cn/20200516190118336.png?#pic_center" alt="在这里插入图片描述" style="zoom:75%;" />

#### 创建状态

为一个新进程创建PCB，把该进程**转入到就绪状态**并**插入到就绪队列之中**。

引起创建的事件：

- 用户登录
- 作业调度
- 提供服务
- 应用请求

创建过程：

1. 申请空白PCB
2. 分配运行所需的物理和逻辑事件，如内存、文件、I/O设备、CPU事件
3. 初始化PCB
4. 如果进程就绪队列可以接受新进程，就把新进程插入就绪队列



#### 就绪状态

当进程已分配到除CPU以外的所有必要资源后，只要再获得CPU，便可立即执行

- 活动就绪：等于三态图中的就绪状态
- 静止就绪/就绪挂起：在内存中



#### 运行状态

获得CPU的使用权，单处理机系统同一时间只能有一个进程处于运行状态



#### 阻塞状态

正在执行的进程由于发生某事件而暂时无法继续执行时，便放弃CPU而处于暂停状态，亦即进程的执行受到阻塞，把这种暂停状态称为阻塞状态，有时也称为等待状态或封锁状态。比如进程当中调用wait()函数，会使得进程进入到阻塞状态。

- 活动阻塞：等于三态图中的阻塞状态
- 静止阻塞/阻塞挂起：在外存中

引起阻塞的事件：

- 向系统请求共享资源失败
- 等待某种操作的执行
- 新数据尚未到达：相互合作的进程可能需要另一个进程提供数据
- 等待新任务到达：某些进程会在完成任务后将自己阻塞，等待新任务到来



#### 终止状态

等待操作系统进行善后处理，然后将其PCB清零，并将PCB空间返还系统

引起进程终止的事件：

- 正常终止：进程任务完成

- 异常结束：进程运行过程发生异常，使得程序无法继续进行

- 外界干预：进程应外界请求而终止

  - 操作员或OS干预
  - 父进程请求终止子进程
  - 因父进程终止而导致其所有子进程终止

  > UNIX中，创建进程的进程称为父进程，其创建的子进程继承父进程拥有的资源，如文件、缓冲区等，父进程与子进程共同组成进程家族
  >
  > Windows中不存在父进程和子进程，创建进程的进程和被创建的进程之间是获得句柄与否、控制与被控制的关系

终止过程：

1. 根据进程标识符，找到对应PCB，读出进程状态
2. 若进程处于执行状态，就立即终止进程的执行，设置调度标志为true，表示进程终止后需要重新进行调度
3. 若进程还有子进程，则需要先把所有的子进程终止
4. 将终止进程拥有的资源归还给父进程或者系统
5. 将终止进程的PCB从所在队列/链表中移出



#### 挂起操作

原本处于运行状态的进程会暂停运行（仍在内存中）；原本处于就绪状态的进程调到外存中，需要被对换到内存中才能被再次调度

引入原因：

1. **终端用户的请求**。当终端用户发现运行中的程序存在问题时，可以立即暂停自己的程序
2. **父进程的请求**。有时父进程希望挂起自己的子进程，以便考察和修改子进程，或者协调各个子进程之间的活动
3. **负荷调节的需要**。当实时系统中工作负荷比较重时，可以选择挂起一些不重要的进程来减轻系统负担，以保证系统能正常运行
4. **操作系统的需要**。操作系统有时希望挂起某些进程，以便检查运行中的资源使用情况或者进行记录

状态变换：

- 活动就绪 ➡️ 静止就绪：通常，操作系统倾向于挂起阻塞态进程而不是就绪态进程，因为就绪态进程可以立即执行，但是如果把所有阻塞进程全部挂起都无法得到足够的内存空间，就会挂起就绪进程，从而引起这种状态变换
- 活动阻塞 ➡️ 静止阻塞：当内存空间比较紧缺时，把一些阻塞（活动阻塞）的进程挂起，即调出到外存（转换为静止阻塞）
- 运行 ➡️ 静止就绪：静止就绪的进程激活后进入活动就绪状态，等待CPU调度

> 激活/释放操作与挂起相对应



#### 激活操作

状态变换：

- 静止就绪 ➡️ 活动就绪：如果内存中已经没有活动就绪进程了，就会选择激活一个静止就绪进程以执行。此外，当处于就绪/挂起状态的进程比处于就绪态的任何进程的优先级都要高时，也可以进行这种转换。这种情况的产生是由于操作系统设计者规定，**调入高优先级的进程比减少交换量更重要**。
- 静止阻塞 ➡️ 活动阻塞



### 挂起与阻塞的区别

1. 挂起是一种主动行为，因此也需要主动进行恢复（激活）；而阻塞时一种被动行为，是在等待事件或资源时任务的表现，不知道什么时候会被阻塞，也不知道什么时候能恢复
2. 挂起（suspend）不会释放CPU，如果任务优先级高就永远轮不到其他任务运行，一般挂起用于程序调试中的条件中断（当出现某个条件的情况时就挂起，然后进行单步调试）；阻塞（pend）会释放CPU，其他任务可以运行
3. 任务调度时，直接忽略挂起状态的任务；任务调度器会关注阻塞状态的任务，当阻塞任务等待的资源就绪后，就会转变为就绪状态，等待CPU



### Java线程什么时候让出CPU？

1. ==Thread.sleep()==  

   sleep就是使得当前正在执行的线程主动让出CPU，在sleep指定的时间过后，CPU才会重新回到这个线程上继续往下执行。

   注意：如果当前线程进入了同步代码块，**sleep方法并不会释放锁**，即当前线程一直持有锁，即使该调用sleep方法的线程让出了CPU，其他被因为无法获取到同步锁的线程也仍然会阻塞，无法得到执行

   > 所以强烈不建议在同步代码块中使用sleep方法。因为amdahl定律：在多线程争用的情况下，持有锁的线程进行一些耗时操作，会极大的降低吞吐量

2. ==Thread.yield()==

   yield是个native静态方法，这个方法是把CPU释放，然后和其他线程一起竞争，是个基本不会用到的方法

3. ==Object.wait() / condition.wait()==

   wait方法会把当前的锁释放掉，并阻塞当前线程；当别的线程调用该Object.notify()或者notifyAll()方法之后，该线程有可能得到CPU，重新获得锁

   > join()方法也会让出CPU，也是因为内部调用了wait()方法

4. ==Thread.stop()==

   立即停止run()方法中剩余的全部工作（包括catch块和finally块中的），并抛出ThreadDeath异常（通常情况下该异常不需要显式捕获），并且同时释放所有锁。可能会引发数据不一致、资源没有释放等问题，不推荐使用

5. ==Thread.currentThread().suspend()==

   挂起当前线程，不会释放锁，直到该线程执行resume()方法后，被挂起的线程才能继续执行。容易引发死锁，同样不推荐使用

   > 如果一个目标线程持有锁后挂起了，另一个线程需要获得锁后才resume目标线程，这样就会引发死锁

6. ==LockSupport.park()==

   与suspend一样挂起线程，但是不会引发死锁问题



### 进程同步

#### 基本概念

- 进程同步机制的作用：使并发执行的进程之间按照一定的时序共享系统资源，从而使程序执行具有可再现性
- 临界资源：如打印机等硬件资源，都属于临界资源。进程访问临界资源需要采取互斥方式

- 临界区：进程中访问临界资源的代码即为临界区
- 同步机制需要遵循的规则

- - **空闲让进**
  - **忙则等待**

- - **有限等待**：要保证要求访问临界资源的进程在有限时间内能进入自己的临界区，避免“死等”
  - **让权等待**：进程不能进入临界区时，需要立即释放处理机，避免“忙等”



#### 硬件同步机制

- 可以将进入临界区的标志看成一个锁
- 需要进入临界区的进程，执行以下步骤：

1. 1. 进行锁测试，判断锁开还是关闭
   2. 锁关闭时，需要等待，直到锁打开

1. 3. 锁打开时，允许当前进程进入临界区并将锁关闭，防止其他进程进入临界区

> 为了防止多个进程测试到锁打开，必须保证锁测试和关锁操作是连续的（原子）



#### 关中断

- 做法：进入锁测试前关闭中断，锁测试结束并上锁后再打开中断
- 结果：进程临界区执行期间，CPU不响应中断，不会引发调度，即不会发生进程或线程切换

- 缺点：

- - 滥用关中断可能导致严重后果
  - 关中断时间过长会限制处理器交叉执行程序的能力，影响系统效率

- - 不适用于多CPU，因为在一个处理器关中断无法避免进程在其他处理器上执行相同的临界区代码



#### Test-and-Set指令

- 一条硬件指令TS
- 做法：尝试设置锁状态为true，并返回原来的锁状态。进程判断原来的锁状态是否为false，是才能进入临界区

```c
boolean TS(boolean *lock){
    boolean old;
    old = *lock;
    *lock = true;
    return old;
}

//进程执行以下
while TS(&lock);
 //临界区代码
lock = false;
```



#### Swap指令

- 类似TS指令
- 缺点：临界资源忙碌时，其他访问进程会一直进行测试，处于“忙等”状态，不符合“让权等待”



#### 临界区

- 通过对多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问
- 优点：保证在某一时刻只能有一个线程访问数据的简便方法
- 缺点：虽然临界区同步速度很快，但却只能用来同步本进程内的线程，而不能用来同步多个进程中的线程



#### 互斥量

- 为协调共同对一个共享资源的单独访问而设计的，互斥对象只有一个，只有拥有互斥对象的线程才具有访问资源的权限
- 优点：使用互斥不仅能够在同一应用程序不同线程中实现资源的安全共享，而且可以在不同应用程序的线程之间实现对资源的安全共享
- 缺点：
  - 互斥量是可以命名的，可以跨越进程使用。所以创建互斥量需要的资源更多，所以如果只是为了在进程内部同步使用的话使用临界区更好，速度更快而且能减少资源占用量
  - 通过互斥量可以指定资源被独占的方式使用，但是无法处理互斥对象大于1的情况。比如现在缓冲池中有三个货品，多线程到缓冲池中取货品，只有缓冲池中没有货品时才无法取



#### 信号量

- 上面说的互斥量其实是信号量的一种特殊情况，当信号量的最大资源数=1就是互斥量
- 优点：适用于对Socket程序中线程的同步
- 缺点：
  - 信号量机制必须有公共内存，不能用于分布式操作系统
  - 信号量的读写和维护十分困难
  - 核心操作PV操作分散在各用户程序的代码中，不易控制和管理，一旦出现错误，后果严重，并且不易发现和纠正



#### 事件

- 用于通知线程事件已发生，从而启动后续任务的开始
- 优点：事件对象通过通知操作的方式来保持线程的同步，并且可以实现不同进程中线程同步操作



### 进程通信方式（IPC）

#### 管道

- 匿名管道：是一种**半双工**的通信方式，**数据只能单向流动**，而且**只能在具有亲缘关系的进程之间使用**（父子进程关系）
- 命名管道（FIFO）：是一种**半双工**的通信方式，**数据能双向流动**，**可以实现任意两个进程之间的通信**

> 管道实际上是一个用于连接读写进程的共享文件，通过一个内核缓冲区实现数据传输



调用pipe函数时会在内核中开辟一块缓冲区（即管道）用于通信，它有一个读端有一个写端，然后通过filedes参数传出给用户程序两个文件描述 符，filedes[0]指向管道的读端，filedes[1]指向管道的写端

1. 父进程调用pipe开辟管道，得到两个文件描述符指向同一管道

2. 父进程调用fork函数复制出子进程，那么子进程也有两个文件描述符指向同一管道

3. 父进程关闭管道读端，子进程关闭管道写端。父进程可以往管道里写，子进程可以从管道里读，管道是用环形队列实现的，数据从写端流入从读端流出，这样就实现了进程间通信。

   > 在linux的pipe管道下，在写端进行写数据时，不需要关闭读端的缓冲文件(即不需要读端的文件描述符计数为0)，但是在读端进行读数据时必须先关闭写端的缓冲文件(即写端的文件描述符计数为0)然后才能读取数据。



#### 信号

是一种比较复杂的通信方式，可以在任何时候发给某一进程，而无需知道该进程的状态

> Linux中常用信号： 
>
> 1. SIGHUP：用户从终端注销，所有已启动进程都将收到该进程。系统缺省状态下对该信号的处理是终止进程。
> 2. **SIGINT：程序终止信号。程序运行过程中，按 Ctrl+C 键将产生该信号。**
> 3. **SIGQUIT：程序退出信号。程序运行过程中，按 Ctrl+\\ 键将产生该信号。**
> 4. SIGBUS SIGSEGV：进程访问非法地址。
> 5. SIGFPE：运算中出现致命错误，如除零操作、数据溢出等。
> 6. **SIGKILL：用户终止进程执行信号。shell下执行kill -9发送该信号。**进程一定会被杀死，但是可能会造成没有释放资源等问题
> 7. **SIGTERM：结束进程信号。shell下执行kill 进程pid发送该信号。**进程不一定能被杀死，会先释放资源
> 8. **SIGALRM：定时器超时信号。**
> 9. **SIGCLD：子进程退出信号。如果其父进程没有忽略该信号也没有处理该信号，则子进程退出后将形成僵尸进程。**

#### 信号量

信号量是一个计数器，可以用来**控制多个进程对共享资源的访问**。它常作为一种**锁机制**，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，**主要作为进程间以及同一进程内不同线程之间的同步手段**。（PV操作）

#### 消息队列

消息队列是消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。

#### 共享内存

共享内存是最快的 IPC （进程间通信）方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。

1. 需要通信的进程在通信前，向系统申请获得共享存储区的一个分区
2. 把申请到的分区映射到自己的地址空间中，进行正常读写
3. 完成后，解除映射
4. 之后其他进程可以映射这个分区到自己的地址空间进行读写，从而实现了进程间的通信



##### Linux中的共享内存

- mmap：mmap系统调用使得进程之间通过映射同一个普通文件实现共享内存。普通文件被映射到进程地址空间后，进程可以像访问普通内存一样对文件进行访问，而不必再调用read/write操作

  > 实际上写的是内存，但是写在这个内存上的内容，过段时间后会被内核写到文件上。

  - 不使用文件映射：读取磁盘到内存中需要经历两次数据复制 ➡️ 磁盘到内核缓冲区，再从内核缓冲区到用户空间。把数据从内存写回到磁盘中同样需要两次数据拷贝，因此修改一次文件就需要拷贝四次数据，开销是非常大的

  - mmap-内存映射文件是操作系统提供的一种机制，目的是减少这种不必要的数据拷贝，从而减少开销。它由mmap将文件直接映射到用户空间，mmap并没有进行数据拷贝，真正的数据拷贝是在缺页中断处理时进行的。由于mmap把文件直接映射到用户空间，所以中断处理函数直接把数据从硬盘拷贝到用户空间，所以只需要进行一次数据拷贝。使用mmap来修改文件总共就只需要拷贝两次数据

    > 零拷贝中的mmap+write是读取数据时把内核缓冲区映射到用户空间，这样子读取数据的时候就无需把内核缓冲区的数据拷贝到用户空间

- 系统V



#### Socket

Socket套接字是用于**网络中不同主机**的通信方式，多用于客户端与服务器之间



#### 优缺点

- 管道：速度慢，容量有限
- 信号：传递信息太少，一般用于通知接收进程某个事件已经发生
- 消息队列：容量受到系统限制，且要注意第一次读的时候，要考虑上一次没有读完数据的问题;
- 信号量：不能传递复杂消息，只能用来同步
- 共享内存：能够很容易控制容量，速度快，但要保持同步，比如一个进程在写的时候，另一个进程要注意读写的问题，相当于线程中的线程安全，当然，共享内存区同样可以用作线程间通讯，不过没这个必要，线程间本来就已经共享了同一进程内的一块内存。
- Socket：任何进程间都能通讯，但速度慢



### 进程调度策略

1. 先来先服务FCS
   - 非抢占式
   - 根据请求顺序进行调度
   - 对I/O密集型进程不利，每次结束I/O之后又要重新排队；对短进程不利，需要长时间等待前面的长进程执行完毕
2. 短作业优先SJF
   - 非抢占式
   - 估计的运行时间最短的作业先调度
   - 不利于长进程，可能会产生饥饿现象
3. 最短剩余时间优先
   - 短作业优先的抢占式版本
   - 如果新到达的进程所需运行时间比当前正在运行的进程的剩余时间短，就调度新到达的进程
4. 时间片轮转RR
   - 所有就绪队列按FCFS原则排成一个队列，逐个调度，每次运行一个时间片的时间
   - 算法效率与时间片大小密切相关，因为进程切换需要保存现场信息并载入新进程的信息
     - 时间片太小，进程切换太频繁，额外开销大
     - 时间片太大，退化成FCS，实时性不能得到保证
5. 优先级调度
   - 为每个进程分配一个优先级，按照优先级进行调度
     - 进程类型：系统进程（如对换进程）的优先级一般高于用户进程
     - 进程对资源的需求：需求少的优先级高
     - 用户要求：根据进程紧迫程度及用户所付费用的多少确定优先级
   - 为了防止低优先级的进程产生饥饿现象，可以随着等待时间提高优先级
5. 多级反馈队列
   - 设置多个就绪队列，每个队列赋予不同的优先级：队列优先级逐个降低，当上一个优先级队列为空时才会调度本队列内的进程
   - 每个队列采用FCFS算法，当某个进程在时间片结束后还没有完成，就将其转入下一个队列的末尾
   - 规定最高优先级队列的时间片略大于大多数人机交互所需的处理时间，保证这种紧急程度较高的进程几乎都能在最高优先级队列内就处理完成，无需转入下一级队列等待再次调度



#### Linux中的进程调度算法

##### O(n)

O(n)调度器是Linux内核在2.4及早期版本中采用的进程调度器，其时间复杂度为O(n)。

维护了两个`queue`：`runqueue`和`expired queue`，这两个`queue`永远保持有序，一个进程用完时间片，就会被插入到`expired queue`；当`runqueue`为空时，就把`runqueue`和`expired queue`进行交换

> 为了保证两个队列是有序的，需要对队列进行扫描寻找插入位置，这一操作的时间复杂度为O(n)

存在问题：

1. 队列所有CPU共享，当对队列进行操作时需要对其进行加锁操作保证可见性和一致性，因此并发度小
2. 需要扫描队列中所有任务来判断最佳插入位置，任务越多，效率越低



##### O(1)

Linux2.6版本开始使用O(1)算法，时间复杂度为O(1)。

O(1)调度器有个非常重要的数据结构就是`prio_array`，是一个用于表示进程动态优先级的数组`queue`，它包括了每一种优先级进程所形成的链表，可分为140个进程优先级（0～139，数值越小优先级越高）。

同时，为了减少多核CPU之间的竞争，每个CPU都维护了一份本地的优先队列。

> O(1)调度算法把140个优先级的前100个（0 ~ 99）作为 实时进程优先级，而后40个（100 ~ 139）作为普通进程优先级
>
> 采用 `bitarray`。它为每种优先级分配一个 `bit`，如果这个优先级队列下面有 `process`，那么就把相应的 bit 置为 1，否则置为 0。这样，问题就简化成寻找一个 `bitarray` 里面最高位是 1 的 `bit`，这基本上是一条 CPU 指令的事

与2.4内核中依次比較每一个进程的优先级不同，因为进程优先级个数是定值，因此查找最佳优先级的时间恒定。它不会像曾经的方法那样受可运行进程数量的影响。

假设确定了优先级。那么选取下一个进程就简单了，仅仅需在优先级数组中相应的链表上选取一个进程就可以。

存在问题：

1. 一个高优先级多线程的应用会比低优先级单线程的应用获得更多的资源，这就会导致一个调度周期内，低优先级的应用可能一直无法响应，直到高优先级应用结束。



##### CFS调度算法（完全公平调度算法）

1. 使用虚拟时间`vruntime`作为进程排序的依据（权值）

   > `vruntime += 实际运行时间 * 1024 / 进程权重`
   >
   > 进程权重是根据任务的nice值进行索引。nice值可以理解为是我们事先为任务分配的优先级。nice值越低优先级越高

2. 使用红黑树来存储要调度的任务队列，虚拟时间越小的节点在红黑树中越靠左，每次调度红黑树中最左的节点。

   > 存储了一个指针指向最左节点，这样调度时就不需要O(nlogn)去遍历红黑树，而是直接O(1)拿到指针指向的节点



### 进程切换的过程

进程的切换，实质上就是被中断运行进程与待运行进程的上下文切换：

1. 切换新的页表，然后使用新的虚拟地址空间
2. 切换内核栈，加入新的内容（PCB控制块、资源相关）
3. 硬件上下文切换



### Linux下创建新进程

- fork
  - fork函数创建的子进程是父进程的复制，得到的父子进程是独立的，具有良好的并发性，但是进程间通信需要专门的机制
  - fork函数调用一次，有两个返回值：对父进程而言返回的是子进程的ID（一个父进程可能有多个子进程，但是没有函数可以使得父进程获取所有子进程ID），对子进程而言返回值是0（子进程可以通过getppid来获取父进程ID）。如果创建失败则返回给父进程-1
  - Linux采用COW写时复制技术来进行优化：即当有多个进程访问相同的资源的时候，它们会获取相同的指针指向相同的资源，只有需要修改资源内容的时候，系统才会复制一份专用副本给那个进程，而其他进程看到的资源仍然保持不变。
- vfork
  - vfork创建的父子进程共享地址空间，而不是复制。因此父子进程的数据是共享的，父子进程间的通信很好解决
  - vfork创建的子进程必须调用exit函数来结束，否则子进程不会结束
  - vfork创建的子进程总会在父进程之前执行
  - vfork函数的出现主要是主要是当年Unix系统没有写时复制技术，所以fork出的子进程即使只是调用exec函数来执行另一个可执行文件，也需要完整的复制父进程的资源，但是实际上完全没有必要复制，这样会造成大量的开销浪费。因此设计了vfork函数来避免这个问题，主要目的就是exec一个新的程序，但是现在有了COW技术，这个函数就渐渐被弃用了



## 线程

### 线程同步方式

#### 操作系统层面

1. 临界区：当多个线程访问一个独占性共享资源时，可以使用临界区对象。拥有临界区的线程可以访问被保护起来的资源或代码段，如果其他线程想访问，则被挂起知道拥有临界区的线程退出临界区为止
2. 事件：允许一个线程处理完任务后主动唤醒另一个线程处理
3. 互斥量：与临界区非常相似，只是其可以命名，因此允许在进程间使用，而临界区只能限制同一进程内的多个线程。互斥量开销更大
4. 信号量：使用计数器来限制可以访问共享资源的线程数目



#### Java层面

Java主要通过加锁的方式实现线程同步，而锁有synchronized和Lock两类。

1. synchronized可以加在三个不同的位置：

   - 加在普通方法上，则锁是当前的实例（this）
   - 加在静态方法上，则锁是当前类的Class对象
   - 加在代码块上，需要在关键字后面的小括号里显式指定一个对象作为锁对象

   > 锁的粒度越大，带来的性能开销也越大

2. synchronized采用`CAS+Mark Word`实现，为了性能的考虑，并通过锁升级机制降低锁的开销。在并发环境中，synchronized会随着多线程竞争的加剧，按照如下步骤逐步升级：无锁、偏向锁、轻量级锁、重量级锁。

3. JDK1.5引入了Lock，支持响应中断、超时机制、以非阻塞方式获取锁、多个条件变量（可以选择阻塞队列）

4. Lock采用CAS+volatile实现，其实现的核心是AQS。AQS是线程同步器，是一个线程同步的基础框架，它基于模板方法模式。在具体的Lock实例中，锁的实现是通过继承AQS来实现的，并且可以根据锁的使用场景，派生出公平锁、不公平锁、读锁、写锁等具体的实现。





## 进程、线程与协程

1. 进程是并发执行的程序在执行过程中分配和管理资源的基本单位，是竞争计算机系统资源的基本单位
2. 线程是进程的一个执行单元，是进程内调度的实体，是比进程更小的独立运行的基本单位，是程序执行的基本单位，也被称为轻量级进程
3. 协程是一种比线程更加轻量级的存在，一个线程可以拥有多个协程。其执行过程类似于不带返回值的函数调用



### 进程与线程的区别

|          |                             进程                             |                             线程                             |
| :------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 地址空间 |                 不同进程之间是独立的地址空间                 |             同一进程的不同线程共享进程的地址空间             |
|   资源   |      进程之间的资源是独立的，能很好的进行资源管理和保护      |         线程共享本进程的资源，不利于资源的管理和保护         |
|  健壮性  | 更加健壮，一个进程崩溃后，在保护模式下不会对其他进程产生影响 |                  一个线程崩溃，整个进程死掉                  |
| 执行过程 | 每个独立的进程有一个程序运行的入口、顺序执行序列和程序入口，执行开销大 | 不能独立运行，必须依存在应用程序中，由应用程序提供多个线程执行控制，执行开销小 |
|   开销   |                    创建、撤销、切换开销大                    | 创建、撤销、切换开销小，速度快（只需要保存和设置少量寄存器内容） |



### 进程与线程的切换流程

1. 虚拟地址空间切换：切换页表以使用新的地址空间，一旦去切换上下文，处理器中所有已经缓存的内存地址一瞬间都作废了

   这一步线程切换不用做，因为同一进程下的所有线程是共享该进程的虚拟地址空间的

   > 这里就是线程切换比进程切换快的主要原因。
   >
   > 虚拟地址空间切换是非常耗费时间的，每个进程都有自己的虚拟地址空间，把虚拟地址空间转换为物理地址需要查找页表，页表查找是一个很慢的过程，因此通常使用Cache来缓存常用的地址映射以加速页表查找，这个Cache就是TLB（快表），之后需要转换成物理地址的时候先到TLB中查找，如果缓存命中了就可以直接拿到对应的地址而不用到主存中查找页表了。
   >
   > 由于每个进程都有自己的虚拟地址空间，那么每个进程都有自己的页表，当进程切换后页表也要切换，页表切换后TLB就失效了，新切换的进程的Cache命中率降低，虚拟地址转换为物理地址的速度就会慢很多，总体上表现出来就是程序运行变慢。

2. 切换内核栈和硬件上下文

   这一步进程切换和线程切换都要做



### 线程与协程的区别

|          |                      线程                      |                             协程                             |
| :------: | :--------------------------------------------: | :----------------------------------------------------------: |
| 占用资源 |            初始单位为1MB,固定不可变            |                初始一般为 2KB，可随需要而增大                |
| 调度所属 |                     OS内核                     |                          程序/用户                           |
| 切换开销 |                       大                       |                              小                              |
| 性能问题 | 资源占用太高，频繁创建销毁会带来严重的性能问题 |              资源占用小,不会带来严重的性能问题               |
| 数据同步 |     需要用锁等机制确保数据的一致性和可见性     | 同一时间其实只有一个协程拥有运行权，相当于单线程。不需要锁机制保障，只需要判断状态 |
| 适用场景 |                    大量计算                    |                           大量并发                           |



## 死锁

### 定义

如果一组进程中对的每个进程都在等待仅由该进程中其他进程才能引发的事件，那么该组进程是死锁的



### 产生的必要条件

- 互斥
- 请求和保持
- 不可抢占
- 循环等待



### 预防死锁

#### 破坏请求和保持条件

1. 所有进程开始运行前，必须一次性申请到运行过程中所需的所有资源才能开始运行
2. 进程可以先申请一部分所需的资源，在运行过程中逐步释放已经分配的且使用完毕的资源，再去申请新的资源

#### 破坏不可抢占条件

1. 当进程申请新资源受阻时，必须释放已经持有的所有资源，之后需要重新执行

   > 如打印机在使用后被抢占，可能会导致之前的工作失效

#### 破坏循环等待条件

1. 为每个资源进行线性排序，进程申请多个资源，必须从小到大申请

2. 如果进程本身持有序号大的资源，想申请序号小的资源，就必须先释放序号大的资源才能继续申请

   > 缺点：
   >
   > - 进程实际申请资源的顺序是不一定的，资源怎么排序难以决定
   > - 序号是固定的，想要为系统进行扩展添加新的设备就变得十分困难



### 避免死锁

银行家算法：新进程进入系统必须申明运行过程中可能需要的每种资源的最大数量，当系统拥有的资源总量能够提供时，再去判断分配资源后系统是否处于安全状态。只有判断处于安全状态才能进行分配



### 检测死锁

资源分配图，如果存在环说明发生了死锁



## 伪共享问题

在程序运行的过程中，由于缓存的基本单元`Cache Line`是64字节，所以缓存每次更新都会从内存中加载连续的64个字节。

如果访问的是一个 `long` 类型数组的话，当数组中的一个值比如 v1 被加载到缓存中时，接下来地址相邻的 7 个元素也会被加载到缓存中。（这也能解释为啥我们数组总是能够这么快，像链表这种离散存储的数据结构，就无法享受到这种红利）。

但是假如定义了两个变量a和b，它们在内存中的地址是紧挨着的。线程1要修改a，线程2要修改b。当线程1修改a时，它除了把a加载到自己的工作内存中，可能还会把b也加载进来。

根据 MESI 缓存一致性协议，修改完 a 后这个 `Cache Line` 的状态就是 `M`（`Modify`，已修改），而其它所有包含 a 的 `Cache Line` 中的 a 就都不是最新值了，所以都将变为 `I` 状态（`Invalid`，无效状态）

这样，当 T2 来读取 b 时，诶，发现他所处的 CPU 核心对应的这个 `Cache Line` 已经失效了，它就需要花费比较长的时间从内存中重新加载了。

由此可见，ab变量之间明明没有任何关系，但是每次都要由于a的修改而要到主存中重新读取b变量，这就是伪共享。表面上 a 和 b 都是被独立线程操作的，而且两操作之间也没有任何关系。只不过它们共享了一个缓存行，但所有竞争冲突都是来源于共享。

用更书面的解释来定义伪共享：==当多线程修改**互相独立**的变量时，如果这些**变量共享同一个缓存行**，就会无意中影响彼此的性能，导致无法充分利用缓存行特性，这就是伪共享。==



### 解决方法

1. 数据填充：在数据后进行填充，对齐缓存行，会浪费内存



### MESI缓存一致性协议

多核CPU的情况下有多个一级缓存，如何保证缓存内部数据的一致,不让系统数据混乱。这里就引出了一个一致性的协议MESI。

|           状态           |                             描述                             |                           监听任务                           |
| :----------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|    M 修改 (Modified)     | 该Cache line有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。 | 缓存行必须时刻监听所有试图读该缓存行相对就主存的操作，这种操作必须在缓存将该缓存行写回主存并将状态变成S（共享）状态之前被延迟执行。 |
| E 独享、互斥 (Exclusive) | 该Cache line有效，数据和内存中的数据一致，数据只存在于本Cache中。 | 缓存行也必须监听其它缓存读主存中该缓存行的操作，一旦有这种操作，该缓存行需要变成S（共享）状态。 |
|     S 共享 (Shared)      | 该Cache line有效，数据和内存中的数据一致，数据存在于很多Cache中。 | 缓存行也必须监听其它缓存使该缓存行无效或者独享该缓存行的请求，并将该缓存行变成无效（Invalid）。 |
|     I 无效 (Invalid)     |                      该Cache line无效。                      |                              无                              |



假设有三个CPU A、B、C，对应的三个缓存行分别为cache a、cache b、cache c，在主存中定义了变量x的值为0

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gzd10kqzumj30nr0n5t9k.jpg" alt="image-20220214143137516" style="zoom:50%;" />

#### 单核读取

CPU A发出读取x指令，从主存中通过总线bus读取x到自己的缓存cache a中，并把该缓存行的状态改为`E`（`Exclusive`，独享）状态

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gzd11qh6vlj30nd0mtt9s.jpg" alt="image-20220214143253681" style="zoom:50%;" />

#### 多核读取

1. CPU B发出读取x指令
2. CPU B尝试从主存中读取时，CPU A检测到地址冲突，此时对相关数据作出响应。
3. CPU B读取x到自己的缓存cache b中，并且cache a、cache b中该缓存行的状态都变为`S`（`Shared`，共享）

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gzd13td3m5j30n40mtjsi.jpg" alt="image-20220214143454117" style="zoom:50%;" />

#### 修改数据

1. CPU A发起修改x指令
2. CPU A把缓存行状态变为`M`（`Modified`，修改），并通知所有缓存了x的CPU
3. 缓存了x的其他CPU把本地缓存行的状态变为`I`（`Invalid`，无效）
4. 等其他缓存状态更新完毕后，CPU A对x进行修改（写入缓存）

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gzd15nwjz2j30nk0n7wfj.jpg" alt="image-20220214143640680" style="zoom:50%;" />

#### 同步数据

1. CPU B发出读取x的指令
2. CPU B通知CPU A，CPU A收到通知后将修改后的x写回主存，并把cache a的状态改为`E`（`Exclusive`，独享）
3. CPU B到主存中读取CPU A修改后写回主存的x，并把cache a、cache b的状态都修改为`S`（`Shared`，共享）

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gzd17rr68gj30nm0n30u3.jpg" alt="image-20220214143841958" style="zoom:50%;" />



#### MESI存在的问题

缓存的一致性消息传递是需要时间的，这就使其切换时会产生延迟。比如上面修改数据的例子中，修改数据的CPU必须阻塞等待其他所有的相关CPU缓存响应完成（状态切换完成）才能继续操作，可能出现的阻塞都会导致各种各样的性能问题和稳定性问题。（明明只是修改一点数据却需要等待很久）

**CPU切换状态阻塞解决-存储缓存（Store Buffers）**

比如你需要修改本地缓存中的一条信息，那么你必须将I（无效）状态通知到其他拥有该缓存数据的CPU缓存中，并且等待确认。等待确认的过程会阻塞处理器，这会降低处理器的性能。应为这个等待远远比一个指令的执行时间长的多。

为了避免这种CPU运算能力的浪费，`Store Buffer`被引入使用。处理器把它想要写入到主存的值写到缓存，然后继续去处理其他事情。当所有失效确认（`Invalidate Acknowledge`）都接收到时，数据才会最终被提交（写回`Cache`）。但是这么做有两个风险：

1. 同一个CPU的读写问题：处理器把修改数据写入`Store Buffer`之后处理其他事情，可能会需要读取这个数据，但是这个修改后的数据还在`Store Buffer`中，还没写回`Cache`。（即前面已经执行写入修改，但是这个修改对后面的代码不可见）

   > 这一点的解决方法是`Store Forwarding`，即CPU可以直接从`Store Buffer`中加载数据，即支持将CPU存入`Store Buffer`的数据传递（`Forwarding`）给后续的加载操作，而无需经过`Cache`

2. 多个CPU的读写问题：假设以下两个方法分别在两个CPU上执行，理想情况下是不可能出现`assert`失败的情况的，但是在多线程环境下，还是有一定几率出现`assert`失败的

   ```c
   // CPU0
   void foo(void)
   {
    a = 1;
    b = 1;
   }
   // CPU1
   void bar(void)
   {
    while (b == 0) continue;
    assert(a == 1);
   }
   ```

   1. 假设 a,b 初始值为 0 ，CPU0缓存了a和b，CPU1只缓存了a
   2. CPU1执行`while (b == 0)`，由于其缓存行中没有b，因此发出读取b的命令
   3. CPU0执行`a=1`，并发送消息给其他CPU通知那些缓存了a的状态要变为`I`（`Invalid`，失效），并把a=1写入`Store Buffer`。注意，a=1没有写入CPU0的缓存，即**CPU0的缓存中a仍然为0**
   4. CPU0继续执行`b=1`，由于只有其缓存了b，即b所在的缓存行是`E`（`Exclusive`，独享）状态，所以可以直接写入缓存。（**此时缓存中b=1**）
   5. CPU0收到2中的读取b的消息，把缓存中当前b=1返回给CPU1，并把b写回主存，状态改为`S`（`Shared`，共享）
   6. CPU1读取到b=1，跳出循环
   7. CPU1执行`assert`，由于此时CPU1中缓存的a仍然为0并且有效（还没有收到3中的通知），断言失败
   8. CPU1收到3中的失效通知，把本地缓存的a置为失效，之后要再读取a要重新到主存中读取，但是此时为时已晚

   > 这个其实就是指令重排的其中一个本质：CPU为了优化指令的执行效率，引入了`Store Buffer`（`forwarding`），而又因此导致了指令执行顺序的变化。
   >
   > 解决方法是写屏障，主要思路是：先把当前`Store Buffer`中的数据刷到`Cache`之后，再执行屏障后的“写入操作”。
   >
   > 两种方式：
   >
   > 1. 简单地刷`Store Buffer`，但如果此时远程`Cache Line`没有返回，则需要等待
   > 2. 将当前`Store Buffer`中的条目打标，然后将屏障后的“写入操作”也写到`Store Buffer`中，CPU继续干其他的事，当被打标的条目全部刷到`Cache Line`，之后再刷后面的条目

3. 对于x86架构来说，`Store Buffer`是FIFO，写入顺序就是刷入`Cache`的顺序。但是对于ARM/Power架构来说，`Store Buffer`并未保证FIFO，因此先写入`Store Buffer`的数据，是有可能比后写入`Store Buffer`的数据晚刷入`Cache`的

4. `Store Buffer`大小有限：尤其是引入写屏障之后，后续的写入操作无论本身是否`Cache Missing`（数据不在本地，即修改的数据状态不是`E`），都会写入`Store Buffer`，因此`Store Buffer`是很容易满的。`Store Buffer`满了之后CPU还是需要等待`Invalidate ACK`以处理`Store Buffer`中的条目。因此还是要回到Invalidate ACK中来，**`Invalidate ACK`耗时的主要原因是CPU要先将对应的`Cache Line`置为`Invalid`后再返回`Invalidate ACK`，一个很忙的CPU可能会导致其它CPU都在等它回`Invalidate ACK`。**

   > 解决方式是化同步为异步: CPU不必要处理了`Cache Line`之后才回`Invalidate ACK`，而是可以先将`Invalid`消息放到某个请求队列`Invalid Queue`，然后就返回`Invalidate ACK`。CPU可以后续再处理`Invalid Queue`中的消息（把`Cache Line`置为`I`），大幅度降低`Invalidate ACK`响应时间。



## 虚拟内存

虚拟内存是计算机系统内存管理的一种技术，它使得应用程序认为自己拥有连续可用的内存，但实际上，它通常是多个物理内存碎片，还有一些存储在外部磁盘存储器上。

直接使用物理内存会产生以下问题：

1. 内存空间利用率：内存碎片化问题，当需要分配一块很大的连续内存空间时，可能会出现明明有足够多的空闲的物理内存，但没有足够大的连续空闲内存而分配失败的情况。
2. 读写内存的安全性问题：物理内存本身不限制访问，任何地址都可以读写，但是现代操作系统需要实现不同页面拥有不同的读写权限
3. 进程间的安全问题：进程没有独立的内存空间，一个进程可以直接修改其他进程的数据
4. 内存读写效率问题：当多个进程同时运行，需要分配给进程的内存总和大于实际可用的物理内存时，需要将其他程序暂时拷贝到硬盘中，然后将新的程序装入内存运行。这样频繁的装入装出，内存的使用效率会非常低



#### 页表

每个进程有自己独立的虚拟地址空间，也有自己一个独立的页表用于对应虚拟地址和物理地址。页表的每一个表项分为两部分：

1. 记录此页是否在物理内存上
2. 如果在物理内存上，记录物理内存页的地址

进程访问某个虚拟地址之前，会先去查看页表，如果对应的数据页不在内存中，则发生缺页中断，需要调页进入内存。

为了加速页表的访问，使用了TLB（快表）Cache缓存。访问某个虚拟地址前，先查看TLB，如果缓存中有对应的物理地址，就不需要到主存去访问页表了



### 页面调度算法

1. 最佳淘汰算法OPT：淘汰以后永不使用/最长时间后才会被访问的页面，最优，但是无法实现，因为不可能预测到程序之后会需要哪些页面
2. 先进先出FIFO
3. 最近最久未使用LRU



## IO

![img](https://img-blog.csdn.net/20150111113049149?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhhbmd6ZXl1YWFh/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



### 阻塞IO

![img](https://img-blog.csdn.net/20150111112815015?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhhbmd6ZXl1YWFh/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

任何一个系统调用都会产生两次切换过程（用户态 ➡️ 内核态和内核态转换回用户态），而进程上下文切换是通过系统中断程序来实现的，需要保存当前进程的上下文状态，这是一个极其费力的过程。

流程：

1. 应用进程向内核发起`recvfrom`请求读取数据
2. 如果数据报还未准备好，应用进程就需要阻塞等待直到数据报准备好
3. 将数据报从内核拷贝到用户空间
4. 拷贝完成后，返回成功提示



### 非阻塞IO

![img](https://img-blog.csdn.net/20150111112909184?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhhbmd6ZXl1YWFh/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

流程：

1. 应用进程向内核发起`recvfrom`请求读取数据
2. 如果此时数据报还没准备好，则立刻返回`EWOULDBLOCK`错误码
3. 应用进程定时向内核发起`recvfrom`请求读取数据（轮询）
4. 数据报准备好后则把数据从内核拷贝到用户空间
5. 拷贝完成后，返回成功提示



### IO复用模型

在并发环境下，如果使用原本的IO模型，可能会出现大量的线程自己向内核发送`recvfrom`请求来读取数据，先不提内核是否能扛得住这么多线程的同时请求，线程是操作系统的宝贵资源，如果大量的线程都用来读取数据了，那么能做其他工作的线程就减少了，这显然是一种浪费。

因此提出了IO复用模型，即由一个线程来监控多个fd文件描述符（Linux把所有网络请求用fd来进行标识），这样就可以只使用一个或几个线程专门完成数据状态询问的操作，当有数据就绪后再分配一个线程去读取数据（把数据从内核拷贝到用户空间），这样就可以节省大量的线程资源。

> IO复用模型主要指的是select和poll



### 信号驱动IO模型

IO复用模型实现了无需为每个fd都创建一个监控线程，一个线程可以监控多个fd的问题，但是`select`是采用轮询的方式来监控多个fd的，通过不断的轮询来检查数据是否已经就绪。但是这样就显得很暴力，因此就产生了信号驱动IO模型，即不需要监控线程一直轮询检查数据是否已就绪，而是当数据准备好后内核主动通知监控线程分配线程来读取数据（把数据从内核拷贝到用户空间），从而避免了大量的不必要的轮询操作。



### 异步IO

即把数据读取（把数据从内核拷贝到用户空间）的操作交给内核，内核完成工作后发起一个通知告知线程工作已完成，可以进行后续操作

> select、poll、epoll本质上都是同步IO



### select、poll与epoll

|                |                       select                       |                     poll                     |                            epoll                             |
| :------------: | :------------------------------------------------: | :------------------------------------------: | :----------------------------------------------------------: |
|  **数据结构**  |                        数组                        |                     链表                     |                            红黑树                            |
| **最大连接数** |                        1024                        |                     不限                     |                             不限                             |
|   **fd拷贝**   |                每次调用select时拷贝                |              每次调用poll时拷贝              |    fd首次调用epoll_ctl时拷贝，每次调用epoll_wait时不拷贝     |
|  **工作效率**  |                      轮询O(n)                      |                   轮询O(n)                   |                     回调O(1)（事件驱动）                     |
|  **主要开销**  |            内核判断是否有文件描述符就绪            |         内核判断是否有文件描述符就绪         | 如果存在很多短期活跃连接，系统调用开销可能会大于select和poll |
|  **轮询问题**  | select和poll都需要不断轮询所有的fd集合直到设备就绪 |        期间可能需要睡眠和唤醒多次交替        |                调用epoll_wait不断轮询就绪链表                |
|    **适用**    |    当监测的fd数量较小，且各个fd都很活跃的情况下    | 当监测的fd数量较小，且各个fd都很活跃的情况下 |      当监听的fd数量较多，且单位时间仅部分fd活跃的情况下      |

1. `select`、`poll`主要基于IO复用模型，`epoll`基于事件驱动模型
2. `select`目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点，事实上从现在看来，这也是它所剩不多的优点之一。
3. `epoll`只在Linux中有实现
4. `select`和`poll`在空闲时会阻塞，当有IO事件发生时就醒过来对所有fd集合进行无差别轮询（因为它们只知道有IO事件发生了，但是不知道具体是哪个流）。同时处理的流越多，无差别轮询所需要的时间就越长
5. 当有事件就绪时，就会自动触发`epoll`回调函数，把就绪的fd放到`epoll`就绪链表中，调用`epoll_wait`实际上就是检查这个就绪链表里面是否有就绪的fd，如果有的话就直接返回，从而避免了无差别轮询的问题
6. `select`和`poll`每次调用的时候都需要把所有的fd从用户态复制到内核态，然后在内核态下轮询；而`epoll`则把整个fd集合维护在内核态中，每次调用`epoll_ctl`注册新的事件到`epoll`句柄中时，都会把fd拷贝进内核，之后调用`epoll_wait`进行轮询的时候就不需要拷贝了
6. `epoll`使用`mmap`加速内核与用户空间的消息传递
6. `epoll`是非阻塞的，只需要一句`socket.setBlocking(False)`
6. `select`、`poll`、`epoll`是同步的

> 表面上看`epoll`的性能最好，但是在连接数少并且连接都十分活跃的情况下，`select`和`poll`更加适用，因为`epoll`的通知机制需要很多函数回调



### epoll的两种工作方式

#### LT（电平触发）

- `LT（level triggered）`是默认的工作方式，并且同时支持`block`和`no-block socket`。
-  内核告诉应用程序fd是否就绪了，然后应用程序可以对这个就绪的fd进行IO操作。如果不作任何操作， **内核还是会继续通知**，所以，这种模式编程出错误可能性要小一点。传统的`select/poll`都是这种模型的代表。
- 在LT模式下，`epoll`相当于一个效率较高的`poll`



#### ET（边缘触发）

- `ET （edge-triggered）`是高速工作方式，只支持`no-block socket`。
- 在这种模式下，当描述符从未就绪变为就绪时， 内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且**不会再为那个文件描述符发送更多的就绪通知**。但是请注意，如果一直不对这个fd作IO操作（从而导致它再次变成未就绪）， 内核不会发送更多的通知（only once），不过在TCP协议中，ET模式的加速效用仍需要更多的`benchmark`确认。



### epoll空轮询问题及Netty解决方法

如果使用Java原生NIO编写：

```java
// 创建、配置 ServerSocketChannel
ServerSocketChannel serverChannel = ServerSocketChannel.open();
serverChannel.socket().bind(new InetSocketAddress(9998));
serverChannel.configureBlocking(false);
 
// 创建 Selector
Selector selector = Selector.open();
 
// 注册
serverChannel.register(selector, SelectionKey.OP_ACCEPT);
 
while (true) {
    selector.select();  // select 可能在无就绪事件时异常返回！
 
    Set<SelectionKey> readyKeys = selector.selectedKeys();
    Iterator<SelectionKey> it = readyKeys.iterator();
 
    while (it.hasNext()) {
        SelectionKey key = it.next();
        ...  // 处理事件
        it.remove();
    }
}
```

理论上`selector.select();`应该一直阻塞，直到有就绪事件返回，但是`Java NIO`中存在bug，即使在没有就绪事件的时候，`select()`也有可能返回。这样的话就会导致`while(true)`不断执行，最后导致某个CPU的利用率直接飙升到100%。

> 实际上，这是 Linux 系统下 `poll/epoll` 实现导致的 bug，但 `Java NIO` 并未完善处理它，所以也可以说是 `Java NIO` 的 bug。
>
> 该问题最早在 Java 6 发现，随后很多版本声称解决了该问题，但实际上只是降低了该 bug 的出现频率，Java 8 貌似还是存在该问题。

出现原因：如果一个`socket`文件描述符，注册的事件集合码为0，然后连接突然被对端中断，那么epoll会被`POLLHUP`或者有可能是`POLLERR`事件给唤醒，并返回到事件集中去。这意味着，`Selector`会被唤醒，即使对应的`channel`兴趣事件集是0，并且返回的`events`事件集合也是0。



Netty 的解决方式分为两步：

> 大部分框架的解决方法都类似，区别主要在如何检测，检测到后基本都是通过重建Selector来解决的

1. 检测 epoll bug

   Netty 使用 `NioEventLoop.select()` 替代 `Selector.select()`，检测epoll bug的逻辑就在`NioEventLoop.select()`中

   如果满足以下两个条件，则认为发生 epoll 空轮询：

   - `selector.select(timeoutMillis)` 阻塞时间小于 `timeoutMillis`
   - `select` 执行次数 > 阈值（默认 512）

   因为阻塞时间无法做到很精准，所以若某次阻塞时间大于等于 `timeoutMillis` 立刻重置 `selectCnt` 为 1，**即需要 连续 512 次 `selector.select(timeoutMillis)` 阻塞时间都小于 `timeoutMillis` 才认为发生了 epoll 空轮询。**

   `timeoutMillis` 有一套计算逻辑，无法进行配置，而次数阈值可以通过 `io.netty.selectorAutoRebuildThreshold` 系统配置进行设置，默认值为 512。

2. 通过重建 `Selector` 解决 epoll bug

   检测到 epoll bug 后，通过 `selectRebuildSelector` 方法来实际解决。在 Netty 中，一个 IO 线程可以处理多个 `channel`，但一个 `channel` 只能被一个 IO 线程处理，重建 `Selector` **必须** 在事件循环线程内完成，如果当前线程是 `NioEventLoop` 线程，则直接在当前线程执行 `Selector` 重建，否则将重建任务 `submit` 给各个 `NioEventLoop`。



## 零拷贝

### 什么是零拷贝？

零拷贝（`Zero-copy`）技术，狭义上是指计算机执行操作时，`CPU`不需要先将数据从某处内存复制到另一个特定区域；但实际中很难真正做到应用场景下的完全零复制，广义上讲可以说是减少冗余拷贝的I/O优化技术。



### 为什么需要零拷贝？

在没有零拷贝技术前，I/O过程中至少需要两次拷贝，这两次拷贝都会一直占用`CPU`，以`read()`调用为例：

1. 当用户进程调用`read`，`CPU`会从用户态转换为内核态，向`CPU`发送IO请求
2. `CPU`收到IO请求后，会把数据放入到磁盘控制器的内部缓冲区中，然后产生中断信号
3. `CPU`收到中断信号后，阻塞当前进程，然后把磁盘控制器缓冲区中的数据拷贝到内核缓冲区中
4. `CPU`再把内核缓冲区内的数据拷贝到用户空间中，并且这两次拷贝期间`CPU`无法执行其他任务
5. 两次拷贝完成后，`read`调用完成，`CPU`从内核态转换为用户态

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/I_O%20%E4%B8%AD%E6%96%AD.png)



### 实现技术

#### DMA（直接内存访问）技术

DMA（Direct Memory Access），就是==**在进行IO设备和内存的数据传输时，搬运数据的工作不再由CPU来完成，而是交给DMA控制器来完成（负责磁盘和内核缓冲区之间的数据拷贝），这样在数据搬运期间CPU就可以去完成其他工作。**==

> 但数据传输过程CPU仍然是必不可少的，因为要传输什么数据，从哪里传输到哪里都需要CPU来指挥，DMA并不具有这样的功能

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png)

1. 用户进程调用`read()`方法，`CPU`从用户态切换为内核态，向操作系统发出IO请求，请求读取数据到自己的内存缓冲区中，并阻塞进程

2. 操作系统收到IO请求后，把IO请求发送到`DMA`，然后唤醒进程使`CPU`继续执行任务

3. `DMA`把收到的IO请求进一步发送给磁盘

4. 磁盘收到IO请求后，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向`DMA`发送中断信号

5. `DMA`收到中断信号后，把磁盘缓冲区中的数据拷贝到内核缓冲区中，**本过程不占用`CPU`**

6. `DMA`读取足够多数据后发送中断信号给CPU

7. `CPU`收到中断信号后就会阻塞当前进程，把数据从内核拷贝到用户空间。至此，`read()`调用完成，`CPU`从内核态切换为用户态

   > 注意，DMA方式内核缓冲区和用户空间之间的数据拷贝仍然由CPU完成



##### DMA的应用

可参考https://www.yisu.com/zixun/459515.html



##### 存在问题

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)

以文件传输为例，需要进行一次`read()`调用和一次`write()`调用。

从图中看出，需要拷贝4次，其中两次是`DMA`拷贝，两次是`CPU`拷贝；`CPU`要在用户态和内核态之间来回切换4次。

> - *第一次拷贝*，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 `DMA` 搬运的。
> - *第二次拷贝*，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 `CPU` 完成的。
> - *第三次拷贝*，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 `socket` 的缓冲区里，这个过程依然还是由 `CPU` 搬运的。
> - *第四次拷贝*，把内核的 `socket` 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 `DMA` 搬运的。
>
> 
>
> - *第一次切换*，是`read()`调用开始，`CPU`从用户态切换到内核态
> - *第二次切换*，是`read()`调用完成，`CPU`从内核态转换回用户态
> - *第三次切换*，是`write()`调用开始，`CPU`从用户态切换到内核态
> - *第四次切换*，是`write()`调用结束，`CPU`从内核态转换回用户态

==所以，要想提高文件传输的性能，就要减少**用户态与内核态的上下文切换**和**内存拷贝**的次数==



###### 如何优化？

- **减少用户态与内核态的上下文切换的次数**

  读取磁盘数据时，之所以要发生上下文切换，这是因为用户态没有权限操作磁盘或网卡，因此需要切换到内核态去使用操作系统提供的系统调用函数来操作。而一次系统调用就必然会发生2次上下文切换：任务开始时，从用户态切换到内核态；任务完成后，从内核态切换为用户态。上图就是因为存在`read()`和`write()`两次系统调用，所以切换了4次。

  ==所以，要想减少上下文切换的次数，就要减少系统调用的次数==

- **减少内存拷贝的次数**

  先看看数据转移的位置：

  ![image-20211018154925613](https://tva1.sinaimg.cn/large/008i3skNgy1gvjiiovbz8j60io03b74i02.jpg)

  在文件传输的应用场景中，在用户空间并不会对数据进行再加工（即把文件原封不动的写回到内存），所以数据实际上可以不用搬运到用户空间，因此用户缓冲区是没有必要存在的



#### mmap+write

这种方式主要是减少内存拷贝的次数，为了减少把数据从内核缓冲区拷贝到用户的缓冲区这里，使用`mmap()`替换`read()`系统调用函数

==`mmap()` 系统调用函数会直接把内核缓冲区里的数据「**映射**」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。==

<img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20+%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:67%;" />

- 应用进程调用了 `mmap()` 后，`DMA` 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核**「共享」**这个缓冲区；
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 `socket` 缓冲区中，这一切都发生在内核态，由 `CPU` 来搬运数据；
- 最后，把内核的 `socket` 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 `DMA` 搬运的。

> 减少了一次拷贝（`mmap()`1次+`write()`2次）
>
> ==3次拷贝，上下文切换仍有4次==，因为仍然有2次系统调用



#### sendfile

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`，函数形式如下：

```c
#include <sys/socket.h>
// out_fd-目的端文件描述符，in_fd-源端文件描述符
// offset-源端偏移量，count-复制数据的长度
// ssize_t实际复制数据的长度
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

首先，它可以替代前面的 `read()` 和 `write()` 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。

其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 `socket` 缓冲区里，不再拷贝到用户态，这样就==只有 2 次上下文切换，和 3 次数据拷贝==。如下图：

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png)

但是这还不是真正的零拷贝技术，如果网卡支持 `SG-DMA（The Scatter-Gather Direct Memory Access）`技术（和普通的 `DMA` 有所不同），我们可以进一步减少通过 `CPU` 把内核缓冲区里的数据拷贝到 `socket` 缓冲区的过程（这样就只进行了2次数据拷贝）。



#### 写时复制COW

写时复制是计算机编程中的一种优化策略，它的基本思想是这样的：如果有多个应用程序需要同时访问同一块数据，那么可以为这些应用程序分配指向这块数据的指针，在每一个应用程序看来，它们都拥有这块数据的一份数据拷贝，当其中一个应用程序需要对自己的这份数据拷贝进行修改的时候，就需要将数据真正地拷贝到该应用程序的地址空间中去，也就是说，该应用程序拥有了一份真正的私有数据拷贝，这样做是为了避免该应用程序对这块数据做的更改被其他应用程序看到。

这个过程对于应用程序来说是透明的，**如果应用程序永远不会对所访问的这块数据进行任何更改，那么就永远不需要将数据拷贝到应用程序自己的地址空间中去。**

这也是写时复制的最主要的优点。

写时复制的实现**需要 `MMU` 的支持**，`MMU` 需要知晓进程地址空间中哪些特殊的页面是只读的，当需要往这些页面中写数据的时候，`MMU` 就会发出一个异常给操作系统内核，操作系统内核就会分配新的物理存储空间，即将被写入数据的页面需要与新的物理存储位置相对应。

写时复制的最大好处就是可以节约内存。

不过对于操作系统内核来说，写时复制增加了其处理过程的复杂性。

➡️ Linux中的fork创建进程就有用到写时复制技术



#### 共享缓冲区

用户态和内核态共享缓冲区，目前仍不成熟

