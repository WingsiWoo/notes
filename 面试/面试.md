# 面试

## 自我介绍

面试官好，我是来自广东工业大学计算机科学与技术专业的吴泳诗。在大学期间我曾经担任过两年的班长，培养了良好的团队协作能力和领导能力。在大二学年我加入了学校的TopView工作室，作为后台组的成员，我主要参与开发了两个项目：分别是基于海量数据的气象分析系统和智慧档案系统。同时，我作为后台组组长，负责组织平日的分享会以及新生的招新和培训工作。工作室的经历让我学习到了许多，但是我觉得我需要一个更大、更高的平台锻炼自己，这也是我到贵公司应聘的原因。以上是我的自我介绍，谢谢。



**为什么要选择计算机？**

在填报志愿的时候，确定自己想要学理工科，身边亲朋好友的推荐+自己对计算机也比较感兴趣，所以填报了计算机。上了大学后，发现身边有很多同学都是在之前就已经有编程的经验了，而我还是一个小白，虽然在开学前的暑假我也在努力自学C语言。我是一个非常不服输的人，所以我积极去自学，并跟师兄师姐交流，有针对性的去学习。在大一下学期的时候，工作室开始招新，我也进入了方向性的学习，从一个刚接触计算机的小白，不断历练自己，逐渐成为一个能自己独立完成项目的人，并顺利通过了工作室的考核。我的总结是：当我选择去做一件事情，我就一定会全力以赴去做到最好

**计算机女生很少，你怎么看/为什么选择？**

理工科方面女生占比比较少确实是一个事实，但是我当时选择这个专业的时候并没有怎么考虑过性别的问题，因为我觉得我可以用我的能力去弥补客观上性别的不足。上大学开始接触计算机后，我对这个行业的兴趣逐渐增大，这也是我继续努力的驱动力。另外，我认为我是有能力从事这一行业的，女生也有很强的思维能力和逻辑能力，计算机也是十分需要这些能力的，同时，我有刻苦钻研的精神，作为一个程序员，遇到bug很正常，重要的是怎么去发现和解决，我认为我有足够的敏锐度、耐心和毅力去发现和解决难题。最后，我很清楚的知道计算机是一个很辛苦的行业，一个优秀系统的背后有很多程序员的努力，我有信心、也很坚定的相信自己是一个很能吃苦、能以自己的热爱去应对这一行业的挑战。

**爱好**

平时喜欢唱歌、听音乐、画画，和朋友组建了一个乐队，担任鼓手，放长假回家的时候有时候会大家相约出来练习一下。

**为什么选择阿里？**

我认为，一个好的平台是培养人才的温室。阿里作为国内知名的互联网大厂，毫无疑问是一个优秀的平台，所以我无比珍惜和渴望进入阿里的机会。



## 项目介绍

### 基于海量数据的气象分析系统

基于海量数据的气象分析系统是一个对分散在各地的气象站监测到的气象数据进行统计和分析，通过图表的方式把数据展示给用户，方便专业的气象分析人员进行气象分析的平台。这个项目的主要难点是**数据量大**，数据量在三四千万级，以及**数据准确性要求高**。我在这个项目中负责的主要工作是月维度和滑动雨量的数据分析，月维度数据分析即基于更小的时间维度的数据（如日数据），对数据进行进一步分析，形成以月为时间单位的气象数据（比如月降雨量等）；滑动雨量的数据分析即对降雨数据进行分析，形成生成我们平时看到的降雨量折线图所需的数据。

还有项目代码的重构。因为这个项目有7个气象维度，5个时间维度，也就是有5*7=35个计算任务。如果不采用模板方法的话，维护这35个计算任务是十分困难的，而且如果代码有什么需要改动的地方，就必须要把35个任务全部都一起改动。因此我们采用了模板方法的设计模式，对计算任务的逻辑进行分层，把它们共同的逻辑抽取出来作为一个模板。最顶层的类的工作是记录任务运行所需的相关信息；第二层是获取需要计算的站点ID或区域ID；第三层是对需要计算的时间范围进行转换；最底层的就是具体的计算类，它的主要工作是计算、存取数据。其中第一二层是所有35个任务共用的模板；第三层是每个时间维度自定义的模板，同一个时间维度的所有气象维度都使用相同的第三层模板。

#### 数据量大的解决

1. 建立索引，加快查询速度。因为大时间维度的数据计算需要查询小时间维度的数据 🌟MySQL索引

   ➡️ 建立索引的注意事项：

   - 最左匹配原则：建立联合索引时候要注意顺序，查询数据时也要注意参数的顺序
   - 尽量使用区分度高的数据（重合少）作为索引
   - 索引列不能参与计算
   - 尽可能选择长度较小的列作为索引，因为这样一个节点能存储的索引数就增多了，从而有效降低树高、减少IO次数
   - 尽量扩展索引，不要新建索引

2. 异步计算：计算时需要请求气象局提供的接口获取源数据，气象局提供的接口有一定的性能限制，导致单次请求的延迟增大，在低负载时延迟在100ms左右，在高负载时甚至可能会达到数秒。为了解决这一I/O瓶颈问题，我们引入了线程池基于站点的并发计算，通过提高并发数量来尽量抵消花费在I/O等待上的计算空闲，但是这样带来的负面影响是CPU会频繁切换上下文，带来额外的开销。

   > 进一步优化：异步调用获取源数据的接口，请求后程序继续执行，等到需要处理源数据的时候，如果还没有请求到源数据就等待

3. 线程池

   - 如果不使用线程池，那么这个线程在执行完任务后就会被销毁。但是我们这个系统的计算需求量还是比较大的，这样的话就会频繁创建和销毁线程，浪费了大量的时间，引起性能损耗

   - 大量的线程创建、执行和销毁是非常耗cpu和内存的，这样将直接影响系统的吞吐量，导致性能急剧下降，如果内存资源占用的比较多，还很可能造成OOM

   - 大量的线程的创建和销毁很容易导致GC频繁的执行，从而发生内存抖动现象

     > 内存抖动现象即某一时间突然创建大量对象，可用内存不足，从而引起频繁GC回收对象，这种已用内存忽高忽低的现象就叫内存抖动现象。由于GC的时候不可避免的需要STW，所以内存抖动现象发生的时候最大的体现就是界面卡顿

   - 所以为了避免前面所说的情况，我们采用了线程池

   - 线程的创建和销毁由线程池维护，一个线程在完成任务后并不会立即销毁，而是由后续的任务复用这个线程，从而减少线程的创建和销毁，节约系统的开销，从而节约系统资源，提高系统吞吐量

   - 而且通过线程池可以很方便的根据运行环境的情况调整线程池的大小

4. 重试机制

   - 增加了数据修正的机制，重算定时任务会定时主动收集发生了变更的数据，然后对数据库中的数据进行更正。比如去年的 12 月 31 日的数据被更正，那么会导致去年的最后一个月、最后一个旬、最后一个季、以及去年的数据等重新计算。
   - 对于某些耗时比较长的计算任务，做到可分治计算，失败时可重试计算，可从上一次计算异常结束的地方接着计算下去的能力

#### 数据准确性要求高的解决

使用BigDecimal，精确到指定的小数后几位数

#### BigDecimal构造的问题

```java
BigDecimal a = new BigDecimal(0.1);	// 0.10000……5551
BigDecimal b = new BigDecimal("0.1");	// 0.1
```

使用浮点数传参可能会产生不可预期的结果（主要还是精度的问题），而使用字符串传参就是预计的结果

> 也就是说BigDecimal实际上也会丢失精度，它只能计算的无限接近这个数，但是无法精确到这个数。所以如果想要获取真正精确的结果，需要转换成字符串进行运算



#### 项目中线程池参数的设置

- 线程池的参数有：核心线程数、最大线程数、空闲线程存活时间和单位、阻塞队列、拒绝策略、线程工厂

  > new ThreadPoolExecutor

- 因为任务的执行频率比较频繁（滑动雨量，分钟级），因此我们创建了一个固定大小的线程池，即使用最大长度的 `LinkedBlockingQueue` 作为阻塞队列，然后核心线程数设置为 `50`。设置为 50 主要从两方面考虑：
  - 任务的 I/O 等待时间比任务的实际计算时间长，因此可以衡量两者的比例关系乘上 CPU 逻辑处理器的数量再乘上 CPU 的期望利用率，得到大约 50 的线程数
  - 由于接口是气象局中很多其他业务共同使用的，因此需要考虑接口提供方的压力，防止并发程度过大影响了其他计算业务，特别是在天气情况恶劣的时间段，要尽可能地减少对其他关键业务的影响。在之前也出现过因为并发度过大，接口那边出现了问题的情况，所以在不断调试中设置了50这个既不会使接口压力太大也不会使程序并发度过低的数值。

- 使用的阻塞队列是LinkedBlockingQueue。没有用Java提供的常用线程池而是自己指定参数，通过配置文件读取参数方便配置，通过单例模式获取线程池工厂

延伸 ➡️：

- 阻塞队列

  - `ArrayBlockingQueue`：先进先出，数组实现

  - `LinkedBlockingQueue`：先进先出，链表实现，是`FixedThreadPool`和`SingleThreadExecutor`使用的阻塞队列，默认设置的长度为`Integer.MAX_VALUE`，可以认为是无限长

  - `PriorityBlockingQueue`：是一个支持优先级的无界阻塞队列。默认情况下元素采取**自然顺序升序排列**。继承`Comparable`类实现`compareTo()`方法来指定元素排序规则，或者初始化`PriorityBlockingQueue`时，指定构造参数`Comparator`来对元素进行排序。需要注意的是不能保证同优先级元素的顺序。

  - `DelayQueue`：是一个支持延时获取元素的无界阻塞队列。队列使用`PriorityQueue`来实现。队列中的元素必须实现`Delayed`接口，在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素

  - `SynchronousQueue`：是一个不存储元素的阻塞队列。每一个`put`操作必须等待一个`take`操作，否则不能继续添加元素。它支持公平访问队列。默认情况下线程采用非公平性策略访问队列，如果构造器参数设置为`true`，则等待的线程会采用先进先出的顺序访问队列。可以看作是一个传递者，把生产者线程的数据直接传给消费者，吞吐量远高于`ArrayBlockingQueue`和`LinkedBlockingQueue`

  - `LinkedTransferQueue`：是一个由链表结构组成的无界阻塞`TransferQueue`队列

    （1）`transfer`方法

    如果当前有消费者正在等待接收元素（消费者使用`take()`方法或带时间限制的`poll()`方法时），`transfer`方法可以把生产者传入的元素立刻`transfer`（传输）给消费者。如果没有消费者在等待接收元素，`transfer`方法会将元素存放在队列的tail节点，并等到该元素被消费者消费了才返回

    （2）`tryTransfer`方法

    `tryTransfer`方法是用来试探生产者传入的元素是否能直接传给消费者。如果没有消费者等待接收元素，则返回false。和`transfer`方法的区别是`tryTransfer`方法无论消费者是否接收，方法立即返回，而`transfer`方法是必须等到消费者消费了才返回。

    对于带有时间限制的`tryTransfer（E e，long timeout，TimeUnit unit）`方法，试图把生产者传入的元素直接传给消费者，但是如果没有消费者消费该元素则等待指定的时间再返回，如果超时还没消费元素，则返回false，如果在超时时间内消费了元素，则返回true

  - `LinkedBlockingDeque`：是一个由链表结构组成的双向阻塞队列。所谓双向队列指的是可以从队列的两端插入和移出元素。双向队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争，可以运用在“工作窃取”模式中

  > 工作窃取模式，即Fork/join框架 
  >
  > 当一个任务比较大的时候，可以把这个大任务分割为多个互不依赖的小任务，为了减少线程竞争，把这些子任务放入不同的阻塞队列，并为每个队列设置一个工作线程。当某个队列完成了任务的时候，它就可以到其他队列中窃取任务来执行，为了减少从队列中取任务的线程冲突，通常会使用双端队列，被窃取线程只从一端取，窃取线程只从另一端取

- 阻塞队列的实现：阻塞队列与普通队列的区别在于，当队列为空的时候，尝试获取元素的线程会被阻塞，直到有线程向队列中添加元素才被唤醒；当队列满的时候，尝试添加元素的线程会被阻塞，直到有线程从队列中取出元素才被唤醒。使用`Lock+Condition`，通过等待/通知机制，来实现线程之间的通信

- 拒绝策略

  - `AbortPolicy`：默认的拒绝策略，直接抛出`RejectedExecutionException`，用户可捕获这个异常并自己编写异常处理逻辑

  - `DiscardPolicy`：抛弃策略，什么都不做，直接抛弃被拒绝的任务

  - `DiscardOldestPolicy`：抛弃最老策略，即抛弃阻塞队列的队首任务（最老任务）

    > 如果阻塞队列是一个优先队列，那么会导致抛弃最高优先级任务。因此该拒绝策略最好不要跟优先级队列放在一起使用

  - `CallerRunsPolicy`：在调用者线程中执行该任务。即把任务回退到调用线程池执行任务的主线程来执行任务，由于主线程要执行任务，因此主线程至少在一段时间内不能提交任务。

- 常用线程池

  - `newCachedThreadPool`：核心线程数为0，最大线程数为`Integer.MAX_VALUE`。创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。**使用`SynchronousQueue`**
  - `newFixedThreadPool` ：创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。**使用`LinkedBlockingQueue`**
  - `newScheduledThreadPool`： 创建一个定长线程池，支持定时（scheduleWithFixedDelay（）函数的initdelay 参数）及周期（delay 参数）任务执行。**使用`DelayedWorkQueue`**
  - `newSingleThreadExecutor` ：创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。**使用`LinkedBlockingQueue`**
  - `newSingleThreadScheduledExecutor`： 创建一个单线程化的支持定时的线程池，可以用一个线程周期性执行任务(比如周期7天，一次任务才用1小时,使用多线程就会浪费资源)。**使用`DelayedWorkQueue`**



#### 定时任务框架的选用

项目接受的时候使用到的技术栈已经定下来了，但是自己有去粗略了解相关的框架，了解选择原因

1. `Timer`

   - 即`java.util.Timer`，JDK 1.3 开始就已经支持的一种定时任务的实现方式

     > `Timer` 内部使用一个叫做 `TaskQueue` 的类存放定时任务，它是一个基于**最小堆**实现的优先级队列。`TaskQueue` 会按照任务距离下一次执行时间的大小将任务排序，保证在堆顶的任务最先执行。这样在需要执行任务时，每次只需要取出堆顶的任务运行即可！

   - `Timer`的任务只能串行执行，不符合业务需求

   - 性能差

   - 发生异常时任务直接停止（`Timer` 只捕获了 `InterruptedException` ）

2. `ScheduledExecutorService`

   - `ScheduledThreadPoolExecutor` 本身就是一个线程池，**支持任务并发执行**。并且，其内部使用 `DelayQueue` 作为任务队列。
   - 无法使用 `Cron` 表达式指定任务执行的具体时间

3. `Spring Task`

   - `Spring`自带的定时调度，使用`@Scheduled`即可定义任务，支持`Cron`表达式
   - 只支持单机，功能单一（考虑到项目之后可能会扩展，所以没有选用）
   - 不支持持久化，无法满足可视化要求
   - 默认单线程执行任务，任务是串行执行的

4. `Quartz`

   - 知名：`Quartz`是一个很火的开源任务调度框架，完全由`Java`写成。`Quartz` 可以说是 `Java` 定时任务领域的老大哥或者说参考标准，其他的任务调度框架基本都是基于 `Quartz` 开发的
   - 可以与 `Spring` 集成，并且支持动态添加任务和集群
   - 支持分布式任务（但是实现并不是十分良好，是基于数据库层面实现的，通过数据库的锁机制做的，有非常多的弊端比如系统侵入性严重、节点负载不均衡。有点伪分布式的味道）
   - 支持持久化，分为调度线程和执行线程，执行效率较高且更为成熟



#### 多线程执行任务的问题

**怎么保证一个任务只能由一个线程执行**

通过ConcurrentHashMap抢占锁，一个锁对应一个任务。保证了一个任务只能由一个quartz线程执行。执行任务的过程中通过站点分割任务，每个站点的数据都是独立的，所以任务互相隔离，之后通过线程池提交每个站点任务保证任务之间不会冲突

**怎么保证任务不会重复执行**

trigger的状态储存在数据库，Quartz支持分布式，所以如果起了多个quartz服务，会有多个调度线程来抢夺触发同一个trigger。mysql在默认情况下执行select 语句，是不上锁的，那么如果同时有1个以上的调度线程抢到同一个trigger，将会导致重复导致，quartz怎么解决？
通过锁名称获取locks表中的一个行锁，开启事物，保证同一时刻只有一个线程可以执行这个方法



#### Quartz

**Quartz怎么避免多线程环境下同时调度同一个任务**

1. Quartz在数据库中有几张自带的表：

   - triggers：记录了某个trigger的PREV_FIRE_TIME（上次触发时间），NEXT_FIRE_TIME（下一次触发时间），TRIGGER_STATE（当前状态）

     > Quartz把任务的触发称为fire

   - locks：Quartz支持分布式，为了处理多个线程同时抢占相同资源的情况，创建了locks表

   - fired_triggers：正在触发的trigger的信息

2. trigger状态

   <img src="https://segmentfault.com/img/bVbdx7u?w=715&h=606" alt="图片描述" style="zoom:75%;" />

3. Job的调度依赖trigger的状态，而trigger的状态存储在数据库中，在多线程环境下，可能会有多个调度线程来抢占同一个trigger。而MySQL的select语句在默认情况下是不上锁的
4. Quartz借助数据库的行级锁来实现分布式锁（这是一种悲观锁，保证集群中一次只能有一个线程来进行操作，如果操作时间过长的话，会带来集群间主线程的等待）
   1. 使用select…for update把当前行锁住，此时如果其他线程再select…for update就会因为没有获取到锁而阻塞
   2. 检查当前任务是否执行过，如果没有执行过就执行任务
   3. 事务提交后释放锁



**Quartz怎么避免任务的重复执行**

1. 如果任务只执行一次：任务执行完后trigger就是COMPLETE状态，不会再被触发
2. 如果任务执行多次：假设任务每20s执行一次，线程12同时想调度该任务，线程1先抢到锁并完成了调度。那么线程1调度完后会修改该任务的trigger的下次出发时间，当线程2抢占到锁后发现trigger中的触发时间还没到，就不会进行触发，从而避免了重复执行



#### 总结

虽然这个项目不算是一个很大的项目，但是我从中学到了很多的东西：

1. 宝贵的项目经验，对于本科生来说还是比较难得的

2. 发现问题和解决问题的能力：对于程序员来说，遇到bug很正常，重要的是怎么快速定位bug并想办法解决

   > 比如遇到慢SQL的时候进行排查，然后通过建立索引解决了慢SQL的问题

3. 精益求精的态度：在项目重构之前，只是那35个计算任务系统也可以正常运行，但是一个优秀的系统不仅仅是能跑就行，它的可维护性、代码的耦合度等都是一个优秀的程序员应该考虑的，我认为不断寻求优化系统的精益求精的态度是一个优秀程序员必备的态度



### 智慧档案系统

智慧档案系统是一个把档案进行电子化管理的系统，它可以存储文件、照片、音频、视频等多媒体档案，还支持人脸识别，智能化管理档案，以及URL自动爬取校园新闻网的新闻。我在这个项目中负责工作是工作台部分的开发和权限的管理，工作台也就是用于存放档案的文件夹，通过工作台用户可以对档案进行分类操作；权限管理即把多个权限按照其功能特点放到一起进行管理，形成一个多层的权限树，使得系统的管理员可以实现给角色一键分配多个权限，比如想要给用户分配一个工作台的增删改查权限，管理员只需要给这个用户分配一个工作台的权限即可。



#### OSS

**图片、视频、音频都存储在阿里云OSS中**

OSS，对象存储服务，提供海量、安全、低成本、高可靠的云存储服务，其具有与平台无关的RESTful API接口，可以在任意应用、任意时间、任意地点存储与访问任何类型的数据。



## 阿里笔试

### 晶体数量

![image-20220309192124517](https://tva1.sinaimg.cn/large/e6c9d24egy1h03un29927j20he0do75m.jpg)

每次增加的晶体数为一个等差数列，如上图三角形，等差d为3-2=1，初始值a1=1，因此第一次增加的晶体数为a2=a1+d=2，第二次增加的晶体数为a2+d=2+1=3

<img src="https://uploadfiles.nowcoder.com/images/20220304/94288463_1646408455947/1672987AA554B6F43D748D962DA36216" alt="img" style="zoom:50%;" />

最后晶体总数就是等差数列求和：S~n~=n*a1 + n\*(n-1)\*d/2

<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h03uv2a120j20zm0f00v8.jpg" alt="image-20220309192906258" style="zoom:50%;" />



### 正多边形内等腰锐角三角形的个数

想法：把正多边形的顶点看作在一个圆上，锐角三角形的个数就是所有三角形个数-直角三角形个数-钝角三角形个数

1. 所有三角形个数：任选三个顶点，即C3n=(n\*(n-1)\*(n-2))/6



## MVC

`MVC`是一种设计模式，在这种设计模式下软件被分为三层：`Model`（模型）、`View`（视图）、`Controller`（控制器）。`Model`代表数据，`View`代表用户界面，`Controller`代表数据的处理逻辑，它是`Model`和`View`之间的桥梁。这样把软件分层的好处是：可以使对象之间的耦合度降低，便于代码的维护。

1. `Model`：指从现实世界中抽象出来的对象模型。是应用逻辑的反应。它封装了数据和对数据的操作，是实际进行数据处理的地方。在MVC的三个部件中，模型拥有最多的处理任务。被模型返回的数据是中立的，模型与数据格式无关，这样一个模型能为多个视图提供数据，由于应用于模型的代码只需写一次就可以被多个视图重用，所以减少了代码的重复性。
2. `View`：是应用和用户之间的接口，它负责将应用显示给用户和显示模型的状态。MVC的好处之一在于它能为应用程序处理很多不同的视图。在视图中其实没有真正的处理发生，它只是作为一种输出数据并允许用户操纵的方式。
3. `Controller`：控制器负责视图和模型之间的交互，控制对用户输入的响应、响应方式和流程。它主要负责两方面的动作，一是把用户的请求分发到相应的模型，二是把模型的改变及时地反映到视图上。

> 最典型的MVC就是`jsp+servlet+javabean`模式。
>
>  以`JavaBean`作为模型，既可以作为数据模型来封装业务数据，又可以作为业务逻辑模型来包含应用的业务操作。JSP作为视图层，负责提供页面为用户展示数据，提供相应的表单来用于用户的请求，并在适当的时候向控制器发出请求来请求模型进行更新。`Serlvet`作为控制器，用来接收用户提交的请求，然后获取请求中的数据，将之转换为业务模型需要的数据模型，然后调用业务模型相应的业务方法进行更新，同时根据业务执行结果来选择要返回的视图。
>
>  当然，这种方式现在已经不那么流行了，`Spring MVC`框架已经成为了MVC模式的最主流实现。`Spring MVC`框架是基于Java的实现的MVC框架模式的轻量级框架。前端控制器是`DispatcherServlet`，映射处理器是`HandlerMapping`接口实现类，视图解析器是`ViewResolver`接口实现类，页面控制器是`Controller`接口实现类。



## Serializable接口为什么需要定义serialVersionUID常量

### serialVersionUID的工作机制

这个`serialVersionUID`是用来辅助对象的序列化与反序列化的，原则上序列化后的数据当中的`serialVersionUID`与当前类当中的`serialVersionUID`一致，那么该对象才能被反序列化成功。这个`serialVersionUID`的详细的工作机制是：在序列化的时候系统将`serialVersionUID`写入到序列化的文件中去，当反序列化的时候系统会先去检测**文件中的`serialVersionUID`是否跟当前的文件的`serialVersionUID`是否一致**，如果一致则反序列化成功，否则就说明当前类跟序列化后的类发生了变化，比如是成员变量的数量或者是类型发生了变化，那么在反序列化时就会发生crash，并且回报出错误`InvalidClassException`



### 原因

主要是出于兼容性的考虑。`serialVersionUID`是序列化版本号，如果没有显式定义的话，JVM会自动为实现了`Serializable`接口的类生成一个常量，但是这个自动生成的常量对类的详细信息非常敏感。这可能会因编译器实现而异，因此可能会在反序列化期间导致`InvalidClassException`。为了保证不同Java编译器实现之间的一致的`serialVersionUID`值，一个可序列化的类必须声明一个显式的`serialVersionUID`值。



### 反序列化失败的原因

1. 如果修改类时只是修改了方法，则反序列化不受影响。
2. 如果修改类时只是修改了静态变量，则反序列化不受影响。
3. 如果修改类时改变了实例变量，则可能导致反序列化失败。



## int与Integer的区别

占用内存：`Integer`是一个对象，需要存储对象的元数据；而`int`是原始数据类型，所以占用内存较少

1. 两个通过`new`出来的`Integer`变量==比较，结果为`false`

   ```java
   Integer i = new Integer(200);
   Integer j = new Integer(200);
   System.out.print(i == j);	// false
   ```

   `Integer`是包装类，生成的i和j这两个变量实际上是对一个`Integer`对象的引用，当`new`一个`Integer`时，实际上是生成一个指针指向此对象，两次`new Integer`生成的是两个对象，其内存地址不同，所以比较结果为`false`。

2. 非`new`生成的`Integer`变量与`new Integer()`生成的变量==比较，结果为`false`

   ```java
   Integer i= new Integer(200);
   Integer j = 200;
   System.out.print(i == j);	// false
   ```

    因为非`new`生成的`Integer`变量指向的是java常量池中的对象，而`new Integer()`生成的变量指向堆中新建的对象，两者在内存中的地址不同。所以比较结果为`false`

3. 两个非`new`生成的`Integer`对象进行==比较，如果两个变量的值在区间[-128,127]之间，比较结果为`true`；否则，结果为`false`

   ```java
   Integer i1 = 127;
   Integer ji = 127;
   System.out.println(i1 == ji);// true
   Integer i2 = 128;
   Integer j2 = 128;
   System.out.println(i2 == j2);// false
   ```

   java在编译`Integer i1 = 127`时，会翻译成`Integer i1 = Integer.valueOf(127)`。==java会将[-128,127]之间的数进行缓存==。`Integer i1 = 127`时，会将127缓存，`Integer j2 = 127`时，就直接从缓存中取，不会`new`了，所以结果为`true`。

   `Integer i2 = 128`时，不会将128缓存，`Integer j2 = 128`时，会`return new Integer(128)`。所以结果为`false`。

4. `Integer`变量(无论是否是`new`生成的)与`int`变量==比较，只要两个变量的值是相等的，结果都为`true`

   ```java
   Integer i1 = 200;
   Integer i2 = new Integer(200);
   int j = 200;
   System.out.println(i1 == j);// true
   System.out.println(i2 == j);// true
   ```

   包装类`Integer`变量在与基本数据类型`int`变量比较时，`Integer`会自动拆包装为`int`，然后进行比较，实际上就是两个`int`变量进行比较，值相等，所以为`true`。



## JWT

JWT即`Json Web Token`，是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准。该`token`被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该`token`也可直接被用于认证，也可被加密。

### 传统的Session认证

HTTP本身是一种无状态的协议，为了记录用户的状态，需要借助其他的东西来记录用户信息。用户请求服务器时，服务器会先查看该用户是否已登陆，如果已登陆则根据用户提交的`SessionID`在自己存储的`Session`集合中检查用户的信息；如果没有登陆或登陆过期，则为该用户创建一个`Session`，在本地保存并返回给用户。用户会把这个`Session`存储在`Cookie`中，作为之后请求的凭证。但是这种传统的基于`Session`的认证有很多缺陷：

1. 难以扩展：由于`Session`是保存在服务器的，在分布式的场景下，如果用户第二次请求的是另一台服务器，那么这台服务器由于本地没有保存该用户的`Session`信息，误以为该用户没有登陆
2. 服务器开销：随着认证用户的增多，服务器需要存储的`Session`增多，压力增大
3. `CSRF`攻击：用户的`Cookie`可能会被劫持，用户就会很容易受到跨站请求伪造的攻击。



### 基于Token的鉴权机制

基于`Token`的鉴权机制不需要服务器保存用户信息，这样就意味着不会有前两点缺点。这个`token`在每次请求时需要在请求头携带，另外，服务端要支持`CORS(跨来源资源共享)`策略，一般我们在服务端这么做就可以了`Access-Control-Allow-Origin: *`，在客户端只需要在请求头加上`Authorization`，并加上`Bearer`标注（后面跟token）

优点：

- 因为json的通用性，所以JWT是可以进行跨语言支持的，像JAVA,JavaScript,NodeJS,PHP等很多语言都可以使用。
- 因为有了`payload`部分，所以JWT可以在自身存储一些其他业务逻辑所必要的非敏感信息。
- 便于传输，JWT的构成非常简单，字节占用很小，所以它是非常便于传输的。
- 它不需要在服务端保存会话信息, 所以它易于应用的扩展

安全相关：

- 不应该在JWT的`payload`部分存放敏感信息，因为该部分是客户端可解密的部分。
- 服务器需要保护好`secret`私钥，如果泄漏出去任何人都能使用该私钥自己签发JWT
- 如果可以，请使用https协议



#### 构成

1. 头部

   - `typ`：声明类型，即JWT
   - `alg`：声明加密的算法，一般使用HMAC、SHA256

   将头部进行`base64`加密（该加密是可以对称解密的),构成了第一部分

2. 载荷`playload`

   - 标准中注册的声明（建议但不强制使用）

     - **iss**: jwt签发者
     - **sub**: jwt所面向的用户
     - **aud**: 接收jwt的一方
     - **exp**: jwt的过期时间，这个过期时间必须要大于签发时间
     - **nbf**: 定义在什么时间之前，该jwt都是不可用的.
     - **iat**: jwt的签发时间
     - **jti**: jwt的唯一身份标识，主要用来作为一次性`token`,从而回避重放攻击。

   - 公共的声明

     可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密

   - 私有的声明

     私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为`base64`是对称解密的，意味着该部分信息可以归类为明文信息

   将其进行`base64`加密，得到JWT的第二部分。

3. 签证

   - `header` (`base64`后的)
   - `payload` (`base64`后的)
   - `secret`

   这个部分需要`base64`加密后的`header`和`base64`加密后的`payload`使用`.`连接组成的字符串，然后通过`header`中声明的加密方式进行加盐`secret`组合加密，然后就构成了JWT的第三部分。==将这三部分用`.`连接成一个完整的字符串,构成了最终的JWT（`base64`加密的头部.`base64`加密的载荷.前两部分按照指定算法加密后得到的字符串）==



## 泛型

### 语法糖

语法糖就是在计算机语言里面通过添加某种语法，这种语法不会对语言的编译结果产生实际的影响，但是可以使得程序员更方便的使用该语言，增加程序的可读性，减少代码量，提高开发效率。java里面常见的语法糖除了泛型除了之外，还有自动装箱拆箱，变长参数，他们都是语法糖。

> 但是实际上JVM虚拟机并不支持这些语法糖中的语法，他们会在编译阶段被还原成原始的基本语法结构。

### 本质

泛型的本质是**参数化类型**，也就是说将操作的数据的类型作为方法签名中的一个**特殊参数**，这种特殊参数能够使用在类，接口和方法的创建中，分别构成泛型类，泛型接口，泛型方法。

### 实现原理

**类型擦除（编译时实现）**：也就是说java里面的泛型只存在于源代码里面，一旦经过编译之后，所有的泛型都会被擦除掉，全部被替换为原来的裸类型，**并在对元素进行访问和修改的时候，才会加上强制类型转换**。（所谓的裸类型指的是，`ArrayList<Integer>` 他的裸类型就是`ArrayList`）。也正是因为这样，如果我们在运行时判断`ArrayList<String>` 和`ArrayList<Integer>`会发现他们属于一个类型。

 如果我们对使用了泛型的`class`文件进行反编译之后就可以发现，所有的泛型都消失了，全部被擦除，被替换成了裸类型，然后所有访问和修改的地方都添加了一个强制类型转换。比如下面，只有在元素访问的时候，做了从`Object`到`String`的强制类型转换。
### 缺点 

1. 使用类型擦除直接导致了对于原始的数据类型无法支持，比如`int`，`long`这种，因为java不支持`Object`类型和基本数据类型之间的强制类型转换，也就是说一旦类型擦除之后，就没法再进行类型转换了。也正是这样，现在的泛型都是不支持原始类型的，比如`ArrayList<Integer>`，而不能使用`ArrayList<int>`。
2. 运行期间无法获得泛型类型信息。因为泛型都被擦除了，都被替换成了裸类型。这样就导致了后续的部分操作会报错，比如无法使用泛型来创建对象，或者数组。



## 一致性哈希

对于分布式系统的负载均衡，由于不同节点存储的数据可能是不一样的，这样就意味着客户端可能需要请求特定的节点才能得到想要的结果，因此负载均衡算法需要考虑到哪些请求需要转发到哪些指定的服务器

### 哈希算法

- 哈希算法解决了分布式系统的负载均衡问题，但是迁移数据量太大

哈希算法可以解决分布式系统的负载均衡问题，但是又出现了一个新的问题：如果节点数量发生了变化（比如系统扩容或者缩容、服务器宕机），就可能会导致大量映射关系的改变，导致需要迁移数据到新的节点。在最坏情况下，所有服务器的数据都需要迁移，这样的话迁移成本太大了



### 一致性哈希算法

- 一致性哈希算法解决了迁移数据量大的问题，但是存在节点分布不均匀、可能引起雪崩反应的问题

为了解决哈希算法的这个问题，提出了一致性哈希算法。一致性哈希算法是对一个固定的数2^32^进行取模，可以想象一个由2^32^个节点组成的哈希环，一致性哈希算法把存储节点和数据（服务器和客户端）都放在这个哈希环上，**映射结果顺时针寻找到的第一个节点就是目标节点**

<img src="https://img-blog.csdnimg.cn/img_convert/30c2c70721c12f9c140358fbdc5f2282.png" alt="img" style="zoom:67%;" />

> 在一致性哈希算法中，**如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响**。

但是一致性哈希算法并不保证节点在哈希环上均匀分布，如下图：节点A承受了所有的请求

<img src="https://img-blog.csdnimg.cn/img_convert/d528bae6fcec2357ba2eb8f324ad9fd5.png" alt="img" style="zoom:67%;" />

除了负载不均衡的问题，在这种节点分布不均匀的情况下，进行容灾与扩容时，哈希环上的相邻节点容易受到过大影响，容易发生雪崩式的连锁反应。

比如，上图中如果节点 A 被移除了，当节点 A 宕机后，根据一致性哈希算法的规则，其上数据应该全部迁移到相邻的节点 B 上，这样，节点 B 的数据量、访问量都会迅速增加很多倍，一旦新增的压力超过了节点 B 的处理能力上限，就会导致节点 B 崩溃，进而形成雪崩式的连锁反应。



### 虚拟节点提高均衡度

为了解决一致性哈希算法存在的问题，可以向哈希环中加入多个虚拟节点以提高均衡度，相当于多了一层映射，如果哈希结果的下一个节点为虚拟节点，则去请求其对应的真实节点

<img src="https://img-blog.csdnimg.cn/img_convert/dbb57b8d6071d011d05eeadd93269e13.png" alt="img" style="zoom:67%;" />

另外，虚拟节点的引入还可以避免节点变动时的雪崩式的连锁反应：当一个真实节点删除时，其对应的所有的虚拟节点也会被删除，而这些虚拟节点顺时针的下一个虚拟节点可能对应的是不同的真实节点（需要承受迁移来的新数据的节点），这就意味着删除节点的数据可能会由多个服务器共同分担，避免了一致性哈希算法那样可能会导致某一个节点压力极速增大的现象。



## 聪明题

### 1000瓶水，其中一瓶有毒，只能测一轮，需要多少只小白鼠？

1. 把水按1-1000的二进制进行编号
2. 从所有最低位为1的水中取出一滴混合成一瓶编号为1的水，从所有倒数第二位为1的水中取出一滴混合成一瓶编号为2的水……
3. 一只小鼠喂一瓶混合的水
4. 死掉的小鼠标为1，没死的标为0，最后得到一个序号就是有毒的水的序号



## 算法

### 寻找两个有序数组中第K大元素

1. 双指针
2. 



### 10G的文件，保存的是32位整数，内存只有1g

1. 怎么找到前100个最大的

   把这个10G文件划分成多个不超过1G的小文件：每个小文件存储不同范围的数字，遍历10G文件，把数字放入对应文件。划分完成后对存储最大数字的那个文件进行排序并取出100个。这个是比较通用的桶排序

   维护小顶堆，把文件中的数字逐个放入小顶堆，当元素超过100个的时候淘汰堆顶元素

2. 怎么找到重复的数字放入

   位图法：10G文件，整数长度为32位，如果所有数字都不重复的话，则最多会有10GB/32b = 320MB个数字，可以用长度为320MB的bitmap来进行标记，一个位代表一个数字，当当前要标记的数字位已经为1时，说明这个数字重复
   
2. 怎么找到出现次数最多的

   桶排序，相同的肯定放在同一个桶里。计算每个桶中出现次数最多的，再比较这些出现次数



## 设计模式

### 单例模式

优点：

- 单例模式保证单例对象只有一个，减少内存开销
- 可以避免对资源的多重占用
- 设置全局访问点，可以优化和共享资源的访问

缺点：

- 单例模式一般没有接口，扩展困难
- 在并发测试中，单例模式不利于代码调试
- 单例模式的功能代码通常都写在一个类中，需要谨慎考虑功能设计的单一职责原则

#### 饿汉式

```java
public class HungrySingleton {
	private static singleton = new HungrySingleton();
	private HungrySingleton() {}
  
	public HungrySingleton getSingleton() {
		return singleton;
	}
}
```



#### 懒汉式

```java
public class LazySingleton {
	private static volatile singleton;
	private LazySingleton() {}
	
	public LazySingleton getSingleton() {
		if(singleton == null) {
			synchronized(LazySingleton.class) {
				if(singleton == null) {
					singleton = new LazySingleton();
				}
			}
		}
		return singleton;
	}
}
```

⚠️双重检查锁+volatile：

1. volatile保证内存可见性+禁止指令重排：创建一个对象有多个步骤，正常的步骤如下：

   1. 加载：如果该类没有加载过的话需要先进行加载。

      - 通过类的全限定类名获取二进制流
      - 将该二进制流的静态存储结构转为方法区的运行时数据结构
      - 在堆中生成类的class对象，作为方法区访问该类的入口

       ➡️ 双亲委派机制

   2. 连接

      - 验证：验证class文件中的字节流信息是否存在安全问题
      - 准备：分配内存，并把这块内存初始化为零值。被final修饰的变量在这一阶段就被显式赋值
      - 解析：符号引用转化为直接引用

   3. 初始化：为变量进行显式初始化

   简单来说就是==**分配内存、初始化、返回引用**==。如果不使用volatile禁止指令重排，可能会按照这个顺序执行：分配内存、返回引用、初始化。假设线程A根据这个顺序来创建单例对象，线程B调用getSingleton方法后发现singleton不为空，直接返回，那么在线程A完成初始化之前，线程B使用这个对象的话就会产生问题

2. 双重判空：

   - 假设只有里面那层判空：那么无论单例对象是否已经创建，所有要获取单例对象的线程都要竞争锁，而如果实际上单例对象已经创建完成了，那么这个线程的加锁和解锁操作是完全没必要的。因此外面那层判空的目的就是减少锁操作，如果单例对象已经创建完成了，那么这个线程无需去获取锁就可以直接拿到单例对象
   - 假设只有外面那层判空：假设线程AB都通过了判空，而线程A先拿到锁创建了一个单例对象，那么线程B之后获取到锁会再次创建单例对象，这样的话单例对象就不是唯一的了。所以里面那层判空就是为了保证多线程环境下单例对象的唯一



#### 应用场景

1. 需要频繁实例化，又频繁被销毁的，比如线程池

2. 对象需要被共享，由于单例模式只允许创建一个对象，共享该对象可以节省内存，并加快对象访问速度

3. 创建实例占用资源多、耗时长、经常使用的

4. Spring中bean的作用域默认都是singleton，即交给Spring容器管理的都默认是单例模式

   > 其他的作用域：
   >
   > - `prototype`原型：每次都会创建一个新的实例对象
   > - `request`：每一次HTTP请求都会创建一个新的对象，该bean仅在当前`HTTP Request`有效
   > - `session`：每一次HTTP请求都会创建一个新的对象，该bean仅在当前`HTTP Session`有效



### 工厂模式

#### 简单工厂模式

如果要创建的产品不多，只需要一个工厂类就可以完成，就称为简单工厂模式。同时因为简单工厂模式中创建实例的方法通常为静态方法，因此简单工厂模式又称为静态工厂模式

优点：

- 免除用户直接创建产品对象的职责，创建产品实例的工作全部交给工厂类，用户无需知道所创建具体产品的类名，只需要传入参数即可
- 可以引入配置文件，在不修改客户端代码的情况下更换和添加新的具体产品类

缺点：

- 工厂类单一，负责所有产品的创建，职责过重
- 系统扩展困难，一旦添加新产品就必须修改工厂逻辑
- 静态工厂造成工厂角色无法形成基于即成的等级结构

```java
public class Client {
    public static void main(String[] args) {
    }
    //抽象产品
    public interface Product {
        void show();
    }
    //具体产品：ProductA
    static class ConcreteProduct1 implements Product {
        public void show() {
            System.out.println("具体产品1显示...");
        }
    }

    final class Const {
        static final int PRODUCT_A = 0;
    }
    static class SimpleFactory {
        public static Product makeProduct(int kind) {
            switch (kind) {
                case Const.PRODUCT_A:
                    return new ConcreteProduct1();
                case Const.PRODUCT_B:
                    return new ConcreteProduct2();
            }
            return null;
        }
    }
}
```



#### 工厂方法模式

工厂方法模式是对简单工厂模式的进一步抽象化，需要定义一个抽象工厂，当需要新增产品的时候只需要编写一个具体的工厂实现类即可。

优点：

- 用户只需要生产该产品的工厂实现类具体名称即可，无需知道产品的具体创建过程
- 扩展方便，新增一种产品，只需要新增一个工厂类即可
- 典型的解耦框架，高层模块只需要知道产品的抽象类，无需关心其他实现类

缺点：

- 类的数量增多
- 抽象产品只能生产一种产品

```java
//抽象产品：提供了产品的接口
interface Product {
    public void show();
}
//具体产品1：实现抽象产品中的抽象方法
class ConcreteProduct1 implements Product {
    public void show() {
        System.out.println("具体产品1显示...");
    }
}
//具体产品2：实现抽象产品中的抽象方法
class ConcreteProduct2 implements Product {
    public void show() {
        System.out.println("具体产品2显示...");
    }
}
//抽象工厂：提供了厂品的生成方法
interface AbstractFactory {
    public Product newProduct();
}
//具体工厂1：实现了厂品的生成方法
class ConcreteFactory1 implements AbstractFactory {
    public Product newProduct() {
        System.out.println("具体工厂1生成-->具体产品1...");
        return new ConcreteProduct1();
    }
}
//具体工厂2：实现了厂品的生成方法
class ConcreteFactory2 implements AbstractFactory {
    public Product newProduct() {
        System.out.println("具体工厂2生成-->具体产品2...");
        return new ConcreteProduct2();
    }
}
```



#### 抽象工厂模式

前面的工厂方法模式只考虑了生产同等级的产品，即一个抽象工厂类的抽象产品参数是同等级的，比如畜牧场只养动物、电视机厂只生产电视机、计算机软件学院只培养计算机软件专业的学生等。抽象工厂模式通过为访问类提供一个创建一组相关或相互依赖对象的接口，使得访问类无须指定所要产品的具体类就能得到同组的不同等级的产品

优点：

- 可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理。
- 当需要产品族时，抽象工厂可以保证客户端始终只使用同一个产品的产品组。
- 抽象工厂增强了程序的可扩展性，当增加一个新的产品族时，不需要修改原代码，满足开闭原则。

缺点：

- 当产品组中需要增加一个新的产品时，所有的工厂类都需要进行修改，增加了系统的抽象性和理解难度

```java
interface AbstractFactory {
    public Product1 newProduct1();
    public Product2 newProduct2();
}

class ConcreteFactory1 implements AbstractFactory {
    public Product1 newProduct1() {
        System.out.println("具体工厂 1 生成-->具体产品 11...");
        return new ConcreteProduct11();
    }
    public Product2 newProduct2() {
        System.out.println("具体工厂 1 生成-->具体产品 21...");
        return new ConcreteProduct21();
    }
}
```



#### 应用场景

1. Spring中的`BeanFactory`和`ApplicationContext`都是工厂模式
   - `BeanFactory`是延迟加载，需要使用的时候才创建对象，程序启动速度快，但是可能会在运行过程中触发NPE
   - `ApplicationContext`是即时加载，程序启动的时候就把所有对象都创建，程序启动速度慢，但是在启动的时候就可以发现配置错误



### 代理模式

当访问一项不适合或者不能直接引用目标对象的时候，需要一个代理对象作为访问对象和目标对象的中介。可以理解为代理对象是一个对目标对象的增强，比如对数据库的操作，在之前需要请求连接，之后需要释放连接，就可以通过代理对象来完成这两个必要的操作。

优点：

- 在客户端与目标对象之间起到一个中介作用和保护目标对象作用
- 可以扩展目标对象的功能
- 把客户端与目标对象分离，在一定程度上降低了系统的耦合度，增加了程序的可扩展性

缺点：

- 增加系统复杂度
- 增加系统设计中类的数量
- 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢

```java
//抽象主题
interface Subject {
    void Request();
}
//真实主题
class RealSubject implements Subject {
    public void Request() {
        System.out.println("访问真实主题方法...");
    }
}
//代理
class Proxy implements Subject {
    private RealSubject realSubject;
    public void Request() {
        if (realSubject == null) {
            realSubject = new RealSubject();
        }
      	// 代理对象的增强
        preRequest();
      	// 目标对象的执行方法
        realSubject.Request();
        postRequest();
    }
    public void preRequest() {
        System.out.println("访问真实主题之前的预处理。");
    }
    public void postRequest() {
        System.out.println("访问真实主题之后的后续处理。");
    }
}
```



#### 应用场景

1. 使用代理模式主要有两个目的：一是保护目标对象，二是增强对象

2. Spring AOP，采用动态代理模式。如果需要被代理的类有实现接口就使用JDK动态代理，如果没有则使用CGLIB动态代理

   根据代理创建的时期来分，有两类：

   - 动态代理

     - JDK动态代理：通过反射来接受被代理的类，并且要求被代理的类必须实现一个接口。核心是InvocationHandler和Proxy类

       > JDK 动态代理的核心实现是一个 `InvocationHandler` 接口的 `invoke()` 方法来实现 AOP 的逻辑。代理对象的获取首先通过目标类的类加载器以及该类的接口去调用 `getProxyClass0()` 获取代理类，这个过程是通过生成字节码文件的方法最终生成一个 `$Proxy0.class`，然后将该字节码文件进行反编译获得代理类。然后通过反射机制获得代理类的构造方法，将 `InvocationHandler` 增强对象传入并且 `newInstance` 创建代理对象实例。这个代理类 `$Proxy0` 继承了 `Proxy` 类并且实现了被代理的接口，因此使用时可以将生成的代理对象实例转换为对应的代理接口，从而实现增强。

     - CGLIB动态代理：CGLIB通过在运行时动态的生成被代理类的子类作为代理，其是通过继承的方式实现的动态代理，如果被代理类使用final修饰（不可被继承），那么是无法使用CGLIB动态代理的

       > CGLIB 动态代理的核心就是 `MethodInterceptor` 接口的 `intercept()` 方法。它生成代理对象的过程是通过 `Enhancer` 来指定要代理的目标对象，以及增强 `Advice` 对象，通过 `create()` 方法生成二进制字节码文件，加载获得 `Class` 对象，通过反射机制获得实例，创建代理类的对象，最后再把这个对象的方法调用转发给 `MethodInterceptor.intercept()` 方法，从而实现增强。CGLIB 生成的代理类是直接继承原来的类，覆盖其中的方法来实现的，因此 `final` 方法和类没办法继承和覆盖，而且 CGLIB 也因此是针对类的代理，不能使用接口的。

   - 静态代理（编译时增强）



### 模板方法模式

即定义一个操作中的总体流程骨架，而将其中一些步骤的具体实现延迟到子类中，使得可以通过定义子类在不改变总体骨架的情况下重新定义某些步骤的具体实现

优点：

- 封装了操作的总体流程，方便扩展可变部分
- 降低代码冗余度

缺点：

- 提高代码复杂度，更加抽象



#### 应用场景

1. 气象局里面的35个任务的实现就是通过模板方法实现的（继承方式）

   最顶层的类的工作是记录任务运行所需的相关信息；第二层是获取需要计算的站点ID或区域ID；第三层是对需要计算的时间范围进行转换；最底层的就是具体的计算类，它的主要工作是计算、存取数据。其中第一二层是所有35个任务共用的模板；第三层是每个时间维度自定义的模板，同一个时间维度的所有气象维度都使用相同的第三层模板。

2. Spring中定义了很多数据库操作模板：如`JdbcTemplate`、`HibernateTemplate`。

   一般模版方法采用继承的方式实现，但是Spring采用`Callback`的方式实现：

   如`StatementCallback`接口内定义了一个方法`doInStatement()`（钩子方法），在这里就无需考虑具体的实现，而是交由数据库模版其内实现子类来实现。

   如`JdbcTemplate`中的`update()`方法就定义了一个实现类`UpdateStatementCallback`，其实现的`doInStatement()`的内容是JDBC更新数据库的操作，然后把该**实现类传给`execute`方法进行回调**，`execute`方法再调用`doInStatement()`方法对数据库进行操作

   >  **为什么采用回调方式？**
   >
   > 对数据库有多种操作方式，如果全部都在父类中定义为抽象方法，那么继承的子类就必须要全部实现，这样就导致子类非常臃肿。采用回调方式的话，子类就可以只定制某个功能的方法，定义内部类实现回调函数，然后把该内部类作为参数传给`execute`方法，由`execute`方法调用回调函数对数据库参数。



### 适配器模式

将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。

优点：

- 通过适配器可以透明的调用目标接口
- 将目标类和适配者类解耦，解决了目标类和适配者类接口不一致问题
- 复用了现存的类，无须修改原有的代码而重用现有的适配者类

#### 应用场景

`SpringMVC`中的适配器`HandlerAdapter`，根据`Handler`规则执行不同的`handler`

实现过程：

1. `DispatcherServlet`根据`HandlerMapping`返回的`Handler`，向`HandlerAdapter`请求`handler`
2. `HandlerAdapter`根据`handler`规则找到对应的`handler`（即`Controller`）并执行
3. 执行完毕后`Handler`会向`HandlerAdapter`返回一个`ModelAndView`
4. 最后由`HandlerAdapter`向`DispatchServelet`返回一个`ModelAndView`。



### 观察者模式

指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式

优点：

- 降低了目标与观察者之间的耦合关系

缺点：

- 当观察者对象很多时，通知的发布会消耗很多时间，影响程序的效率

```java
//抽象目标
abstract class Subject {
    protected List<Observer> observers = new ArrayList<Observer>();
    //增加观察者方法
    public void add(Observer observer) {
        observers.add(observer);
    }
    //删除观察者方法
    public void remove(Observer observer) {
        observers.remove(observer);
    }
    public abstract void notifyObserver(); //通知观察者方法
}
//具体目标
class ConcreteSubject extends Subject {
    public void notifyObserver() {
        System.out.println("具体目标发生改变...");
        System.out.println("--------------");
        for (Object obs : observers) {
            ((Observer) obs).response();
        }
    }
}
//抽象观察者
interface Observer {
    void response(); //反应
}
//具体观察者1
class ConcreteObserver1 implements Observer {
    public void response() {
        System.out.println("具体观察者1作出反应！");
    }
}
//具体观察者1
class ConcreteObserver2 implements Observer {
    public void response() {
        System.out.println("具体观察者2作出反应！");
    }
}
```



#### 应用场景

1. 对象间存在一对多的关系，一个对象的状态发生改变会影响其他对象
2. epoll，当epoll_wait就绪队列不为空的时候，就通知分配线程来读取数据
3. Spring事件驱动模型
   - **事件**：`ApplicationEvent`，继承自JDK的`EventObject`，所有事件将继承它，并通过`source`得到事件源。
   - **事件发布者**：`ApplicationEventPublisher`及`ApplicationEventMulticaster`接口，使用这个接口，我们的`Service`就拥有了发布事件的能力。
   - **事件订阅者**：`ApplicationListener`，继承自JDK的`EventListener`，所有监听器将继承它。



### 装饰者模式

指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式

优点：

- 装饰器是继承的有力补充，比继承更灵活，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用
- 通过使用/不用装饰器的排列组合，可以实现不同的效果

```java
public static void main(String[] args) {
  Component p = new ConcreteComponent();
  // 装饰前
  p.operation();
  System.out.println("---------------------------------");
  // 装饰后
  Component d = new ConcreteDecorator(p);
  d.operation();
}

//抽象构件角色
interface Component {
    public void operation();
}
//具体构件角色
class ConcreteComponent implements Component {
    public ConcreteComponent() {
        System.out.println("创建具体构件角色");
    }
    public void operation() {
        System.out.println("调用具体构件角色的方法operation()");
    }
}
//抽象装饰角色
class Decorator implements Component {
    private Component component;
    public Decorator(Component component) {
        this.component = component;
    }
    public void operation() {
        component.operation();
    }
}
//具体装饰角色
class ConcreteDecorator extends Decorator {
    public ConcreteDecorator(Component component) {
        super(component);
    }
    public void operation() {
        super.operation();
        addedFunction();
    }
    public void addedFunction() {
        System.out.println("为具体构件角色增加额外的功能addedFunction()");
    }
}

```



#### 应用场景

1. Java中的文件流：在读取文件时，我们经常会使用`new BufferedReader(new FileReader(""));`进行嵌套实例化，这种嵌套调用转型的方式就是一种装饰器模式的体现。如果没有使用这种方式，子类的数目将大幅增长，难以维护。

2. 双重认证：在账号密码输入完成之后，往往需要进行手机短信验证码的双重认证。这个时候就可以考虑使用装饰器模式，在原来逻辑不变的情况下可以扩展登录模块。

   > ```java
   > ImageCaptcha imageCaptcha = new ImageCaptcha(new SmsCaptcha(new LoginComponentImpl()));
   > ```
   >
   > 在Main函数中，我们嵌套调用，**谁要在最前面执行，就把它放到最外层**。我们实例化一个图形验证码的装饰器，并且在其中传入短信验证码装饰器，最后将登录组件的具体实现传入到短信验证码的装饰器中。通过一系列巧妙的嵌套调用，我们成功在不修改原有逻辑的情况下引入了两种认证方式便于后期的拓展。



### 策略者模式

该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。

优点：

- 多重条件语句（if…else、switch..case）不易维护，使用策略者模式可以避免使用多重条件语句
- 可以在不修改原有代码的情况下，灵活增加新算法，方便扩展

```java
//抽象策略类
interface Strategy {
    public void strategyMethod();    //策略方法
}
//具体策略类A
class ConcreteStrategyA implements Strategy {
    public void strategyMethod() {
        System.out.println("具体策略A的策略方法被访问！");
    }
}
//具体策略类B
class ConcreteStrategyB implements Strategy {
    public void strategyMethod() {
        System.out.println("具体策略B的策略方法被访问！");
    }
}
//环境类
class Context {
    private Strategy strategy;
    public Strategy getStrategy() {
        return strategy;
    }
    public void setStrategy(Strategy strategy) {
        this.strategy = strategy;
    }
  	// 传入不同的strategy，调用的就是不同算法
    public void strategyMethod() {
        strategy.strategyMethod();
    }
}
```



#### 应用场景

1. `Java Comparator`接口：通过向`Collections.sort()`和`Arrays.sort()`方法传入不同的`Comparator`比较器，可以实现不同的排序效果

2. `Spring Resource`：该接口提供了更强的资源访问能力，Spring 框架本身大量使用了 `Resource` 接口来访问底层资源。

   Spring 为 `Resource` 接口提供了如下实现类：

   - `UrlResource`： 访问网络资源的实现类。
   - `ClassPathResource`： 访问类加载路径里资源的实现类。
   - `FileSystemResource`： 访问文件系统里资源的实现类。
   - `ServletContextResource`： 访问相对于 `ServletContext` 路径里的资源的实现类.
   - `InputStreamResource`： 访问输入流资源的实现类。
   - `ByteArrayResource`： 访问字节数组资源的实现类。

   这些 `Resource` 实现类，针对不同的的底层资源，提供了相应的资源访问逻辑，并提供便捷的包装，以利于客户端程序的资源访问。

3. `Spring Bean`：Spring实例化Bean有三种方式：构造器实例化、静态工厂实例化、实例工厂实例化。具体实例化Bean的过程中，Spring中角色分工很明确，创建对象的时候先通过 `ConstructorResolver` 找到对应的实例化方法和参数，再通过实例化策略 `InstantiationStrategy` 进行实例化



## 字节码编译过程

主要过程：词法解析--->语法解析--->语义解析--->生成字节码

①词法解析：javac编译器执行字节码编译的第一步。主要任务就是将java源码中的关键字和标示符等内容转换为符合java语法规范的Token序列，然后按照指定的顺序规则进行匹配校验，以便为后续的语法解析步骤做准备。

②语法解析：将匹配后的Token序列整合为一颗结构化的抽象语法树。也就是说，词法解析后的Token序列其实还并不完善，因为这些Token所代表的只是一个对应的单个源码字符集合，还并没有按照指定的语法规则将其相关的一组或者一段Token整合起来。

③语义解析：为没有构造方法的类型添加缺省的无参构造方法/检查任何类型的变量在使用前是否都已经经历过初始化/检查变量类型是否与值匹配...

④生成字节码：是将符合Java语法规范的java代码转换为符合JVM规范的字节码文件。



## 字节产研面经

### 进程和线程的区别

1. 调度：进程是资源分配的基本单位，线程是调度和分配的基本单位
2. 并发性：进程之间可以并发执行，一个进程内的线程之间也可以并发执行
3. 资源：不同进程有自己独立的虚拟地址空间，同一个进程下的所有线程共享这个进程的资源
4. 系统开销：切换进程需要消耗的系统开销比线程大

➡️ 延伸：

- **协程**

  协程是一种用户态的轻量级线程，调度完全由用户控制。协程拥有自己的寄存器上下文和栈，协程切换时，将寄存器上下文和栈保存，切换回来后需要恢复。直接操作栈基本没有内核切换的开销，可以不加锁的访问全局变量，因此上下文切换非常快

- **协程与线程**

  - 一个线程下可以有多个协程，一个进程也可以拥有多个协程
- 线程进程都是同步机制，协程是异步机制
  - 协程能保留上一次调用时的状态，每次过程重入时，就相当于进入上一次调用的状态
- 线程是抢占式的，而协程是非抢占式的。需要用户释放使用权切换到其他协程，因此同一时间其实只有一个协程拥有运行权，相当于单线程的能力
  - 协程不被操作系统内核管理，完全是由程序控制的
- 线程是被分割的CPU资源，协程是组织好的代码流程
  - 线程是协程的资源，但协程不会直接使用线程，协程直接利用的是执行器关联任意线程或线程池

- **为什么进程切换的开销比较大？**

  因为每个进程有自己独立的虚拟地址空间，在程序执行的时候需要把虚拟地址转换到物理地址，为了加速这个转换，操作系统引入了TLB快表，它相当于一个缓存，存储着最近访问过的虚拟地址与物理地址的映射，这样之后再次访问相同的虚拟地址直接到TLB里面取对应物理地址就可以了，速度会快很多。但是虚拟地址空间切换后，这个TLB快表缓存的内容就全部失效了，需要重新进行缓存，新TLB的缓存命中率降低，在程序上体现出来的就是速度变慢。

  > LRU算法淘汰

  而同一个进程下的所有线程共享这个进程的资源，也就是它们共享同一个虚拟地址空间，因此线程切换的时候无需切换虚拟地址空间，TLB仍然有效。

  剩下的两步切换内核栈和硬件上下文是进程和线程都需要做的，差别不大。

- **页表淘汰算法LRU算法实现方案：**

  1. 数组：设置一个定长数组，为每个元素设置一个标志位，这个标志位随着时间自增，当某个元素被访问时，就把其标志位置0。当数组空间不足（需要淘汰）时，就把标志位最大的元素删除

     > 需要不停地维护整个数组的标志位，并且数组删除元素很麻烦，时间复杂度为O(n)

  2. 链表：有序链表，当访问一个元素时，如果其在链表中存在，就把其移到链表末尾，否则直接插入；当需要淘汰时直接删除链头元素即可

     > 检查元素是否存在的时候需要遍历整个链表，时间复杂度为O(n)；删除元素的时间复杂度为O(1)

  3. ==双向链表+哈希表 ➡️ LinkedHashMap==：为了解决仅使用链表的时候查找元素需要遍历整个链表的问题，可以引入哈希表，这样子查找的时间复杂度就是O(1)。当访问一个元素的时候直接通过哈希表找到对应节点，然后把其移到末尾（为什么需要双向链表？因为删除节点是通过哈希表直接定位到的，不像遍历那样可以留个慢一步的指针指向前驱节点用于删除之后的连接，所以需要双向链表来找到前驱节点）

  > Redis中也有使用到LRU算法，不过没有完整的实现。每个Redis Object中有24bit存储了一个低位时间戳（194天数据），对部分键进行取样，然后淘汰其中最近最久未使用的键
  >
  > `Redis`3.0之后又改善了算法的性能，会提供一个**待淘汰候选key的`pool`**，里面默认有16个key，按照空闲时间排好序。更新时从`Redis`键空间随机选择N个key，分别计算它们的空闲时间`idle`，**key只会在`pool`不满或者空闲时间大于`pool`里最小的时**，才会进入`pool`，然后从`pool`中选择空闲时间最大的key淘汰掉。

- **Linux下创建新进程**
  - fork
    - fork函数创建的子进程是父进程的**复制**，得到的父子进程是独立的，具有良好的并发性，但是进程间通信需要专门的机制
    - fork函数调用一次，有两个返回值：对父进程而言返回的是子进程的ID（一个父进程可能有多个子进程，但是没有函数可以使得父进程获取所有子进程ID），对子进程而言返回值是0（子进程可以通过getppid来获取父进程ID）。如果创建失败则返回给父进程-1
    - Linux采用**COW写时复制技术**来进行优化：即当有多个进程访问相同的资源的时候，它们会获取相同的指针指向相同的资源，只有需要修改资源内容的时候，系统才会复制一份专用副本给那个进程，而其他进程看到的资源仍然保持不变。
  - vfork
    - vfork创建的父子进程共享地址空间，而不是复制。因此父子进程的数据是共享的，父子进程间的通信很好解决
    - vfork创建的子进程必须调用exit函数来结束，否则子进程不会结束
    - vfork创建的子进程总会在父进程之前执行
    - vfork函数的出现主要是主要是当年Unix系统没有写时复制技术，所以fork出的子进程即使只是调用exec函数来执行另一个可执行文件，也需要完整的复制父进程的资源，但是实际上完全没有必要复制，这样会造成大量的开销浪费。因此设计了vfork函数来避免这个问题，主要目的就是exec一个新的程序，但是现在有了COW技术，这个函数就渐渐被弃用了



### 线程有独立的程序计数器吗

有，负责记录当前线程执行到的指令行数



### 进程通信，哪个效率高？

- 管道：分为匿名管道和命名管道，匿名管道只能在有亲缘关系的进程之间使用，并且数据只能单向流动；命名管道可以在任意两个进程之间使用，数据可以双向流动。两个都是半双工通信

- 信号：是一种比较复杂的通信方式，它可以在任意时候发送给某一进程，而无需知道进程的状态

  ![image-20220225232547633](https://tva1.sinaimg.cn/large/e6c9d24egy1gzq69nh4ixj20lx0aq0tv.jpg)

- 信号量：是一个计数器，通常用来控制同时访问一个共享资源的进程数量，它常常用来作为一种锁机制

- 共享内存：是最快的进程通信（IPC）的方式，但是需要结合锁机制使用

- 消息队列：克服了信号量承载信息少、管道只能流动无格式字节流、缓冲区大小受限的限制

- Socket：可以实现网络中不同主机的通信



### IO复用

- 阻塞IO：应用进程向内核请求数据，如果数据还没有准备好就阻塞等待，等待到数据后把数据从内核空间拷贝到用户空间后返回
- 非阻塞IO：应用进程轮询内核，不阻塞，一直占用CPU
- IO复用：由一个或几个线程来专门监控fd状态，当有fd就绪的话就分配一个线程来读取数据。这样就避免了大量的线程都把时间耗费在等待数据读取上，也减轻了内存的压力
- 信号驱动IO：像select和poll都是通过轮询来监控fd状态的，fd越多，性能就越差。信号驱动模型只有在有fd准备就绪时才通知监控线程来分配线程读取数据，避免了大量无用的轮询操作
- 异步IO：拷贝数据的工作由内核完成



### epoll的原理

epoll分为三部分，分别是`epoll_create`、`epoll_ctl`和`epoll_wait`。epoll只有在Linux中才有实现，它向Linux申请了一个简易的文件系统。

1. 通过调用`epoll_create`来创建epoll对象，相当于创建了红黑树和就绪链表
2. epoll使用红黑树来存储所有的fd，通过调用`epoll_ctl`来把fd加入到红黑树中进行监控，并建立回调关系。
3. 当fd就续时，就会触发回调函数，把这个fd移到就绪链表中。
4. 所以调用`epoll_wait`获取就绪fd时，并不需要轮询/检查所有的fd，只需要检查就绪链表里面有没有元素即可，所以`epoll_wait`的执行速度是非常快的

延伸 ➡️：

**epoll的两种工作模式**：

1. LT电平触发：是默认的工作模式，当内核通知程序有就绪IO了，如果程序不作任何操作，那么内核仍然会继续通知。
2. ET水平触发：当内核通知程序有就绪IO了，即使程序不作任何操作，内核也不会再次通知



### select、poll、epoll区别

1. 数据结构：select底层使用数组存储fd，长度有限，最大支持1024；poll使用链表存储，长度不限；epoll使用红黑树存储

2. 轮询：select、poll都需要把所有fd复制到内核，并在内核态下遍历寻找是否有fd就绪，时间复杂度为O(n)，fd越多性能越差；而epoll只需要调用`epoll_wait`函数检查就绪链表中是否有元素，有的话直接取出就绪fd，就绪fd是通过回调函数自动从红黑树移到就绪链表中的，`epoll_wait`的时间复杂度为O(1)

   > select、poll在空闲时会阻塞，当有IO事件发生时就把所有fd拷贝到内核进行无差别轮询来寻找就绪的fd，而epoll避免了这种无差别轮询

3. 拷贝：每次调用select、poll都需要把整个fd集合拷贝到内核态进行无差别轮询；而epoll把fd集合维护在内核态下，只有第一次调用`epoll_ctl`函数把fd加入红黑树时，才会进行一次拷贝，之后不会拷贝

4. epoll只在Linux有实现

5. 适用场景：select、poll适用于fd较少、并且都比较活跃的情况；epoll适用于fd较多、并且相对没那么活跃的情况

   > 因为epoll的通知机制需要很多回调函数，所以当fd较少的时候可能select、poll的性能反而更好一点



### 用户态与内核态的区别

- 处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的
- 处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的



### TCP与UDP的区别

1. 基本特点：
   - TCP：有连接、可靠、一对一、面向字节流
   - UDP：无连接、不保证可靠交付（尽最大努力）、一对一/多对一、面向数据报
2. 速度：TCP有超时重传、拥塞控制的机制，速度相对较慢；UDP没有这些，只是尽最大努力交付数据报，速度相对较快
3. 适用场景：TCP适用于可靠性要求较高的；UDP适用于实时性要求较高的，如会议等

延伸 ➡️：

- **TCP三次握手及原因**

  1. 客户端选择一个序号`seq=x`，向服务端发送连接请求报文（SYN包），进入`SYN-SENT`（连接已发送）状态
  2. 服务端收到1后，选择一个序号`seq=y`，并设置`ack=x+1`，向客户端发送SYN-ACK包，进入`SYN-RECV`（连接已收到）状态
  3. 客户端收到2后，设置`ack=y+1`，向服务端发送ACK包，TCP连接建立完成，进入`ESTABLISHED`状态
  4. 服务端收到ACK包后进入`ESTABLISHED`状态

  原因：

  1. 告知对方自己的初始序号，并得知对方的确认号：如果只是两次握手，由于客户端不会向服务端发送ACK包，因此服务端无法得知客户端的确认号

  2. 确认双方的收发能力都正常：如果只是两次握手，无法确认服务端的发送能力和客户端的接收能力是否正常

  3. 防止已经过期的连接请求报文重传到服务器，建立了虚假的TCP连接，造成资源浪费：如果只有两次握手，假设客户端之前发送过一个连接请求报文A，但是其由于某种原因迟迟不到达服务端，因此客户端重传连接请求报文B并顺利与服务端建立连接，之后释放连接，服务端和客户端都进入`CLOSED`状态。

     之后这个报文A又到达了服务端，服务端回应了ACK包并直接进入`ESTABLISHED`状态，即服务端单方面认为TCP连接建立完成了。但是客户端是处于`CLOSED`状态，它并不会回应这个ACK包，也不会发送数据给服务端，因为在它的角度连接早就释放了。因此服务端一直在做没有意义的等待客户端发送数据，直到其不得不主动关闭连接。如果网络环境十分差的话，维护大量的虚假连接，服务端很有可能会崩溃

     > 如果服务端2小时都没有收到客户端发送的数据，就会每隔75s向客户端发送一个探测报文，如果连续发送10次客户端都没有回应的话，服务端就会主动关闭这个连接。如果之后客户端再发送数据的话，服务端会返回一个RST包表示连接异常关闭

- **TCP四次挥手及原因**

  1. 客户端设置`seq=u`，`ack=k`，向服务端发送连接释放请求报文（FIN-ACK包），进入`FIN-WAIT-1`（终止等待1）状态
  2. 服务端收到1后，设置`ack=u+1`，向客户端发送ACK包，进入`CLOSE-WAIT`（关闭等待）状态
  3. 客户端收到2后，进入`FIN-WAIT-2`（终止等待2）状态，客户端之后不再发送数据给服务端，但是仍然要接收服务端发送的数据
  4. 服务端发送完数据后，设置`ack=u+1`，向客户端发送FIN包，进入`LAST-ACK`（最后确认）状态
  5. 客户端收到4后，向服务端发送ACK包，进入`TIME-WAIT`（时间等待）状态等待2MSL
  6. 服务端收到5后，进入`CLOSED`状态
  7. 等待2MSL后客户端进入`CLOSED`状态，连接关闭完成

  原因：

  主要是客户端请求连接释放的时候，服务端可能还有数据需要发送。不像客户端一样可以把FIN和ACK合并发送，服务端的FIN和ACK是分开发送的：

  - 如果收到客户端发来的FIN-ACK包不立即回应的话，客户端会误以为自己的包没有顺利到达服务端，就会不断进行重传，这样十分浪费资源
  - 收到客户端发来的FIN-ACK包时服务端可能还有数据要发送，因此服务端只能先回应ACK包，等自己的数据发送完成之后再发送FIN释放连接

- **TCP拥塞控制机制**

  1. 慢开始：窗口值从1开始，呈指数级增长，每一个传输轮次窗口值翻倍
  2. 拥塞避免：当窗口值到达门限值ssthresh后，进入拥塞避免阶段，窗口值加法增大，每一个传输轮次窗口值+1
  3. 快重传：当连续收到3个重复的ACK包时，就认为网络发生了拥塞，不等到超时重传，直接进行重传
  4. 快恢复：执行完快重传算法后执行快恢复算法，门限值和窗口值都调整为当前窗口值的一半，之后执行拥塞避免算法

  > 以上说的这个拥塞控制机制过程是TCP Reno版本的；TCP Tahoe版本只有慢开始、拥塞避免、快重传三种算法，快重传之后门限值减半、窗口值调整为1并执行慢开始阶段，现在已经废弃

- **TIME_WAIT原因**：下面有这个问题，不写了

- **TCP Fast Open**

  常规情况下，三次握手中前两个发送的包是不能携带数据的，最后一个客户端发送的ACK包可以携带数据一起发送，因为此时客户端已经是`ESTABLISHED`状态了，对客户端来说连接已经建立完成。在Linux3.7版本之后，提供了TFO功能，目的是绕开三次握手的过程直接发送数据，从而减少三次握手带来的时延，要使用TFO功能，需要客户端和服务端都支持：

  1. 客户端发送SYN报文，并带上Fast Open选项（里面的Cookie为空），表示客户端向服务端请求Fast Open Cookie
  2. 服务端收到这个SYN报文后，先检查自己是否支持TFO，如果支持的话，就生成一个Cookie，并放在SYN-ACK报文中一起返回给客户端
  3. 客户端收到后把Cookie保存好，发送ACK包给服务端

  > 以上基本是三次握手的过程，只不过多了一个Fast Open Cookie的传输，TFO要起作用首先要经历一次三次握手得到Fast Open Cookie

  4. 之后如果客户端和服务端需要再次建立连接，客户端只需要**在第一次握手的时候把之前缓存的Fast Open Cookie放在Fast Open选项里，并携带应用数据发送SYN包给服务端**
  5. 服务端发现客户端发来了Cookie，则会对Cookie进行校验，如果Cookie无效，则丢弃客户端发来的应用数据，进行正常的三次握手操作；如果Cookie有效，则收下应用数据并在SYN-ACK包中进行确认
  6. 客户端通过服务端的回应来判断刚刚发送的Cookie是否生效

- **TCP四个计时器**

  1. 重传计时器：用于超时重传，发送报文后开始计时，到时后就进行超时重传
  2. 坚持计时器：当收到对方发来的零窗口通知报文后，就会停止发送数据，直到收到对方的窗口大小的通知（意味着可以继续发送了）。但是这个通知报文可能会丢失，发送方如果没收到通知就一直不会发送数据，而接收方认为自己已经成功通知对方了，一直等待对方发送数据。为了避免这样的死锁局面，当一方收到零窗口通知后，就会设立一个坚持计时器，坚持计时器到时后就发送一个窗口探测报文，检查对方是否仍然是零窗口
  3. 保活计时器：当服务端连续2小时没有收到客户端发送的数据，就会每隔75s发送一个探测报文，如果连续发送10次都没有得到回应，就关闭连接
  4. 等待计时器：用于2MSL等待

- **粘包、拆包问题**

  由于TCP是面向字节流的，没有消息边界（UDP面向数据报，因此不会产生粘包拆包问题）。TCP把多个数据量较小的请求合并为一个请求进行发送，这就是粘包；TCP把一个数据量大的请求拆分为多个小请求进行发送，这就是拆包。

  解决方案：

  1. 把每个包都封装成固定的大小，如果不足则通过补0或空等进行填充到指定长度

     > 对于高并发、数据量大的场景，应该尽量避免传输无用的数据，因此这种方式不太可取

  2. 在每个包末尾使用特定的分隔符

  3. 把消息分为头部和消息体，头部内含有消息的长度，只有读取到指定长度后才是完整的消息

  4. Netty提供了一些解码器解决粘包拆包问题：

     - `LineBasedFrameDecoder`：以行为单位进行数据包的解码；
     - `DelimiterBasedFrameDecoder`：以特殊的符号作为分隔来进行数据包的解码；
     - `FixedLengthFrameDecoder`：以固定长度进行数据包的解码；
     - `LenghtFieldBasedFrameDecode`：适用于消息头包含消息长度的协议（最常用）；

- **流量控制和拥塞控制有什么区别**

  - 流量控制是端到端的控制，防止发送速度过快，接收端接收不过来的情况，原理是改变滑动窗口的大小
  - 拥塞控制是防止过多的数据注入到网络中造成网络拥塞

- **糊涂窗口综合症**

  糊涂窗口综合症指发送端应用进程产生数据很慢、或者接收端应用进程消耗数据很慢，导致应用进程之间发送的数据很少，极端情况下，有效载荷（数据）可能只有1字节，但是必要的报文开销却有40字节（20IP+20TCP），严重影响效率，浪费了网络带宽

  - 发送端的优化-Nagle算法：如果包长度达到MSS或包含FIN、设置了TCP_NODELAY选项，则允许发送；否则等待包长度达到MSS才发送

    > MTU为IP数据包的最大长度，在以太网中为1500B，MSS为数据部分的最大长度，为1460B（IPv4）或1440B（IPv6）

  - 接收端的优化

    - Clark：收到数据后立即发送确认，但是宣布窗口为零，直到缓存空出一半
    - 延迟确认：收到后不立即发送确认，延缓了发送方滑动窗口的移动速度，减少了通信量。但是可能会引起超时重传

- **怎么实现UDP可靠**

  由于UDP这个运输层协议本身并不提供可靠传输，因此需要在应用层来实现：

  1. 添加seq/ack机制，确保数据到达对端
  2. 添加发送和接收缓冲区，以便重传
  3. 添加超时重传机制

  实现：

  - RUDP：提供一组数据服务质量增强机制，如拥塞控制的改进、重发机制及淡化服务器算法等，在不干扰协议的实时特性的同时，可靠 UDP 的拥塞控制机制允许 TCP 方式下的流控制行为。
  - RTP：有效负载识别，序列编号，时间戳和投递监听，对于下层协议是独立的
  - UDT：UDT的主要目的是支持高速广域网上的海量数据传输，而互联网上的标准数据传输协议TCP在高带宽长距离网络上性能很差。UDT建于UDP之上，并引入新的拥塞控制和数据可靠性控制机制。UDT是面向连接的双向的应用层协议。它同时支持可靠的数据流传输和部分可靠的数据报传输

- **UDP洪水DoS攻击**

  UDP洪水攻击是一种拒绝服务攻击，它通过发送大量的UDP数据包到目标服务器，尝试消耗其性能，从而导致其对正常流量的拒绝服务。

  大多数操作系统部分限制了ICMP报文的响应速率，以中断需要ICMP响应的DDoS攻击。这种缓解的一个缺点是在攻击过程中，合法的数据包也可能被过滤。如果UDP Flood的容量足够高以使目标服务器的防火墙的状态表饱和，则在服务器级别发生的任何缓解都将不足以应对目标设备上游的瓶颈。



### DNS用的哪个协议？能用TCP吗？

- 区域传送时使用TCP：辅域名服务器会定时向主域名服务器查询，如果数据变动，则需要一次区域传送，即数据同步，这个过程使用TCP
- 域名解析时使用UDP



### TCP保证可靠性的措施

1. 校验和：TCP在计算检验和时，会在TCP首部加上一个**12字节的伪首部**。检验和总共计算3部分：TCP首部、TCP数据、TCP伪首部
2. 序列号与确认应答机制：通过三次握手双方确定了各自的序号和确认号，当收到一个包时，需要对这个包发送确认
3. 超时重传机制：如果没有按时收到ACK包，就认为数据丢失，自动进行重传
4. 连接管理：三次握手和四次挥手
5. 流量控制：控制发送速率，避免发送过快接收端接受不过来导致接收缓冲区溢出
6. 拥塞控制：控制窗口大小避免过多数据发送到网络中造成网络拥塞



### TIME_WAIT状态，哪一端会出现这个状态？为什么要等待2MSL

客户端（请求释放连接方）会出现这个状态。如果客户端发送完最后一个ACK包后直接进入CLOSED状态，如果这个ACK包在网络中丢失了，没有顺利到达服务端，那么服务端就会超时重传FIN包，但是处于CLOSED状态的客户端是不会理这个FIN包的，这样就导致服务端这边的连接无法顺利关闭。

可以认为在2MSL之后之前发送的所有数据都已经离开网络，即之前发送的ACK包要么已经顺利到达服务端，否则就会收到服务端超时重传的FIN包，这样客户端就能重发ACK包。

延伸 ➡️：

**如何优化高并发TCP连接中产生的大量TIME_WAIT状态**

如果系统中存在大量的TIME_WAIT状态连接，可能会导致系统无法创建新的连接。

1. 调整系统内核参数，缩短等待时间，但是这个有风险
2. 把短连接转化为长连接



### HTTP协议请求过程

1. DNS域名解析
   1. 浏览器搜索自己的DNS缓存（浏览器维护着一张域名和IP的缓存映射表）
   2. 如果没有，就搜索操作系统中的DNS缓存
   3. 搜索操作系统的hosts文件
   4. 操作系统把域名发送到本地域名服务器，本地域名服务器收到后查询自己的DNS缓存（这个过程是递归查询方式）
   5. 本地域名服务器向根域名服务器发起请求（这个过程是迭代查询方式），根域名服务器会返回顶级域名服务器的地址（其并不存储域名的具体信息）
   6. 本地域名服务器向顶级域名服务器发起请求（迭代查询方式），顶级域名服务器会返回权限域名服务器的地址
   7. 本地域名服务器向权限域名服务器发起请求，权限域名服务器返回IP地址
   8. 本地域名服务器 ➡️ 操作系统 ➡️ 浏览器
2. TCP三次握手
3. 发起HTTP请求
4. 响应HTTP请求
5. 浏览器渲染页面

延伸 ➡️：

- **HTTP几个版本的区别**

  - HTTP1.0与HTTP1.1

    1. 长连接：HTTP1.0默认为短连接，HTTP1.1默认开启Connection:Keep-Alive开启长连接

    2. 缓存处理：HTTP1.0使用Pragma+Expires（绝对过期时间）的方式管理缓存，通过设定的过期时间与本地时间进行比较来判断缓存是否过期（如果本地时间不正确则可能达不到预期的效果）；HTTP1.1使用Cache-Control来进行缓存控制，在制定的max-age时间（相对过期时间）内缓存有效

    3. 新增请求方法：HTTP1.0只支持GET、POST、HEAD等；HTTP1.1新增了PUT、PATCH、DELETE等

    4. 新增响应码：比如410-资源永久删除等

    5. 断点续传：HTTP1.0不支持断点续传，传输的对象必须是完整的；HTTP1.1支持断点续传，如果传输中断了，下次可以从中断处开始继续传输

       > 请求可通过range头域请求部分数据，响应为206说明数据还没有传输完

    6. 请求部分对象：HTTP1.0必须请求完整的对象；HTTP1.1可以通过制定Content-Range请求部分对象，响应码是206Partial Content

    7. Host头处理：HTTP1.0认为一个IP地址只会有一个主机，因此不传输host；随着技术发展，一个IP地址可能对应多台主机，因此HTTP1.1支持host头域，请求时需要指定host，否则会返回400Bad Request

  - HTTP1.1与HTTP2.0

    1. 二进制分帧：HTTP1.1基于文本格式解析，由于文本格式多样，因此健壮性很差；HTTP2.0基于二进制解析，并且会把传输信息分为多个二进制分帧

    2. 头部压缩：HTTP1.1中，只对消息体进行了压缩，状态行和头部并没有进行压缩；HTTP2.0对头部也进行了压缩，并且通讯双方都各自缓存一份头部字段表，既避免了重复头部的传输，又减小了需要传输的数据

    3. 多路复用：HTTP1.1依赖多个TCP连接来进行多流并行；HTTP2.0采用多路复用，多个HTTP请求可以共用一个TCP连接，数据可以被拆分成多个互不依赖的帧，在不同的流中并行、乱序发送，然后接收方再根据序号进行组装，既可以减少消息交互往返的时间，又可以避免创建新连接造成的延迟，使得TCP效率更高

       > HTTP2.0的这个特点解决了HTTP层面的队头阻塞问题，但是由于底层使用的还是TCP，所以并没有完全解决，而且这个方法在一定程度上恶化了TCP方面的队头阻塞问题；HTTP1.1提出过管道化，但是实现难度过大，这只是一个设想

    4. 服务器推送：HTTP1.1中服务端要发送消息必须先经过客户端的请求；HTTP2.0中服务端可以在没有请求的情况下主动向客户端发送消息

  - HTTP3.0

    1. 弃用了TCP协议，使用基于UDP协议的QUIC协议 ➡️ 彻底解决了HOLB问题
    2. 0RTT建链：HTTPS协议需要3RTT；而HTTP3.0通过缓存连接数据，在非首次连接的时候可以实现0RTT建链
    2. 连接迁移：现在移动设备连接网络的话IP地址会频繁变动，而之前的HTTP版本一旦IP变化了，就需要断开重连；HTTP3.0的QUIC层实现了连接迁移功能，允许移动设备的IP地址改变后，只要仍然保有上下文信息（比如连接ID、TLS密钥等）即可以复用原连接。
    2. 向前纠错：每次传输数据的时候还会传输一部分其他的冗余数据，这样当发生丢包的时候就可以借助这些冗余数据组装
    2. 内置安全性：QUIC 初始握手合并了 TCP 的典型三次握手和 TLS 1.3 握手，提供了端点的身份验证和密码参数的协商

- **队头阻塞问题**



### HTTPS协议请求过程

1. DNS解析

2. TCP三次握手

3. 浏览器向服务端的443端口发起请求，携带自己的SSL/TLS版本、支持的加密算法、生成的随机数

4. 服务器收到请求后，会确认自己的SSL/TLS版本，如果不支持就直接关闭此次连接；如果支持的话就从中选择一种加密算法，从CA认证机构拿到证书，自己保留私钥，向客户端发送响应：SSL/TLS版本、选择的加密算法、证书（含公钥）、生成的随机数

5. 客户端收到后先检验证书的有效性，如果证书有效，则随机生成一个随机数，并用服务端发来的证书进行加密，然后发送给服务端

   > 这个随机数就是对称加密的密钥

6. 服务端收到后，使用自己保留的私钥对加密后的随机数进行解密，之后双方进行数据传输都是使用这个随机数对数据进行对称加密。

7. 服务端使用对称加密密钥对数据进行加密传输

> 对数据进行对称加密，对称加密的密钥使用CA证书中的密钥对进行RSA非对称加密

延伸 ➡️：

- **TLS1.2与TLS1.3**

  - ==握手缩短==：上面说的过程是TLS1.2的过程，需要**2RTT**；TLS1.3缩短到了**1.5RTT**：

    1. 浏览器发送请求，携带支持的椭圆曲线类型，并且对每个自己支持的椭圆曲线类型都计算一个公钥

    2. 服务端向CA拿到证书，选择一个椭圆曲线参数根据私钥生成一个公钥，并给证书签个名，返回给客户端：选择的椭圆曲线参数、公钥、证书（签名）。然后使用自己的私钥和客户端的公钥计算出主密钥

    3. 客户端收到后获得服务端选择的椭圆曲线参数，从之前自己生成的公钥中选择对应的，然后使用自己的私钥和服务端的公钥计算出主密钥

       > 双方计算出来的主密钥是一致的

  - ==0-RTT==：TLS1.3添加了0-RTT模式，在双方已经经历过一次完整的握手后，再次握手的时候允许客户端在第一次发送就可以携带数据，并且需要附上一个预共享密钥PSK。如果服务端验证这个PSK没有问题就收下数据，否则需要走正常的握手流程

    > 但是，0-RTT牺牲了前向安全

  - ==前向安全==：TLS1.3废除了RSA算法，因为其不保证前向安全，服务器的私钥一旦泄漏，那么对称加密的密钥就可以被破解，之前通过对称加密传输的数据就可以被破解了。TLS1.3采用了DH算法，双方交换公钥，并且使用自己的私钥+对方的公钥构造本地主密钥，构造出来的主密钥在两端是一样的，这个主密钥就是之后对称加密的密钥，之后双方可以使用这个本地保留的密钥进行数据的加密和解密。

- **DH算法**

  RSA非对称加密有一个缺点，就是不支持前向保密。也就是说服务器的私钥一旦泄漏，那么对称加密的密钥就可以被破解，那么所有通过对称加密传输的数据都可能会被第三方破解了。

  为了解决这个缺点，提出了DH算法，即浏览器和服务端各自都产生一个密钥对，并交换公钥，然后双方通过自己的私钥+对方的公钥构造本地密钥，构造出来的密钥是一样的，之后双方就可以通过本地密钥进行数据的加密和解密。因此即使公钥泄漏了，也不会导致密钥被第三方解密

- **什么是中间人攻击**

  中间人攻击指第三方（中间人）劫持客户端发送的请求，伪造成CA认证机构给其颁发证书，然后又伪装成客户端把数据转发给服务端，这样子之后客户端和服务端之间的数据传输都会经过中间人的转发。又因为这个证书是中间人自己伪造的，它持有着证书中的私钥，因此可以对数据进行解密。因此HTTPS强烈建议使用CA认证机构颁发的证书，就是为了避免中间人攻击。

- **只有CA认证机构可以生成证书吗**

  一般来说，浏览器只会提示安全风险，并不限制网站的访问，所以不一定只有CA认证机构才能生成证书

- **HTTPS会被抓明文包吗**

  HTTPS传输的数据都是对称加密过的，因此抓包工具抓到的HTTPS的包都是加密状态的。但是由于不一定只有CA认证机构才能生成证书，因此我们可以合理的利用中间人攻击的模式来进行抓包。即在用户授权的情况下，组建中间人网络：抓包工具生成一个证书给客户端，客户端选择接受，之后抓包工具就相当于中间人攻击的中间人，截获客户端与服务端之间传送的数据。而由于证书是抓包工具颁发的，因此抓包工具是有能力对数据进行解密的。所以HTTPS并不能防止抓包，它的安全的意义是防止用户在不知情的情况下被劫持操作，对于像抓包这种授权操作是不提供防护的

- **HTTPS可以保证服务端和客户端都是可信的吗**

  HTTPS单向认证只保证了服务端是可信的，并没有保证客户端是可信的。如果想要保证双方都是可信的，就要进行双向认证：

  1. 客户端向服务端发送建立连接请求
  2. 服务端返回证书
  3. 客户端认证证书，成功后取出证书中的公钥，发送自己的证书给服务端
  4. 服务端认证证书，成功后取出证书中的公钥，加密选择的加密算法发送给客户端
  5. 客户端使用自己的私钥解密获得加密算法，生成一个随机数，使用服务端的公钥加密后发送给服务端
  6. 服务端使用自己的私钥解密获得对称加密的密钥

  > 交换证书 ➡️ 交换公钥，使用对方的公钥进行加密，自己的私钥进行解密



### HTTP状态码

<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1gzr92fq0jwj20k9088aak.jpg" alt="image-20220226214813998" style="zoom:80%;" />

- 200 - 请求成功
- 204 - 服务器接收的请求已成功处理，但在返回的响应报文中不含实体的主体部分。另外，也不允许返回任何实体的主体。（客户端发送数据给服务端，但服务端不需要返回任何数据）
- 206 - 范围请求，响应报文中包含了Content-Range指定范围的实体内容
- 301 - 资源（网页等）被永久转移到其它URL
- 302 - 临时重定向
- 403 - 不允许访问
- 404 - 请求的资源（网页等）不存在
- 500 - 内部服务器错误



### MySQL索引类别

1. B树索引（非聚簇索引）、B+树索引（聚簇索引）：支持范围查找和统配查找

2. 哈希索引：等值匹配效率很高

   > 哈希索引无法进行范围查询、不能进行排序、不支持多列联合索引的最左匹配规则，并且如果键值大量重复的情况下，哈希碰撞严重，哈希索引效率急速下降

3. 全文索引：查找文本中的关键字，针对于Text、Blob这种大文本类型（对英语比较有效，因为本身就已经进行分词了，但是对于中文还是需要借助ES、Slor的站内索引引擎）

   > MySQL的全文索引实际上非常鸡肋，在实际使用中，缓存使用Redis或者Mongodb实现，查询大文本中的字符用ES或者Solr实现。

4. 普通索引：一个索引只包含一个列，没有唯一性限制，允许为NULL

5. 唯一索引：索引列的值必须唯一，允许有空值

6. 复合索引：一个索引可以包含多个列

7. 主键索引：值唯一，不可为NULL，一张表只能有一个

4. 聚簇索引与非聚簇索引

延伸 ➡️：

- **什么是索引下推（5.6引入）**

  假设查询条件为`WHERE A AND B`，普通的查询步骤如下：

  1. 根据条件A找到对应的索引
  2. 回表
  3. 过滤不符合条件B的数据

  可以发现，有一部分回表的数据实际上是不符合要求的，而回表是随机IO，效率比较慢，我们应该尽可能的减少回表。如果该索引为AB的联合索引，则可以进行索引下推：

  1. 根据条件A找到对应的索引
  2. 根据条件B过滤
  3. 回表

  索引下推后，回表的所有数据都是符合条件的数据

- **为什么需要注意联合索引的顺序**

  在联合索引中，想要命中索引，就要严格按照建立索引时候的顺序挨个写条件。假设联合索引ABC，那么索引的排序为：先根据A排序，在A相同的数据中再根据B排序，在B相同的数据中再根据C排序。所以索引中A不同的数据下B是乱序的，如果是按照BC查找，那么就不会走联合索引。因此我们建立联合索引的时候，应该把选择性大的列放在前面

- **最左前缀原则**

  首先要明确的是，这个概念是针对于联合索引的，因为只有联合索引才有左右之分。检索数据时会从最左边（建立联合索引时的第一个列）开始一直向右匹配，直到遇到范围查询位置。

  假设一个联合索引ABCD：

  - 查询条件`WHERE A=? AND B=? AND C>? AND D=?`，那么ABC都会走索引，D不会走索引

    > 等值匹配和in可以乱序，也就是说如果是`WHERE B=? AND A=? AND C>? AND D=?`也是同样的结果，因为MySQL的查询优化器会自动把顺序调整为联合索引建立的顺序

- **为什么建议使用官方自增主键作为索引**

  主要是出于维护索引成本的考虑，当新增一条数据时，也要向索引树中插入数据。如果不使用自增的话，可能会需要在中间插入，引起多个页表的分裂；但是如果使用自增的话，一定是在索引树的最后一个页表中插入数据，最多只会引起一个页表分裂

- **索引失效的情况**

  1. 使用!=或者<>

     > 这一点和NOT IN不一定就不走索引，如果数据严重分布不均，查询优化器可能会选择走索引，但是如果数据分布均匀，查询优化器更偏向于全表扫描

  2. 类型不一致：假设height为int类型，如果查询条件为height = ‘1’，那么就不会走索引，因为这里面有一个隐式类型转换

  3. 复杂表达式、函数导致索引失效：如DATE(time)、age-1=20，这种就不会走索引，所以查询条件涉及运算的时候要尽量把运算操作放在右边

  4. or引起的索引失效：⚠️，连接同一个字段的or仍然走索引，所以in查询也是走索引的，因为in查询等效于用or连接起来的等值匹配；连接不同字段的or条件就不走索引

  5. 模糊搜索引起的索引失效：如like ‘%ab’，因为索引的排序是从左边开始的

  6. IS NULL走索引，IS NOT NULL不走索引

  7. NOT IN、NOT EXISTS导致索引失效
  
  7. 如果查询优化器认为全表扫描跟走索引的消耗差不多，可能会选择全表扫描
  
- **查看是否有使用索引**

  使用EXPLAIN，其中possible_key表示可能会使用的索引，key表示实际使用的索引。type字段很重要，可以通过这个字段判断此次查询是全表扫描还是索引扫描

  - `ALL < index < range ~ index_merge < ref < eq_ref < const < system`，其中ALL表示全表扫描、index扫描全表索引的覆盖索引、const主键索引或者唯一二级索引进行等值匹配、ref普通的二级索引列与常量进行等值匹配

- **为什么选择B+树作为索引结构**

  - Hash索引等值查询很快，但它是无序结构，无法进行范围查询。但是B+树是一种多路平衡查询树，节点是天然有序的
- 二叉搜索树：可以排序，但是不保证平衡，极端情况下会退化成链表
  - 平衡二叉树：维护成本太高
- 红黑树：通过舍弃一定的平衡度来降低维护成本，但是树高太高，IO次数多
  - B树：每个节点都存储数据，所以B树的节点要比B+树节点大，因此一个数据页中可存储的节点数比B+树小，因此树的高度相对较高；并且需要通过中序遍历进行范围查询，而B+树的数据都存储在叶子结点，并且相邻节点用链表项链，只需要找到起始和结尾节点就可以以遍历链表的方式获取范围查询的数据




### 聚簇索引非聚簇索引区别

聚簇索引的数据和索引存放在一起，找到索引就找到了数据（B+树叶子结点存放的就是数据）；非聚簇索引的数据和索引分开存放，索引里面只存放了索引列的数据，想要访问完整数据需要一个回表的操作

延伸 ➡️：

- **非聚簇索引一定需要回表吗/什么是覆盖索引**

  不一定，如果查询列被索引列完全覆盖，即需要查询的数据在索引中就已经有了，就不需要回表，这就是覆盖索引。（因此不建议select *）



### InnoDB跟MyISAM区别

1. InnoDB支持事务和外键，MyISAM不支持
2. InnoDB不支持全文索引，MyISAM支持
3. InnoDB不存储表的具体行数，只存储了一个大概的数字；MyISAM存储了精确的行数
4. InnoDB支持聚簇索引和非聚簇索引，MyISAM只支持非聚簇索引（索引文件和数据文件分开存放）
5. InnoDB支持行级锁和表级锁，默认为行级锁；MyISAM只支持表级锁，但是MyISAM在仅新增和查询的场景下效率比InnoDB高
5. InnoDB支持崩溃恢复，MyISAM不支持

> 因为MyISAM相对简单且查询效率快，如果系统读多写少、没有事务要求，那么可以选择使用MyISAM；但是如果有事务要求、系统写多读少，尤其是并发写入高的时候，InnoDB是首选

延伸 ➡️：

- **为什么InnoDB不存储准确行数**

  根本原因是InnoDB支持事务，并且对于不同事务来说它们所期待的行数是不一样的。假设有一张原本有100条数据的表，有两个事务A和B，其中事务A先插入一条数据，然后查询总行数；事务B先删除一条数据，然后查询总行数。对于这两个事务来说，它们可见的行数是不一样的。

  - 对事务A来说，其新增了一条数据，并且事务B删除的数据还没提交，对其不可见，因此事务A看到的总行数是101
  - 对事务B来说，同理，事务A新增的数据还没提交，对其不可见，因此事务B看到的总行数是99

- **MySQL锁的类型**

- **InnoDB怎么实现行级锁**

  - 共享锁（S锁）：lock in share mode，当一行数据被加上读锁时，允许其他事务并发读取，但是不允许写入
  - 排他锁（X锁）：for update、update/insert/delete，当一行数据被加上写锁时，其他事务既不能读也不能写
  - InnoDB的行锁是加在索引上的，当没有索引的情况，InnoDB只能使用表锁

- 



### 介绍下MySQL事务

事务是一个不可分割的数据库操作序列，它要么全部执行，要么全部不执行。

ACID基本特性：

- 原子性`Atomicity`：一个事务中的所有操作，要么全部执行，要么全部不执行
- 一致性`Consistency`：事务使得数据库从一个一致性状态转变为另一个一致性状态
- 隔离性`Isolation`：并发执行的各个事务之间不会互相干扰
- 持久性`Durability`：提交的事务对数据的修改是永久的

延伸 ➡️：

- **事务是怎么实现的**

  事务是基于redo log和undo log实现的，需要把事务的所有日志都写入redo log进行持久化（不是提交事务才写入，而是执行过程中就写入），这样子数据库就可以通过redo log实现事务的持久性；每当事务中有修改操作的时候，还会产生对应的undo log，如果需要回滚事务，就执行undo log的反向语句进行回滚，这样就可以实现事务的原子性。
  
  1. 原子性：undolog，当事务中执行了修改数据的操作的时候，会向undolog（逻辑日志）追加一条意义相反的数据，当事务需要回滚的时候，只要执行undolog中的语句即可把数据恢复到事务执行前的初始状态
  
  2. 隔离性：锁机制+MVCC
  
  3. 持久性：redolog+double write
  
     MySQL的数据页大小默认为16K，而文件系统的数据页大小为4K，IO操作是以页为单位进行的。这样就意味着把MySQL的一个脏页刷到磁盘中需要4次IO，假设这4次IO中途数据库宕机了，导致MySQL这一页并没有完全写到磁盘上。当数据库重启之后，发现有数据页不完整，即使有redolog也无法进行恢复。这个就是partial write页断裂问题
  
     > redolog的页大小本身设计为512B，因此其本身不会发生页断裂问题，但是问题在于redolog是物理日志，其记录的是数据页的变化，所以并不能解决页断裂问题
  
     为了解决这个页断裂问题，MySQL引入了double write特性，其含义就是：在将脏数据刷盘的过程中，一份写到了共享表空间，一份写到了真正的数据文件永久保存，即写了两次脏数据。
  
     1. 把脏页顺序的写入内存中的double write buffer（大小为2MB，即128个页，每次写1MB），并调用fsync保证这些数据被刷新到double write磁盘中（即共享表空间）（**顺序IO，效率高**）
     2. 然后再调用fsync把double write buffer中的脏页刷新到实际存储位置（**随机IO，效率低**）
     3. 当崩溃恢复的时候，会先检查数据文件（真正落库的地方）和共享表空间的内容，如果共享表空间中的页处于页断裂状态，则简单丢弃；如果数据文件中的页处于页断裂状态，则从共享表空间中找到副本进行还原
  
     > 因为**共享表空间落盘和数据文件落盘是不同的时机，所以不会出现同时发生页断裂的情况**，从而解决了页断裂问题，保证了redolog能顺利进行，顺利恢复数据库
  
     > fsync：MySQL为了降低IO次数，使用了延迟写，即先把数据写入缓冲区，等到缓冲区满之后才写入到磁盘中。这种方法虽然降低了IO次数，但是也降低了文件的更新速度，因为一次要写入的数据变多了。如果在这个写文件的过程中宕机了，就会导致文件更新内容丢失，因此为了保证磁盘上的实际文件与缓冲区中的内容保持一致，提供了fsync系统调用，其会确保一直到写磁盘结束才会返回，也就是说如果fsync返回了，就可以认为数据已经完整落盘了。（sync系统调用把缓冲区排入写队列，不等其结束就返回了，所以其返回不能保证数据的安全落盘）
  
     - 缺点：写入共享表空间会导致系统有更多的fsync操作，从而影响部分性能，但是由于double write是一个连续的存储空间，即对其是顺序IO，效率比较高，所以影响不大
     - 其他解决方案：==解决页断裂问题的本质就是保证数据页16K的原子写入==。阿里云的polardb，在底层分布式文件系统PolarFS能提供页大小的原子写入，所以就不需要double write机制来避免页断裂问题了
  
- 日志的类型（物理、逻辑）

  - 物理日志：存储的是在某个页面的某个偏移量修改的前后内容

    - 日志是幂等的，重复执行日志不会造成数据不一致问题

    - 可以用于宕机前后事务并发控制的一致性

      > 物理日志本身就是存储就是基于不可分隔的更新操作，因此其存储先后顺序就代表了执行器的调度顺序。而且由于很容易判断两个 page 是否是同一个 page，如果不是，完全可以安全并行地并行执行。

    - 日志重放效率高，但是日志量大（比如一句update语句可能会往物理日志中追加多条日志）

    - redolog是物理日志

  - 逻辑日志：记录对表的操作，可以简单理解为sql语句

    - 日志不幂等，假如存储的操作是非幂等的，比如a+1，可能会造成数据不一致问题

    - 难以保证宕机前后事务并发控制额一致性

      > 由于逻辑日志难以携带并发执行顺序的信息，当同时有多个事务产生更新操作时，数据库内部会将这些操作调度为串行化序列执行，需要机制来保障每次回放操作的执行顺序与调度产生的

    - 日志重放效率低，但是日志量小

    - undolog、binlog是逻辑日志

  - 物理逻辑日志：对应页是物理的，页内部操作是逻辑的




### 隔离级别怎么实现

|           隔离级别           | 脏读 | 不可重复读 | 幻读 |
| :--------------------------: | :--: | :--------: | :--: |
| 读未提交（READ UNCOMMITTED） |  √   |     √      |  √   |
|  读已提交（READ COMMITTED）  |  ✕   |     √      |  √   |
| 可重复读（REPEATABLE READ）  |  ✕   |     ✕      |  √   |
|    串行化（SERIALIZABLE）    |  ✕   |     ✕      |  ✕   |

> Oracle默认的隔离级别为读已提交，MySQL默认的隔离级别为可重复读。
>
> MySQL的InnoDB引擎在可重复读级别已经通过MVCC部分解决了幻读的问题

1. 读未提交：事务更新时，要先对该行加**行级共享锁**，使得其他事务可以并发读取但是不能并发写，直到事务结束才释放该行级共享锁
2. 读已提交：事务更新时，要先对该行加**行级排他锁**，使得其他事务不能并发读写，直到事务结束才释放该行级排他锁。**每次读取的时候都生成一个新的ReadView**，使得每次看到的都是最新提交的数据，所以存在不可重复读问题
3. 可重复读：与读已提交相同，但是**只在第一次读取数据时才生成ReadView**，本事务无法看到本事务执行期间其他事务提交的数据，从而实现了可重复读
4. 串行化：读取前加**表级共享锁**，写前加**表级排他锁**，都是事务结束才释放

- 快照ReadView：MySQL采用MVCC的方式来实现可重复读，ReadView包括以下几个部分：

  - `m_ids`：在生成该ReadView时活跃的事务id列表
  - `min_trx_id`：`m_ids`中的最小值，可以理解为生成该ReadView时活跃的事务中最早的事务
  - `max_trx_id`：生成该ReadView时系统应该分配给下一个事务的id，不一定是`m_ids`的最大值（因为可能在生成之后分配之前有其他事务）
  - `creator_trx_id`：生成该ReadView的事务id

  访问某条记录时，根据下列步骤判断该条记录在当前版本是否可见：

  - `trx_id = creator_trx_id`，这个记录就是由这个事务产生的，因此该条记录在该版本内可见
  - `trx_id < min_trx_id`，表示生成该记录的事务在生成ReadView之前就已经提交，因此该条记录在该版本内可见
  - `trx_id >= max_trx_id`，表示生成该记录的事务在生成ReadView之后开启的，因此该条记录对该版本不可见
  - `min_trx_id < trx_id < max_trx_id`，如果`trx_id`在`m_ids`中，说明为该版本的活跃事务，不可见；否则可见

  > 即在`m_ids`中或者大于`max_trx_id`的不可见
  >
  > 如果该版本对当前事务不可见的话，就顺着版本链用下一个版本继续比较

  ==读已提交级别在事务中每次读取数据都生成一个ReadView，可重复读级别只在第一次读取数据的时候生成一个ReadView==

- 并发写问题解决：如果有索引，则给索引加上行锁；如果没有索引，就把所有行加上行锁，然后进行一遍过滤，发现不满足的行就释放锁，虽然还是最终只有符合条件（需要加锁）的行才加了锁，但是给整个表每一行都加上行锁然后又释放时很耗费性能的。

  > 注意MySQL的设计行锁是加在索引上的，所以即使访问的不是同一行数据，如果使用的是同一个索引，也可能会发生锁冲突

延伸 ➡️：

- **乐观锁与悲观锁**
  - 悲观锁：认为一定会存在并发问题，因此采用加锁的方式来保证对资源的独占。
    - 用来解决读写冲突和写写冲突的加锁并发控制
    - 适用于写多读少、写冲突严重的情况、数据一致性要求高的场景，因为悲观锁是在读取数据的时候就加锁的，读多的场景会需要频繁的加锁和很多的等待时间，而在写冲突的场景下使用悲观锁可以保证数据的一致性
    - 可以解决脏读、幻读、不可重复读、更新丢失问题
  - 乐观锁：假设一般不会产生冲突，通常使用CAS或版本号来进行并发控制
    - 解决写写冲突的无锁并发控制
    - 适用于读多写少的情况、数据一致性要求不高、要求高响应，因为如果出现大量的写操作，写冲突的可能性就会增大，就需要不断重试降低系统性能
    - 无法解决脏读、幻读、不可重复读问题，可以解决更新丢失问题
  - MVCC：通过快照读解决读写冲突的无锁并发控制



### 幻读怎么产生的，MySQL怎么解决

在事务A多次读取过程中，事务B插入了一条新的数据，导致事务A再次读取发现多了一条数据

example：事务A要把所有红色数据都改成蓝色，在A修改完毕提交之前，B插入了一条红色数据，A提交之后发现明明自己已经把数据全部都改了，数据库中还是有一条红色数据

并发写（不可重复读）是通过加行锁来解决的，但是**幻读的产生并不是行锁能解决的，因为其插入的位置是行的间隙。因此MySQL通过间隙锁来解决幻读问题。**

假设有两条记录

<img src="https://pic3.zhimg.com/80/v2-6a7c4b03297309813d57213a2a2d7366_720w.jpg" alt="img" style="zoom:50%;" />

则有三个间隙

<img src="https://pic1.zhimg.com/80/v2-f07516da40274690e8ece9bc415eab88_720w.jpg" alt="img" style="zoom:50%;" />

间隙锁就是加在这三个间隙上面的。

所以如下列情况：

![img](https://pic2.zhimg.com/80/v2-3ce9ac24fb6fdf3ff60d03e070136175_720w.jpg)

在事务A进行操作的时候，**数据库不仅给age=10的行加上了行锁，还给区间(负无穷，10]和(10，30)加上了间隙锁（即需要被行锁锁定的左右的间隙）**，从而导致事务B无法插入，只能等待事务A提交后才插入数据。不仅插入 age = 10 的记录需要等待事务A提交，age<10、10<age<30 的记录页无法完成，而**大于等于30的记录则不受影响，这足以解决幻读问题了，同时也提高了性能**

> 这是age为索引列的情况，如果age不是索引列，那么数据库会为整个表都加上间隙锁，此时即使age>30，事务B也需要等待事务A完成才可以插入

延伸 ➡️：

- **MVCC**

  MVCC，即多版本并发控制，通过保存数据在某个时间点的快照来实现，根据事务的开始时间的不同，不同事务对同一张表看到的数据（版本）可能是不一样的。MVCC最大的好处就是**读不加锁，读写不冲突**，极大的提升了MySQL的并发性，同时保证了事务的隔离性

  > MVCC的快照读在一定程度上规避了幻读，但是没有完全避免

- **MVCC的实现原理**

  InnoDB每一行数据有几个隐藏列：

  - ROW ID：如果没有自己建立主键的话，就会自动生成该隐藏列并根据其生成聚簇索引
  - 事务ID：最后一次修改该条记录的事务ID
  - 回滚指针：指向该条记录的上一个版本

  假设事务1插入了一条数据：

  ![image-20220228161052143](https://tva1.sinaimg.cn/large/e6c9d24egy1gztak104eij20ci02c3yh.jpg)

  此时事务2进行update t1 set b=666 where a=1;则执行步骤如下：

  1. 把a=1的所有行加上排他锁
  2. 把原记录复制到undo log中
  3. 修改记录b=666，事务ID为2
  4. 把记录的回滚指针指向undo log中的原记录
  5. 事务提交，释放排他锁



### 排序算法？哪些是稳定的？为什么需要稳定排序？哪个排序效率比较高

1. 冒泡排序（稳定，时间复杂度O(N^2^)）

   ```java
   public void bubbleSort(int[] arr) {
   	for(int i = 0; i < arr.length - 1; i++) {
       for(int j = arr.length - 1; j > i; j--) {
         // 相邻元素相等时不会交换，因此稳定
         	if(arr[j] < arr[j - 1]) {
             int temp = arr[j];
             arr[j] = arr[j - 1];
             arr[j - 1] = temp;
           }
       }
     }
   }
   ```

   缺陷：当数组已经有序时，仍然需要进行排序直到经历arr.length-1轮，后面的比较是没有意义的，因此增加一个标志位，标记上一轮是否有进行交换，如果没有说明数组已经有序，后面无需再进行排序

   ```java
   public void bubbleSort(int[] arr) {
   	for(int i = 0; i < arr.length - 1; i++) {
   		boolean swap = false;
       for(int j = arr.length - 1; j > i; j--) {
         // 相邻元素相等时不会交换，因此稳定
         	if(arr[j] < arr[j - 1]) {
             int temp = arr[j];
             arr[j] = arr[j - 1];
             arr[j - 1] = temp;
             swap = true;
           }
       }
       if(!swap) {
   	    break;
       }
     }
   }
   ```

2. 选择排序（不稳定，时间复杂度O(N^2^)）

   ```java
   public void selectSort(int[] arr) {
   	for(int i = 0; i < arr.length - 1; i++) {
   		int min = i;
       // 0-i为有序序列，每次从无序序列中寻找最小值插入到有序序列最后面
   		for(int j = i + 1; j < arr.length - 1; j++) {
   			if(arr[j] < arr[min]) {
   				min = j;
   			}
   		}
   		if(min != i) {
   			int temp = arr[min];
         arr[min] = arr[i];
         arr[i] = temp;
   		}
   	}
   }
   ```

   > 数组实现不稳定，链表稳定

3. 插入排序（稳定，时间复杂度O(N^2^)）

   ```java
   public void insertSort(int[] arr) {
   	for(int i = 1; i < arr.length; i++) {
   		int pos = i;
   		int temp = arr[i];
   		// 把元素arr[i]一直向前移，直到遇到第一个小于其的数
   		while(pos > 0 && arr[pos - 1] > value) {
   			arr[pos] = arr[pos - 1];
   			pos--;
   		}
   		arr[pos] = temp;
   	}
   }
   ```

缺陷：插入排序在数组基本有序的时候可以达到O(N)的时间复杂度，但是由于其每次只能移动一个元素，因此在数组较大并且基本无序的时候性能会迅速恶化。

优化：希尔排序，选择一个增量序列（长度为k），对数组进行k趟直接插入排序。每趟直接排序根据增量将待排序列划分为多个子序列

> 希尔排序的增量数列可以任取，需要的唯一条件是最后一个一定为1（因为要保证按1有序）。但是，不同的数列选取会对算法的性能造成极大的影响。

4. 归并排序（稳定，时间复杂度O(NlogN)，空间复杂度O(N)）

   ```java
   public void mergeSort(int[] arr) {
   	int temp = new int[arr.length];
   	split(arr, temp, 0, arr.length - 1);	
   }
   
   private void split(int[] arr, int[] temp, int left, int right) {
   	if(left < right) {
       int mid = (left + right) / 2;
       split(arr, temp, left, mid);
       split(arr, temp, mid + 1, right);
       merge(arr, temp, left, mid, right);
     }
   }
   
   private void merge(int[] arr, int[] temp, int left, int mid, int right) {
   	int i = left;
     int j = mid + 1;
     int k = 0;
     while(i <= mid && j <= right) {
       // 注意这里要有等号才是稳定的，即如果遇到相同的，优先把左侧的放进去以维持它们的顺序
       temp[k++] = arr[i] <= arr[j] ? arr[i++] : arr[j++];
     }
     // 把剩余的元素放入temp
     while(i <= mid) {
       temp[k++] = arr[i++];
     }
     while(j <= right) {
       temp[k++] = arr[j++];
     }
     // 把数据复制回原数组
     for(int i = 0; i < k; i++) {
       arr[left + i] = temp[i];
     }
   }
   ```

5. 快速排序（不稳定，时间复杂度O(NlogN)）

   ```java
   public void quickSort(int[] arr) {
   	sort(arr, 0, arr.length - 1);
   }
   
   private void sort(int[] arr, int left, int right) {
   	if(left < right) {
       // 获得分界点
   		int part = partition(arr, left, right);
       // 递归排序左右子数组
       sort(arr, left, part - 1);
       sort(arr, part + 1, right);
   	}
   }
   
   private int partition(int[] arr, int left, int right) {
   	// 选择最左边的数作为哨兵
   	int temp = arr[left];
   	while(left < right) {
   		// 从右边开始寻找第一个比temp小的元素
   		while(left < right && arr[right] > temp) {
   			right--;
   		}
   		if(left < right) {
   			// 说明找到元素，放到left指针指向的地方
   			arr[left++] = arr[right];
   		}
   		// 从左边开始寻找第一个比temp大的元素
   		while(left < right && arr[left] < temp) {
   			left++;
   		}
   		if(left < right) {
   			arr[right--] = arr[left];
   		}
   	}
   	// 左右指针相遇，把缓存的数填入相遇处（分界点）
   	arr[left] = temp;
   	return left;
   }
   ```

6. 计数排序（稳定）

   ```java
   public void countSort(int[] arr) {
   	// 寻找最大最小值
   	int min = arr[0];
   	int max = arr[0];
   	for(int i = 1; i < arr.length; i++) {
   		min = Math.min(min, arr[i]);
   		max = Math.max(max, arr[i]);
   	}
   	// 对数组元素进行计数
   	int[] count = new int[max - min + 1];
   	for(int i = 0; i < arr.length; i++) {
   		count[arr[i] - min]++;
   	}
   	int j = 0;
   	int i = 0;
   	while(i < arr.length) {
   		while(count[j] > 0) {
   			arr[i++] = j + min;
   			count[j]--;
   		}
   		j++;
   	}
   }
   ```

   > 适用于数据比较集中、极值容易判断的情况

7. 堆排序

8. 桶排序：把数组中的元素按照大小装进不同的桶，然后对桶内元素进行排序。稳定性取决于对桶内元素进行排序选择的算法

9. 基数排序：是桶排序的进展，从元素的低位到高位依次进行桶排序。即：先对元素的个位数进行一次桶排序，然后对元素的十位数进行桶排序，直到最高位比较完

   > 稳定，适用于字符串



### Synchronized和Lock的区别，哪个性能比较差

1. 使用范围：`synchronized`可以加在变量、方法、类上，Lock只能加在代码块上
2. 使用：`synchronized`无需手动获取和释放，发生异常时会自动释放，不会产生死锁问题；Lock需要手动释放锁，发生异常时也不会释放，否则可能会引起死锁的发生
3. 锁的类型：`synchronized`为可重入锁、不可中断、非公平；Lock为可重入锁、可中断（通过`lockInterruptibly()`方法获取）、默认为非公平，可指定为公平锁
4. 底层实现：`synchronized`底层使用指令码方式来控制锁，锁对象为monitor，获取锁指令为`monitorenter`、释放锁指令为`monitorexit`；Lock底层使用CAS实现，依赖AQS
5. 调度：`synchronized`使用Object对象的`wait`、`notify`、`notifyAll`方法进行调度；Lock可以使用`Condition`进行线程之间的调度
6. 判断锁的状态：`synchronized`无法得知锁的状态；Lock可以判断是否获取到锁
7. `synchronized`是JVM层面支持的；Lock是JDK实现，在API层面
8. 性能比较：当竞争较少的时候，`synchronized`性能较高；当竞争激烈的时候，Lock性能较高（`synchronized`性能急速下降，但是Lock能维持在一定水平）

延伸 ➡️：

- **synchronized锁升级机制**

  1. 偏向锁阶段：初始时锁对象（monitor）对象头中的threadId为空，当一个线程过来要使用锁的时候，发现其threadId为空，说明现在锁对象是空闲的。则直接自己占用，并填入自己的线程ID
  2. 轻量级锁阶段：如果又有线程需要获取锁，并且发现锁对象的threadId不为空。则先比较自己的线程ID与锁对象的是否一致，一致的话则不需要进行加锁的操作，直接令锁计数器+1（可重入）；如果不一致，则升级为轻量级锁
  3. 重量级锁阶段：升级为轻量级锁后，线程会不断自旋尝试获取锁，当自旋一定次数仍然失败后，就升级为重量级锁

  原因：重量级锁依赖于操作系统底层的同步函数实现，重量级锁的相关操作需要转换到内核态进行，因此重量级锁开销很大。如果一开始就加上重量级锁，对于锁竞争很小的场景是很浪费的。

- **synchronized与volatile区别**

  1. 本质区别：synchronized本质是只允许一个线程访问；volatile是告诉JVM在寄存器的缓存的值不可信，要到主存中读取，同时写的时候要立即刷新最新值到主存中

  2. 作用范围：synchronized可以添加到变量、方法、类上；volatile只能修饰变量

  3. 保证的性质：synchronized可以保证可见性、原子性；volatile只能保证可见性，不能保证原子性

     > synchronized保证可见性的原理是Java内存模型中的：对一个变量unlock操作之前，必须要先同步到主存中；如果对一个变量进行lock操作，则将会清空工作内存中该变量的值，在使用此变量前要重新到主存中加载

  4. 指令重排：synchronized内的代码可能会被重排；volatile修饰的变量禁止指令重排

  5. 线程阻塞：synchronzied会使拿不到锁的线程阻塞；volatile不会使线程阻塞

- **JVM对synchronized的优化**

  - 锁膨胀/锁升级
  - 锁消除：通过JIT编译消去不必要添加的锁
  - 锁粗化：扩大加锁的范围从而减少加锁和释放锁操作的次数，最常见的就是会把循环内的加锁优化成循环外加锁
  - 自旋锁与自适应自旋锁：自旋锁是自旋指定次数后失败，而自适应自旋锁则是根据上次自旋的次数来动态调整本次的自旋次数

  

### 乐观锁和悲观锁的区别

### 乐观锁的实现

CAS和版本号



### CAS怎么做的？版本号怎么做？

- CAS即Compare And Swap，CAS包含了三个操作数：需要读写的内存值V、旧的预期值A、要修改的更新值B。首先读取内存获取原来的值A，然后尝试原子更新，当且仅当A=V的时候才能更新成功。
- 版本号有两种实现方式：数据版本机制、时间戳机制

延伸 ➡️：

- **CAS存在的问题，怎么解决？**

  - **ABA问题**：假设在线程1读取了值还没有进行原子更新的期间，另一个线程把值修改成别的值然后又修改回原来的值，这样的话V仍然等于A，线程1是无法感知到这个数据实际上已经被修改过了，因此CAS成功。

    可以通过版本号（AtomicStampedReference）来解决，通过比较版本号而不是值就不会存在ABA问题。

    > 版本号存在竞争怎么办？
    >
    > 
  
  - **循环时间长开销**：CAS是靠自旋不断尝试的，并不会阻塞，也就是说CAS期间一直占用CPU。解决方法是限制自旋次数，达到一定次数后返回CAS失败
  
  - **只能保证一个变量的原子操作**：使用互斥锁、把多个变量封装到一个对象中并使用AtomicReference



### ConcurrentHashMap怎么保证并发安全

- JDK1.7把底层数组分成了多个小的Segment，Segment继承了ReentrantLock，即其实际上是一把可重入锁，当数据想要访问的时候需要先获取锁
- JDK1.8取消了Segment，采用了数组+链表+红黑树的数据结构，通过CAS+synchronized实现了更细粒度的加锁（锁住的是哈希桶）

延伸 ➡️：

- **ConcurrentHashMap的put方法需要上锁，为什么get不需要？**

  因为Node的元素val和指向下一个节点的指针next都使用volatile修饰，保证了可见性。

- **ConcurrentHashMap为什么不支持key或者value为null？**

  因为ConcurrentHashMap是支持并发的，如果支持为null的话，那么就无法判断取出来的null值是代表没有这个key还是这个key对应的value为null。

  对于HashMap来说，这个可以通过containsKey()方法来判断含义。但是对于ConcurrentHashMap来说，假设现在get方法返回的null值的含义是没有这个key，那么使用containsKey()方法应该返回的是false，但是如果在get方法之后，containsKey方法之前，另一个线程插入了key，那么containsKey方法就会返回true，与预期的结果不一致。

  - HashMap对象的key、value值均可为null
  - HahTable对象的key、value值均不可为null

- **ConcurrentHashMap的迭代器是强一致性还是弱一致性**

  弱一致性。ConcurrentHashMap的迭代器在创建出来之后，就会遍历每个元素，假若遍历过程中发生了修改，如果修改的元素是已经遍历过的，那么不会反映出来；如果修改的元素还没有被遍历到，那么在之后的遍历就会反应过来。这样子既可以保证迭代器可以使用老的数据，写线程也可以并发完成修改，从而提高了ConcurrentHashMap的性能。

  而HashMap的迭代器是强一致性的。其内有一个属性modCount，每次对元素进行修改的时候也会对modCount进行修改，当迭代时发现modCount改变了，就会抛出ConcurrentModificationException

- **HashTable怎么保证并发安全**

  给整个哈希表加上一把大锁，在竞争激烈的情况下性能非常低。

- **HashMap线程不安全的原因**

  1. JDK1.8前多线程环境下扩容导致的死锁：JDK1.8之前的HashMap的扩容机制采用头插法，在多线程环境下可能会导致环形链表的出现，可能会导致死锁。在JDK1.8时改为了尾插法，解决了这个问题
  2. 多线程put可能会导致元素丢失：假设有两个线程插入的key的hash值一样，实际上这两个线程插入的元素会产生哈希碰撞，但它们都以为HashMap中没有这个key，所以都没有去rehash，这样就会导致先插入完成的元素被后插入完成的元素覆盖，导致先插入的元素丢失
  3. put和get并发时，可能会导致get返回空（但实际上key对应的值不为空）：假设线程A调用get，线程B调用put，线程A对key哈希之后找到了目标位置，此时线程B插入元素引起了扩容操作，导致线程A的目标元素被移到了新的位置，但是线程A并不知情（它已经获取过位置了），还是到原来的位置获取元素，于是返回空。



### 10M的包尽可能发送，用UDP还是TCP

IP报头的最小长度为20字节，数据部分有一个最大长度限制，即最大传输单元MTU，在以太网中是1500字节。IP数据报大于1500字节，大于MTU，这个时候发送方IP层就需要分片(fragmentation)。把数据报分成若干片，使每一片都小于MTU，而接收方IP层则需要进行数据报的重组。这样就会多做许多事情，而更严重的是，由于UDP的特性，当某一片数据传送中丢失时，接收方无法重组数据报，将导致丢弃整个UDP数据报，所以一般建议将UDP的数据控制在1472字节以下。而UDP是面向报文的协议，其从应用层收到10M的数据，就会向网络层交付10M的数据

QQ以UDP为主，TCP为辅：主要出于服务器的考虑，每一个客户实际上都是在和服务器交互，一个用户发送信息给服务器，然后服务器转发给正在通信的用户。如果每一个用户在在线期间都采用TCP长连接，会给服务器带来巨大的负担，但是如果采用TCP短连接，频繁的创建和断开连接也会造成巨大的网络负担，而使用无连接的UDP协议即可避免连接带来的负担。而由于UDP不保证可靠传输，因此需要通过上层的协议来保证可靠传输，比如确认重传机制



### 数据库用什么规则建立索引，男女在哪种情况下需要建立索引，身份证怎么建立索引

身份证号有18位，如果全部都作为索引，因为其长度过长，会导致一页内能存储的索引个数减少，从而导致树增高，IO次数增多，因此不能全部存储。

1. 前缀索引身份证前6位代表地区，对于那种市政管理系统（一般管理的人员都是同一个区域的，也就是说前6位基本是一样的）来说，前缀索引就不适用了。

> 7-14为生日码，15-17为顺序码（它是县、区级政府所辖派出所的分配码。如果想知道性别情况，则看第17位码，奇数则代表男性，偶数则代表女性），18为校验码。

2. 倒序索引：所以相对来说后几位的辨识度比较高，所以可以选择倒序建立索引

   > 只适用于等值查询

3. 哈希索引：由于可能会产生哈希碰撞，所以查询条件需要加上身份证号进行等值查询

   > 只适用于等值查询



### 搜索引擎的自动补全或纠正关键字是怎么做的？

前缀树

优化 ➡️ 节省存储空间：有限状态机DFA



### 介绍下Redis字符串结构，zset结构

- 字符串底层结构是SDS，即简单动态字符串
  - SDS没有采用C原生的字符串，而是自己实现了数据结构，其中使用字符数组来保存字符串，并且记录了字符串的长度和字符数组中未使用的字节数。为了与C字符串进行兼容，SDS也在字符串结尾添加了一个空字符。
  - 叫做动态字符串的原因：SDS有五种结构，分别是sdshdr5/8/16/32/64，这五种结构初始化在没有存储数据前所占用的空间是不一样的，后面数字越大占用空间越大，允许存储的字符串长度也越长。Redis会根据需要存储的字符串长度选择最合适的SDS结构进行存储，以此达到节省内存的目的
- zset底层结构是ziplist（压缩列表）或skiplist（跳跃表）：
  - 集合元素个数小于128个+所有元素长度小于64字节，同时满足上述两个条件则使用压缩列表作为底层实现，否则采用跳跃表+字典作为底层实现
  - 压缩列表通过两个相邻的节点来保存一个zset元素，前一个节点保存成员，后一个节点保存分值，按照从小到大的顺序
  - 跳跃表使得zset可以进行排序操作，元素是有序的；字典存储这成员到分值的映射，可以通过O（1）的时间复杂度获取到成员对应的分值。二者通过指针来共享元素，不会产生额外的内存

延伸 ➡️：

- **使用SDS而不是C字符串的好处**
  1. 获取字符串长度：C字符串没有存储长度，需要逐个遍历累计，时间复杂度为O(N)，SDS存储了长度，时间复杂度为O(1)
  2. 杜绝缓冲区溢出：对于C字符串，如果有两个字符串在内存中相邻，当对前一个进行追加的时候，后一个字符串就会被新追加的内容所覆盖，这就是缓冲区溢出；而对于SDS，它在执行追加操作之前，会先判断内存是否满足要求，如果不满足的话会对内存进行重分配
  3. 减少修改字符串带来的内存重分配操作：C字符串采用N+1的字符数组来存储字符串（多出来的一个为结尾的空字符），所以每次操作都需要对这个数组进行内存重分配；但是SDS通过两种策略大大减少了内存重分配的次数：
     - 空间预分配：当对SDS字符串进行增长，需要内存重分配时，会分配额外的内存空间，从而减少了后面继续追加可能会产生的内存重分配操作次数，这个额外的内存空间和增长后的字符串长度相同，最大为1MB
     - 惰性空间释放：当对SDS字符串进行缩短，不会立即回收内存空间，而是使用free来记录字符数组中没有被使用的长度
  4. 二进制安全：C字符串使用空字符来判断字符串是否结束，导致其只能保存文本数据；SDS采用len来判断字符串是否结束，所以可以保存各种类型的数据



### T1，T2，T3 怎么保证这三个线程顺序执行？

1. join

   ```java
   ThreadJoinFunction t1 = new ThreadJoinFunction("系统A调用开始");
   ThreadJoinFunction t2 = new ThreadJoinFunction("系统B调用开始");
   ThreadJoinFunction t3 = new ThreadJoinFunction("系统C调用开始");
   // 注意join方法的位置，如果t1、t2都start后才调用join，那么就不能保证顺序执行了
   t1.start();
   t1.join();
   t2.start();
   t2.join();
   t3.start();
   t3.join();
   ```

   > 对子线程使用`Thread.join()`方法之后就可以让父线程等待子线程运行结束后，再开始执行父线程，这样子线程执行被强行变成了同步的

2. CountDownLatch

   ```java
   CountDownLatch latch = new CountDownLatch(2);
   T1 t1 = o.new T1("系统A调用开始");
   T1 t2 = o.new T1("系统B调用开始");
   T2 t3 = o.new T2("系统C调用开始");
   
   t1.start();
   t3.start();
   t2.start();
   
   class T1 extends Thread{
     public T1(String name) {
     	super(name);
     }
   
     @Override
     public void run() {
       System.out.println(this.getName());
       try {
       	Thread.sleep(1000);
       } catch (InterruptedException e) {
       	e.printStackTrace();
       }
       latch.countDown();
     }
   }
   
   class T2 extends Thread{
     public T2(String name) {
     	super(name);
     }
   
     @Override
     public void run() {
       try {
       	latch.await(); //等待2个T1线程执行完（阻塞直到计数为0）
       } catch (InterruptedException e) {
       	e.printStackTrace();
       }
       System.out.println(this.getName());
       }
     }
   }
   ```

   > 调用latch.await()的线程需要阻塞等待直到countDownLatch的计数为0

3. 创建单一化线程池newSingleThreadExecutor()

   ```java
   ExecutorService executorService = Executors.newSingleThreadExecutor();
   executorService.submit(thread1);
   executorService.submit(thread2);
   executorService.submit(thread3);
   executorService.shutdown();        //使用完毕记得关闭线程池
   ```



### 怎么控制并发线程数-怎么保证只有3个线程在同一时间执行？

```java
Semaphore semaphore = new Semaphore(3);
for(int i = 0; i < 10; i++) {
	Thread t = new Thread(new Runnable() {
    @Override
    public void run() {
			semaphore.acquire();
			// 执行
			semaphore.release();
  	}
  })
}
```

执行前申请semaphore，执行后记得要释放



## 准备用HashMap存1k条数据，构造时传1000会触发扩容吗？

1. 调用tableSizeFor()方法把传入的参数调整为大于其的最小的2次方，即1024，这个1024即threshold（扩容阈值）

2. 在调用构造方法后，table实际上还没有被初始化，只有在第一次调用put方法的时候发现table为null或者容量不足时，才会去调用resize方法进行初始化/扩容

3. 在初始化时，table.size=threshold*loadFactor，即table实际上的大小为1000\*0.74=750。

4. 所以在存储了750条数据的时候就会触发扩容了，实际使用中我们应该传入的参数为待存数据条数/0.75

   > 触发扩容的两种可能：达到阈值、树化过程中发现数组空间不足



## MVCC

### MySQL中的并发场景

1. 读-读：不存在冲突，不需要并发控制

2. 读-写：存在线程安全问题，可能造成脏读、幻读、不可重复读问题

3. 写-写：存在线程安全问题，可能会造成更新丢失

   1. 第一类更新丢失（脏读导致的）

      ![image-20220308153733954](https://tva1.sinaimg.cn/large/e6c9d24egy1h02ijua7w2j20qm07gjrv.jpg)

      即一个事务B可访问另一个事务A还没有提交的数据（脏读），并进行了修改并提交，之后事务A进行回滚，导致事务B的更新丢失

      > 脏读：即一个事务可以访问另一个事务还没有提交的数据
      >
      > 脏写：一个事务修改了另一个事务还没有提交的数据，覆盖了那个事务还没有提交的值

   2. 第二类更新丢失（需要加锁控制）

      ![image-20220308153958631](https://tva1.sinaimg.cn/large/e6c9d24egy1h02imb11bwj20qo07h0t5.jpg)

      即两个事务都读取了初始的数据，并对数据进行了修改，其中一个事务B先完成了修改并提交，但是事务A并没有感知到数据已经被修改了，仍然在原来的数据的基础上进行更新，导致事务B的更新丢失



### MVCC是什么？解决了什么问题？

MVCC即多版本并发控制，它通过实现**读写冲突的无锁并发控制**，大大提高了数据库的性能。

在没有MVCC前，想要解决读写冲突的话，当同一行发生读写请求的时候，则给该行加上行锁。但是MVCC通过快照读，使得读写不需要加锁也可以实现并发安全。

但是，MVCC无法解决写写冲突问题（上面说的第二类更新丢失）



延伸 ➡️：

- **快照读与当前读**

  select lock in share mode（共享锁）、select for update（排他锁）、update、insert、delete（都是排他锁）这些都是当前读

  非串行化隔离级别下的select操作都是快照读



### MVCC实现原理？

几个要点：版本链、undo_log、ReadView

1. 每行数据中有一些隐藏字段：

   - `row_id`：如果该表没有设置主键的话，就会自动添加该隐藏列作为自增主键
   - `trx_id`：记录着最后一次更新该条记录的事务ID
   - `roll_pointer`：指向`undo_log`中该条记录的上一个版本，通过这个`roll_pointer`指针，把一条记录的多个版本串联起来形成了一个版本链
   - 删除flag：当删除一条数据时，其实并不是真正的删除，而是修改了这个flag，因为如果真正删除的话就无法回滚删除操作了

2. 更新一条记录的过程：

   - 把该条记录的旧值移入`undo_log`
   - 修改记录的`trx_id`，并让`roll_pointer`指向旧值
   - 当事务回滚的时候就可以通过对`undo_log`的日志进行逆向操作来进行数据还原了

3. ReadView，即快照

   - 快照中维持着几个有关于事务ID的值：

     - `m_ids`：在生成该ReadView时活跃的事务id列表
     - `min_trx_id`：`m_ids`中的最小值，可以理解为生成该ReadView时活跃的事务中最早的事务
     - `max_trx_id`：生成该ReadView时系统应该分配给下一个事务的id，不一定是`m_ids`的最大值（因为可能在生成之后分配之前有其他事务）
     - `creator_trx_id`：生成该ReadView的事务id

     > 某条记录的最新版本对于当前事务不一定是可见的，会根据当前事务id和快照中这几个值来判断该版本是否可见，如果不可见则顺着版本链向前一个版本继续判断
     >
     > 可见的：
     >
     > 1. creator_trx_id = 数据中的trx_id，即该条记录就是这个事务自己修改的，因此可见
     > 2. 数据中的trx_id < min_trx_id，说明这条数据在当前事务开启之前就已经存在了，因此可见
     >
     > 不可见的：
     >
     > 1. 数据中的trx_id >= max_trx_id，说明这个版本是在ReadView创建之后才产生的
     > 2. 数据中的trx_id在m_ids中，说明生成ReadView的时候，生成这个版本的事务还没有提交（活跃中），因此对于当前事务也是不可见的（无法看见其他事务还没有提交的数据）

   - 读已提交级别下，事务中每次操作都生成一个ReadView；可重复读级别下，事务中只有第一次查询生成一个ReadView，之后所有操作都在这个ReadView下完成



### 怎么解决写写冲突？

1. 悲观锁：
2. 乐观锁：加一个版本号字段



### MySQL怎么在可重复读层面就解决了幻读问题？

幻读问题即一个事务的提交造成了另一个事务多次查询结果的不一致性。（其实是不可重复读的一种特殊情况，主要针对的是插入操作）

1. MVCC并没有完全解决幻读问题，其只是利用快照读在某些场景下规避了幻读：如上图，事务A的两次查询结果是一致的，因此没有幻读问题

   <img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h02k5uhel9j20ka0aw3yt.jpg" alt="image-20220308163320506" style="zoom:80%;" />

2. 但是如果如下图，事务A不加条件的update会作用在表中所有行上，包括事务B插入的新数据。因此导致这条新数据产生了一个新版本，trx_id为事务A的id。所以事务A第二次查询的时候会误以为这条数据对其是可见的，导致第二次查询出现了新的数据，这个场景下，就算是产生了幻读问题了

   | 时间点 |                       事务A                        |       事务B       |
   | :----: | :------------------------------------------------: | :---------------: |
   |   1    |                      开启事务                      |                   |
   |   2    |                                                    |     开启事务      |
   |   3    |      查询数据where name=haha，得到（1，2，3）      |                   |
   |   4    |                                                    | 插入数据(4,sgtsf) |
   |   5    |                                                    |       提交        |
   |   6    | update set xx=xx ➡️ 使得事务B插入的数据产生了新版本 |                   |
   |   7    |    查询数据where name=haha，得到（1，2，3，4）     |                   |

3. 想要完全避免，需要手动把快照读调整为当前读，通过行锁+间隙锁的方式来彻底解决幻读问题，详看-幻读怎么产生的，MySQL怎么解决



## JVM

### 对象的实例化步骤

1. 加载

   1. 通过类的全限定类名获取该类的二进制流
   2. 将该二进制流的静态存储结构转为方法区的运行时数据结构
   3. 在堆中为该类生成一个`class`对象，作为方法区中该类的数据的访问入口

2. 连接

   1. 验证：验证该`class`文件中的字节流信息是否符合JVM要求，不会威胁到JVM的安全

   2. 准备：为该`class`对象的静态变量分配内存，初始化其初始值（零值），保证对象实例字段在不赋值时可以直接使用

      - **如果内存规整-指针碰撞**:如果内存是规整的,那么虚拟机将采用指针碰撞来为对象分配内存,意思是所有用过的内存放在一边,空闲内存在另一边,中间放着一个指针作为分界点的指示器,分配内存就是将指针向空闲内存方向移动与对象大小相同的距离,如果垃圾收集器选择的是Serial,ParNew这种基于压缩算法的,虚拟机采用这种分配方式,一般带有compact(整理)过程的收集器,使用指针碰撞
      - **如果内存不规则-空闲列表分配**:如果内存不是规整的,已经使用和未使用的内存相互交错,那么虚拟机采用的是空闲列表法来为对象分配内存,虚拟机维护一个列表,记录上那些内存块是可用的,在分配时从表中找到一块足够大的空间来分配对象实例,并更新表上的内容,这种分配方式称为"空闲列表(Free List)”
      - 至于选择哪种分配方式由java堆是否规整决定,而java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定
      - 处理并发安全问题：
        - 采用CAS失败重试,区域加锁保证更新的原子性
        - 每个线程设预先分配一块TLAB

   3. 解析：该阶段主要完成符号引用转化为直接引用，设置对象头，即将对象的所属类(即类的元数据信息),对象的HashCode和对象的GC信息,锁信息等数据存储在对象头中,这个过程具体设置方式取决于JVM实现

      > **符号引用**就是一组符号来描述目标，可以是任何字面量；
      >
      > **直接引用**就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。

3. ==初始化==：为类的静态变量赋予正确的初始值，即代码中显式初始化的值。在Java中对类变量进行初始值设定有两种方式：

   - 声明类变量时指定初始值
   - 使用静态代码块为类变量指定初始值

   > 在java程序视角来看,初始化才正式开始,初始化成员变量,执行实例化代码块,调用类的构造方法,并把堆内对象的首地址赋值给引用变量,因此一般来说(由字节码是否跟随有invokespecial指令所决定),new指令之后会紧接着就是执行方法,把对象按照程序员的意愿进行初始化,这样一个真正可用的对象才算完全创建出来



### Minor GC和Full GC触发条件

- Minor GC触发条件：当Eden区满时，触发Minor GC。

  > 可以提一嘴Minor GC是有风险的，触发了MinorGC之后仍然有可能触发FullGC，以便引出空间分配担保原则那一块

- Full GC触发条件：

  - 调用System.gc时，系统建议执行Full GC，但是不必然执行
  - 老年代空间不足
  - 方法区空间不足
  - 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 ➡️ 空间分配担保原则
  - 由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小



### 空间分配担保原则

JVM的分配内存机制有三大原则和空间分配担保机制：

1. 优先分配到Eden区

2. 大对象直接进入老年代

3. 长期存活的对象分配到老年代

4. 空间分配担保机制

   在发生`Minor GC`之前，虚拟机会检查老年代最大可用的连续内存空间是否大于新生代所有对象的总空间（是否具有足够可用空间）

   - 如果大于，说明有足够空间可用，则此次`Minor GC`是安全的
   - 如果小于，则虚拟机会查看`HandlePromotionFailure`参数是否允许担保失败
     - `HandlePromotionFailure = true`，则会继续检查老年代最大可用连续空间是否大于历次晋升到老年代的对象的平均大小。如果大于，则尝试进行一次`Minor GC`，但是这次`Minor GC`仍然是有风险的；如果小于，则进行一次`Full GC`
     - `HandlePromotionFailure = false`，说明不允许`Minor GC`失败的情况产生，直接进行一次`Full GC`

> 为什么说`Minor GC`是不安全/有风险的？
>
> 这是因为新生代采用复制算法进行GC，如果大量对象在`Minor GC`后仍然存活，那么这些对象就全部都会复制进S区，然后S区是很小的（Eden:From:To=8:1:1），当**S区空间不足时就需要把无法容纳的对象放入老年代**。
>
> 但是前提是老年代有足够的空间能够容纳对象，这时候就需要进行空间分配担保，由于有多少对象能在`Minor GC`之后存活是不可知的，因此只能通过过去的垃圾回收后晋升到老年代对象大小的平均值作为参考，来评估这次`Minor GC`的风险。
>
> 通过开启空间分配担保，虽然无法完全避免`Minor GC`的风险，但是在一定程度上降低了`Full GC`的执行频率



### JVM调优

#### 性能定义

- 吞吐量：指不考虑GC引起的停顿时间或内存消耗，能支撑应用达到的最高性能指标
- 延迟：其度量标准是缩短由于垃圾收集引起的停顿时间，避免应用运行时发生抖动
- 内存占用：垃圾收集器流畅运行所需要的内存数量

#### 调优的基本原则

最有效的优化手段其实是架构和代码层面的优化，JVM优化是最后的手段

1. MinorGC回收原则：每次MinorGC都要尽可能多的收集垃圾对象，以减少应用程序发生Full GC的频率
2. GC内存最大化原则：处理吞吐量和延迟问题的时候，垃圾收集器能使用的内存越大，GC效果越好
3. GC调优3选2原则：难以三者兼得

#### 堆大小管理

|       参数        |                描述                |
| :---------------: | :--------------------------------: |
|       -Xms        | 初始堆的大小，默认为物理内存的1/64 |
|       -Xmx        |             最大堆大小             |
|    -XX:NewSize    |        新生代空间大小初始值        |
|  -XX:MaxNewSize   |        新生代空间大小最大值        |
|       -Xmn        |   新生代空间大小（Eden+2个S区）    |
|   -XX:PermSize    |     永久代空间的初始值&最小值      |
|  -XX:MaxPermSize  |         永久代空间的最大值         |
| -XX:MetaspaceSize |          元空间的初始大小          |

> 老年代的空间大小根据新生代的大小隐式设定：
>
> - 初始值=-Xmx - -XX:NewSize
> - 最小值=-Xmx - -XX:MaxNewSize
>
> 也可以根据-XX:NewRatio调整新生代与老年代的比例，假如-XX:NewRatio=3，那么新生代:老年代=1:3

#### 调优思路

- 内存占用调优
  - jvm内存分配&参数
  - 计算活跃数据大小
- 延迟调优
  - 优化新生代的大小
  - 优化老年代的大小
- 吞吐量调优
  - 选择合适的垃圾收集器
  - 进行吞吐量测试，并内存占用调优和延迟调优进行微调

#### 调优目标

使每一次GC都回收尽可能多的对象：

1. 如果新生代的内存使用率处在高位，导致频繁的Minor GC，而GC效率又不高，说明对象没那么快能被回收，可以通过`-Xmn`把新生代大小调大一点（也可以调高`-XX:NewRatio`，这是新生代与老年代的比例）

2. 如果老年代的内存使用率处在高位，导致频繁的Full GC，而GC效率又不高，首先需要排查是否存在内存泄漏；如果不是内存泄漏导致的，就要考虑通过调高`-XX:NewRatio`来把老年代大小调高

   > 导致内存泄漏的情况：
   >
   > - 单例造成的内存泄漏：单例的静态特性使得其生命周期和程序的生命周期一样长，这就意味着如果其持有某个对象的引用，那么那个对象就不能回收
   >
   > - 非静态内部类创建静态实例造成的内存泄漏：非静态内部类默认持有外部类的引用，如果又给该非静态内部类创建了一个静态的实例，该实例的生命周期和应用一样长，那么就会导致该静态实例一直持有对外部类的引用，导致外部类不能被正常回收
   >
   >   ```java
   >   public class MainActivity extends AppCompatActivity {
   >   
   >       private static TestResource mResource = null;
   >   
   >       @Override
   >       protected void onCreate(Bundle savedInstanceState) {
   >           super.onCreate(savedInstanceState);
   >           setContentView(R.layout.activity_main);
   >           if(mResource == null){
   >               mResource = new TestResource();
   >           }
   >           //...
   >       }
   >       
   >       class TestResource {
   >       //...
   >       }
   >   }
   >   // 解决：把该内部类设为静态内部类或者抽取出来成一个单例
   >   ```
   >
   > - 线程池+ThreadLocal不正确使用：线程内部存有着一个`ThreadLocalMap`，`ThreadLocalMap`中的key为弱引用，只要一GC就会被回收，但是如果key一旦被回收了，就没有办法通过key找到value，那么value就无法被回收，就会引起内存泄漏问题。因此使用完`ThreadLocal`之后一定要调用`remove`方法释放内存空间
   > - 集合容器中的内存泄露：如果把一些对象的引用加入到集合容器中，那么这个容器就会一直持有对该对象的强引用，如果之后不使用该对象，又不把其从集合中移除的话，就会引起内存泄漏问题



#### 调优命令

![image-20220310192411337](/Users/wingsiwoo/Library/Application Support/typora-user-images/image-20220310192411337.png)



## 如何设计存储海量数据的存储系统

以商城评论为例

- 数据存储

  - 基础数据存储：使用MySQL，只存储非文本的评论基础信息，比如评论状态、用户id、评论时间等，并且根据数据场景和数据量级去决定是否需要分库分表

  - 文本存储：选择使用nosql如mongodb、hbase，一是减轻了MySQL的存储压力，二是nosql的高性能读写大大提升了系统的吞吐量并降低了延迟

    - cassandra适用于写多读少的情况
    - mongodb也是基于分布式文件存储的数据库，介于关系型数据库与非关系型数据库之间，同时也是内存级数据库，mongo写性能不及 cassandra，但读写分离情况下读性能相当不错

  - 热点数据：Redis缓存，降低数据库的压力，注意淘汰策略的选择（热点数据：比如说最多人点赞的评论、最多人查看的商品）

    |      策略       |                         描述                         |
    | :-------------: | :--------------------------------------------------: |
    |  volatile-lru   | 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 |
    |  volatile-ttl   |   从已设置过期时间的数据集中挑选将要过期的数据淘汰   |
    | volatile-random |      从已设置过期时间的数据集中任意选择数据淘汰      |
    |   allkeys-lru   |       从所有数据集中挑选最近最少使用的数据淘汰       |
    | allkeys-random  |          从所有数据集中任意选择数据进行淘汰          |
    |   noeviction    |                     禁止驱逐数据                     |

- 数据索引：为了提高对海量数据的查询效率，可以选择elasticsearch、solr等全文搜索引擎

  > 全文搜索引擎是目前广泛应用的主流搜索引擎。它的工作原理是计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。

  - 原因：
    - 全文搜索引擎支持非结构化数据的搜索，可以更好更快的搜索大量存在的任何单词或单词组的非结构化文本。而对于非结构化的数据文本，关系型数据库搜索不是能很好的支持
    - MySQL的全文索引实现的很鸡肋，建立了索引，维护起来也很麻烦

  ![image-20220310202949123](https://tva1.sinaimg.cn/large/e6c9d24egy1h0528jc42hj20eg0bs75e.jpg)

- 数据容灾与高可用：使用多机房主从方式部署，各机房内部实现主从结构进行数据同步/读写分离



## 创建对象的5种方式

| 使用new关键字                      | } → 调用了构造函数   |
| ---------------------------------- | -------------------- |
| 使用Class类的newInstance方法       | } → 调用了构造函数   |
| 使用Constructor类的newInstance方法 | } → 调用了构造函数   |
| 使用clone方法                      | } → 没有调用构造函数 |
| 使用反序列化                       | } → 没有调用构造函数 |



## Redis

### Redis与Memcached

两者都是非关系型内存键值数据库，主要有以下不同：

1. 数据类型：Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型（string、list、set、zset、hash），可以更灵活地解决问题。
2. 数据持久化：Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。
3. 分布式：Memcached 不支持分布式，只能通过在客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。Redis Cluster 实现了分布式的支持。
4. 内存管理机制：
   - 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。
   - Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。



## Spring

### 生命周期/IOC容器加载过程

1. 实例化`Instantiation`
2. 属性赋值`Population`
3. 初始化`Initialization`
4. 销毁



### 解决循环依赖

构造器注入的、非单例的Bean无法解决

假设AB相互依赖：

1. A首先实例化，并把`ObjectFactory`半成品暴露在三级缓存`singletonFactories`中

2. A进行属性赋值，发现其依赖的B还没有创建，于是先去加载B

3. 实例化B，同样把`ObjectFactory`半成品暴露在三级缓存`singletonFactories`中

4. B进行属性赋值，在三级缓存`singletonFactories`找到A的半成品`ObjectFactory`，并调用`ObjectFactory.getObject()`方法获取到A的早期引用

   > `ObjectFactory.getObject()`方法最终会调用到`getEarlyBeanReference()`方法，如果该bean被AOP切面代理的话则返回的是代理对象，否则返回bean实例

5. 拿到A后（还没有属性填充），**把其从三级缓存`singletonFactories`移除，放入二级缓存`earlySingletonObjects`中**。此时B中注入的是一个半成品A，但是随着B的初始化完成，A可以继续后续的初始化操作，最终B注入到的就是一个完整的A实例，因为这两者在内存中是同一个对象

   > 注意这个移到二级缓存的操作，如果不移动的话，当其他对象也依赖A并尝试创建A，那么他们也会调用`ObjectFactory.getObject()`方法，这样子跟B这里生成的代理对象不是同一个的，但是如果其他对象先在二级缓存中找到已经创建过的代理对象，就不会再去调用`ObjectFactory.getObject()`方法了

6. B完成后续初始化操作后返回，A中的属性赋值完成，继续完成后续操作



### 为什么需要三级缓存？二级缓存可以解决循环依赖问题吗

假设只有二级缓存，其实也可以解决循环依赖问题，像上述讲的过程根本就跟一级缓存`singletonObjects`无关。

> 实测：如果没有AOP代理，二级缓存使用也没有问题。但是如果存在AOP代理，会抛出异常，显示没有使用最终版本的bean对象：
>
> `()->getEarlyBeanReference(beanName,mbd,bean)`

对于有AOP代理的循环依赖解决，无非就是两种方法：

1. 无论是否有循环依赖，都提前创建好代理对象，放入缓存
2. 不提前创建好代理对象，出现循环依赖时，才实时生成代理对象

Spring设计的原则是`bean实例化->属性设置->初始化->生成aop对象`，如果是按照第一种方法，显然就违背了Spring设计的原则。

另外，假设去掉了二级缓存`earlySingletonObjects`，直接生成半成品`ObjectFactory`到三级缓存`singletonFactories`中，那么当多个对象依赖该对象的时候，它们都会调用`ObjectFactory.getObject()`方法来获取早期引用，如果有AOP代理的话，每次调用都会返回不同的代理对象，这样就导致代理对象不是唯一的了。所以需要新增一个二级缓存`earlySingletonObjects`来存储代理对象。



### 事务传播

![这里写图片描述](https://img-blog.csdn.net/20170420212829825?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc29vbmZseQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

> 默认为PROPAGATION_REQUIRED





## Linux

### Linux下创建新进程

- fork
  - fork函数创建的子进程是父进程的复制，得到的父子进程是独立的，具有良好的并发性，但是进程间通信需要专门的机制
  - fork函数调用一次，有两个返回值：对父进程而言返回的是子进程的ID（一个父进程可能有多个子进程，但是没有函数可以使得父进程获取所有子进程ID），对子进程而言返回值是0（子进程可以通过getppid来获取父进程ID）。如果创建失败则返回给父进程-1
  - Linux采用COW写时复制技术来进行优化：即当有多个进程访问相同的资源的时候，它们会获取相同的指针指向相同的资源，只有需要修改资源内容的时候，系统才会复制一份专用副本给那个进程，而其他进程看到的资源仍然保持不变。
- vfork
  - vfork创建的父子进程共享地址空间，而不是复制。因此父子进程的数据是共享的，父子进程间的通信很好解决
  - vfork创建的子进程必须调用exit函数来结束，否则子进程不会结束
  - vfork创建的子进程总会在父进程之前执行
  - vfork函数的出现主要是主要是当年Unix系统没有写时复制技术，所以fork出的子进程即使只是调用exec函数来执行另一个可执行文件，也需要完整的复制父进程的资源，但是实际上完全没有必要复制，这样会造成大量的开销浪费。因此设计了vfork函数来避免这个问题，主要目的就是exec一个新的程序，但是现在有了COW技术，这个函数就渐渐被弃用了



### Linux中的进程调度算法

#### O(n)

O(n)调度器是Linux内核在2.4及早期版本中采用的进程调度器，其时间复杂度为O(n)。

维护了两个`queue`：`runqueue`和`expired queue`，这两个`queue`永远保持有序，一个进程用完时间片，就会被插入到`expired queue`；当`runqueue`为空时，就把`runqueue`和`expired queue`进行交换

> 为了保证两个队列是有序的，需要对队列进行扫描寻找插入位置，这一操作的时间复杂度为O(n)

存在问题：

1. 队列所有CPU共享，当对队列进行操作时需要对其进行加锁操作保证可见性和一致性，因此并发度小
2. 需要扫描队列中所有任务来判断最佳插入位置，任务越多，效率越低



#### O(1)

Linux2.6版本开始使用O(1)算法，时间复杂度为O(1)。

O(1)调度器有个非常重要的数据结构就是`prio_array`，是一个用于表示进程动态优先级的数组`queue`，它包括了每一种优先级进程所形成的链表，可分为140个进程优先级（0～139，数值越小优先级越高）。

同时，为了减少多核CPU之间的竞争，每个CPU都维护了一份本地的优先队列。

> O(1)调度算法把140个优先级的前100个（0 ~ 99）作为 实时进程优先级，而后40个（100 ~ 139）作为普通进程优先级
>
> 采用 `bitarray`。它为每种优先级分配一个 `bit`，如果这个优先级队列下面有 `process`，那么就把相应的 bit 置为 1，否则置为 0。这样，问题就简化成寻找一个 `bitarray` 里面最高位是 1 的 `bit`，这基本上是一条 CPU 指令的事

与2.4内核中依次比較每一个进程的优先级不同，因为进程优先级个数是定值，因此查找最佳优先级的时间恒定。它不会像曾经的方法那样受可运行进程数量的影响。

假设确定了优先级。那么选取下一个进程就简单了，仅仅需在优先级数组中相应的链表上选取一个进程就可以。

存在问题：

1. 一个高优先级多线程的应用会比低优先级单线程的应用获得更多的资源，这就会导致一个调度周期内，低优先级的应用可能一直无法响应，直到高优先级应用结束。



#### CFS调度算法（完全公平调度算法）

1. 使用虚拟时间`vruntime`作为进程排序的依据（权值）

   > `vruntime += 实际运行时间 * 1024 / 进程权重`
   >
   > 进程权重是根据任务的nice值进行索引。nice值可以理解为是我们事先为任务分配的优先级。nice值越低优先级越高

2. 使用红黑树来存储要调度的任务队列，虚拟时间越小的节点在红黑树中越靠左，每次调度红黑树中最左（`vruntime`最小）的节点。

   > 存储了一个指针指向最左节点，这样调度时就不需要O(nlogn)去遍历红黑树，而是直接O(1)拿到指针指向的节点
   >
   > `vruntime` 小说明占用cpu时间短，优先选择。这样既能公平选择进程，又能保证高优先级进程获得较多运行时间



### linux的一些常见shell命令

#### 必须掌握

1. ==**netstat**==：查看网络状态，显示Proto-协议、Local Address-本地地址、Foreign Address-外部地址、State-状态
   - -a：列出所有当前的连接
   - -r：显示路由表信息
   - -t：所有的tcp协议的连接
   - -u：所有的UDP协议的连接
   - -s：按照每个协议来分类进行统计，默认显示IP、IPv6、ICMP、ICMPv6、TCP、TCPv6、UDP、UDPv6的信息
   - -p：查看进程信息，必须在root权限之下运行，否则不能得到运行在root权限下的进程名
   - -c：持续输出
2. ==**tcpdump**==：截取网络分组，并输出分组内容，通常用于网络分析和问题排查
   - -i：指定需要监听的端口，如果不指定，默认只会监视第一个网络接口，一般是eth0
   - host [ip]：监视指定主机的数据包
   - host [ip1] and [ip2]：监视ip1与ip2之间的通信
3. ==**ipcs**==：Linux下显示进程间通信设施状态的工具
   - -a：查看当前使用的共享内存、消息队列及信号量所有信息
   - -p：查看与共享内存、消息队列相关进程之间的消息
   - -u：查看各个资源的使用总结信息，比如信号量集的个数、信号量的个数、消息队列中当前使用的消息总数
   - -l：查看系统限制
4. ==**ipcrm**==：用于删除一个或多个消息队列、信号量集或共享内存标识。相关的标识和关键字可以使用ipcs命令查看
   - -m SharedMemory id：删除共享内存标识 SharedMemoryID。与 SharedMemoryID 有关联的共享内存段以及数据结构都会在最后一次拆离操作后删除
   - -M SharedMemoryKey：删除用关键字 SharedMemoryKey 创建的共享内存标识。与其相关的共享内存段和数据结构段都将在最后一次拆离操作后删除
   - -q MessageID：删除消息队列标识 MessageID 和与其相关的消息队列和数据结构
   - -Q MessageKey：删除由关键字 MessageKey 创建的消息队列标识和与其相关的消息队列和数据结构
   - -s SemaphoreID：删除信号量标识 SemaphoreID 和与其相关的信号量集及数据结构
   - -S SemaphoreKey：删除由关键字 SemaphoreKey 创建的信号标识和与其相关的信号量集和数据结构

5. ###### **core dump**：当程序运行的过程中异常终止或者崩溃，操作系统会将程序当时的内存状态记录下来，保存在一个文件中，这种行为就叫做core dump

   - 查看是否开启core dump：ulimit -c，如果输出结果结果为0，说明处于关闭状态
   - 开启core dump：ulimit -c unlimited，开启后当程序异常终止时就会生成core dump文件



#### 僵尸进程查看并杀死

**什么是僵尸进程？**

父进程通过fork创建子进程，父子进程的运行是异步的，即父进程无法得知子进程什么时候结束，所以父进程需要调用wait()或者waitpid()系统调用（也就是pthread_join函数，阻塞等待线程退出，获取线程退出状态）来获取子进程的终止状态。如果父进程没有调用，当子进程结束后，由于父进程并不知道其已经结束，子进程的进程描述符仍然保留在系统中，此时该子进程就是一个僵尸进程

> 如果父进程先退出，那么这个子进程就会变成孤儿进程，被托孤给init进程
>
> 如果子进程先退出，父进程有调用wait/waitpid来处理的话，子进程会被父进程清理，不会产生僵尸进程；但是如果没有调用的话，子进程就变成了僵尸进程，在父进程存在期间一直存在

**避免产生僵尸进程的方法**

1. 父进程调用wait/waitpid，但是这样会使得父进程阻塞
2. fork twice：父进程调用wait的话会被阻塞，如果想要避免这个阻塞并且避免产生僵尸进程的话，就要fork twice。即父进程先fork创建一个子进程A，A执行子进程的任务；然后父进程第二次fork创建子进程B，B执行父进程的任务，然后让父进程本身退出。这样AB进程都不会变成僵尸进程，并且父进程的任务由子进程B正常执行，无需阻塞
3. 让父进程先死
4. 在父进程中设置信号处理函数signal，直接忽略子进程的SIG_CHLD信号，这时候内核会将子进程交给init进程处理，这样的话之后父进程调用wait/waitpid是不会接收到子进程信息的

**什么是孤儿进程？**

如果父进程已经结束而子进程还没结束，则子进程的父进程则变为init进程，此时子进程就是一个孤儿进程

**什么是守护进程？**

守护进程就是在后台运行，不与任何终端关联的进程，通常情况下守护进程在系统启动的时候就在运行，它们以root用户或者其他特殊用户运行，并能处理一些系统级的任务，通常守护进程的名字以d结尾

1. `top`：如果`zombie`前的数字不为0说明有僵尸进程

2. `ps -A -ostat,ppid,pid,cmd | grep -e '^[Zz]'`：定位僵尸进程以及僵尸进程的父进程

   > 要杀死僵尸进程就只能让父进程退出，即使运行kill -9也无法杀死僵尸进程

   - -A：列出所有进程
   - -o：自定义输出字段：stat-状态、ppid-父进程id、pid进程id、cmd-命令
   - grep -e '^[Zz]'：抓取stat为z或Z的，因为僵尸进程的状态为z或Z

3. kill -hup 父进程id



#### 内存

1. free

   - -m：以MB为单位来展示内存使用信息

     ![image-20220316213215076](https://tva1.sinaimg.cn/large/e6c9d24egy1h0c1rbnz6tj20j403hjri.jpg)

   - -h：以人类可读单位来展示内存使用信息

   1. Mem一行表示物理内存，是从OS的角度来看的，因为对于OS来说，buffer和cache都是被使用的

   2. Swap表示交换内存（也就是常说的虚拟内存），是从应用程序的角度来看的，对于应用程序来说，buffer和cache就是可以使用的内存，所以其可用内存为free+buffer/cache

      > 当可用内存少于额定值的时候，就会开始进行交换，交换将通过三个途径来减少系统中使用的物理页面的个数：
      >
      > 1. 减少缓冲与页面cache的大小
      > 2. 将系统V类型的内存页面交换出去
      > 3. 换出或者丢弃页面
      >
      > 事实上，少量地使用swap是不是影响到系统性能的。

   3. (-buffers/cache) used内存数：第一部分Mem行中的 used – buffers – cached，反映出的是被程序实际使用的内容（去掉缓存）

   4.  (+buffers/cache) free内存数: 第一部分Mem行中的 free + buffers + cached，反应的是可以挪用的内存总数，包括空闲内存和缓存

   5. total：总共的内存 ➡️ total=used+free

   6. used：内存使用量

   7. free：空闲内存

   8. buff/cache：表示缓存和缓冲内存量；Linux会将很多东西缓存起来以提高性能，这部分内存可以在必要时进行释放，给其他程序使用

      > 为了提高磁盘存取效率, Linux做了一些精心的设计, 除了对dentry进行缓存(用于VFS,加速文件路径名到inode的转换), 还采取了两种主要Cache方式：`Buffer Cache`和`Page Cache`。**前者针对磁盘块的读写，后者针对文件inode的读写**。这些Cache有效缩短了 I/O系统调用(比如read,write,getdents)的时间。
      >
      > 磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。
      >
      > `Page cache`实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到`page cache`。文件的逻辑层需要映射到实际的物理磁盘，这种映射关系由文件系统来完成。当`page cache`的数据需要刷新时，`page cache`中的数据交给`buffer cache`，因为`Buffer Cache`就是缓存磁盘块的。但是这种处理在2.6版本的内核之后就变的很简单了，没有真正意义上的cache操作。
      >
      > `Buffer cache`是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到`buffer cache`中，例如，文件系统的元数据都会缓存到`buffer cache`中。
      >
      > 简单说来，`page cache`用来缓存文件数据，`buffer cache`用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到`page cache`，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到`buffer cache`。
      >
      > 所以我们看linux,只要不用swap的交换空间,就不用担心自己的内存太少.如果常常swap用很多,可能你就要考虑加物理内存了.这也是linux看内存是否够用的标准.
      >
      > 如果是应用服务器的话，一般只看第二行，+buffers/cache,即对应用程序来说free的内存太少了，也是该考虑优化程序或加内存了。

   9. available：可用内存

2. 读取/proc/meminfo文件：cat /proc/meminfo，展示的信息和free的差不多
3. vmstat -s，跟/proc/meminfo差不多
4. top：KiB那一行为内存相关情况；KiB Swap那一行为交换内存相关情况



#### CPU

查看CPU使用率：

1. top：top命令一般用于查看进程的CPU和内存使用情况

   ![image-20220316211301430](https://tva1.sinaimg.cn/large/e6c9d24egy1h0c17bjrcuj20is09a0u2.jpg)

   - %us：表示用户空间程序的CPU使用率（没有通过nice调度）

   - %sy：表示系统空间的CPU使用率，主要是内核程序

   - %ni：表示用户空间且通过nice调度过的程序的CPU使用率

   - %id：空闲CPU ➡️ 可算出CPU使用率

   - %wa：CPU运行时在等待IO的时间
   - %hi：CPU处理硬中断的数量
   - %si：CPU处理软中断的数量
   - %st：被虚拟机偷走的CPU

2. vmstat：一般用于查看CPU使用率、内存使用情况、虚拟内存交换情况、IO读写情况，top只能看到CPU和内存使用率。第一个数字参数表示采样的时间间隔数，单位是秒；第二个数字参数是采样次数
3. sar：命令语法和vmstat一样，需要安装sysstat包



#### 磁盘

1. df：磁盘使用率=(Used列数据之和)/(1k-blocks列数据之和)



#### 进程相关

1. 查看进程：ps（进程的瞬间状态）
   - -e：显示系统内所有进程的信息，也可以用-A
   - -f：使用完整的格式现实进程信息，如果只有 ps -e 则输出进程信息的格式和只使用 ps 一样（都只有`PID TTY TIME CMD这几项，`但是输出信息的内容和ps的不一样，纯ps只会显示在当前用户会话中打开的进程）
   - pstree：以树状显示正在运行的进程
   - 结合 ｜ grep xxx查询目标进程
2. top（查看进程动态信息）
3. 杀死进程：kill
   - kill pid/kill -15 pid：系统会发送一个SIGTERM的信号给对应的进程，进程收到该signal后，可能会停止/释放资源后停止/继续运行，对应的子进程不会收到该信号，如果收到该信号的父进程停止了，则子进程会成为孤儿进程，托孤给init进程（pid为1）
   - kill -9 pid：系统会发送一个SIGKILL信号，该信号是无法被捕获的，也就是说进程无法执行信号处理程序，会直接退出，所以kill -9 pid一定能杀死程序，但是这样也会造成进程结束前无法关闭相应资源
   - Ctrl + C：只针对当前前台进程，和它所在的进程组的每个进程都发送SIGINT信号，之后这些进程会执行信号处理程序再终止（也就是说和SIGTERM一样都是可以被捕获的）







1. 给定一个整数n和k，写出一个长度为n的数组，内容为[1,n]（不重复），相邻元素之和为奇数的对数为k
2. 给定一个数组a，元素为染色所需的成本，给定一个字符串s，长度与a一致，标志着元素的染色情况，R为染色W为为染色。求获得长度为k的染色区间（区间内全部元素都染色）所需的最小成本
3. 象走田字格，其周围一圈称为象眼（x+-1,y+-1），如果对应方向的象眼没有障碍，则可以走外面一圈(x+-2,y+-2)，如果踩中的格子有兵则可以把兵吃掉。给定一个棋盘（上面有些地方有兵），求象从指定起始位置走到指定目标位置所需的最小步数
4. 