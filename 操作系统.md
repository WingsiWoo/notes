# 操作系统

## IO

### 文件描述符

在`Linux`下，万物皆文件，而为了让一个应用程序对应上一个文件，就产生了文件描述符（`File Descriptor`，简称`fd`）。当应用程序请求内核打开/新建一个文件时，内核会返回一个文件描述符用于对应这个打开/新建的文件，其fd本质上就是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于`UNIX`、`Linux`这样的操作系统。

#### fd唯一性

每个进程都维护着一个自己专属的文件描述符表，因此进程+文件描述符可以确定唯一一个文件，而同一个文件在不同进程中的fd可能是不一样的。



### 多路IO复用

#### 基本概念

##### 什么是多路IO复用？

IO多路复用是一种同步IO模型，实现一个线程可以监视多个文件句柄；一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；没有文件句柄就绪时会阻塞应用程序，交出cpu。多路是指网络连接，复用指的是同一个线程

##### 为什么要采用多路IO复用？

没有该机制时，只有`BIO`和`NIO`（阻塞IO和非阻塞IO）。采用阻塞IO的进程在发出IO请求后，会一直阻塞等待，直到IO返回成功或失败；非阻塞IO的轮询操作十分浪费CPU资源



#### 三种实现方式

|                |                       select                       |                     poll                     |                            epoll                             |
| :------------: | :------------------------------------------------: | :------------------------------------------: | :----------------------------------------------------------: |
|  **数据结构**  |                       bitmap                       |                     数组                     |                            红黑树                            |
| **最大连接数** |                        1024                        |                     不限                     |                             不限                             |
|   **fd拷贝**   |                每次调用select时拷贝                |              每次调用poll时拷贝              |    fd首次调用epoll_ctl时拷贝，每次调用epoll_wait时不拷贝     |
|  **工作效率**  |                      轮询O(n)                      |                   轮询O(n)                   |                           回调O(1)                           |
|  **主要开销**  |            内核判断是否有文件描述符就绪            |         内核判断是否有文件描述符就绪         | 如果存在很多短期活跃连接，系统调用开销可能会大于select和poll |
|  **轮询问题**  | select和poll都需要不断轮询所有的fd集合直到设备就绪 |        期间可能需要睡眠和唤醒多次交替        |                调用epoll_wait不断轮询就绪链表                |
|    **适用**    |    当监测的fd数量较小，且各个fd都很活跃的情况下    | 当监测的fd数量较小，且各个fd都很活跃的情况下 |      当监听的fd数量较多，且单位时间仅部分fd活跃的情况下      |

> `select`和`poll`需要把所有的`fd`从用户态复制到内核态，然后在内核态状态下轮询这些`fd`去寻找已经就绪的，会造成巨大的资源浪费，因此能处理的并发连接少很多
>
> 而`epoll`则把它们拆分成三部分：
>
> 1. 调用`epoll_create`创建`epoll`对象（在`epoll`文件系统中给这个句柄分配资源）
> 2. 调用`epoll_ctl`向`epoll`对象中添加`fd`（此时会把这些`fd`复制到内核态中）
> 3. 调用`epoll_wait`收集就绪的`fd`



##### select

```c
// select的调用会阻塞到有文件描述符fd可以进行IO操作或被信号打断或者超时才会返回
int select(int nfds, fd_set *readfds, fd_set *writefds,
                fd_set *exceptfds, struct timeval *timeout);

// 把fd从set中移除
void FD_CLR(int fd, fd_set *set);
// 检测一个文件描述符是否在组中，用该方法来检测一次select调用之后有哪些文件描述符可以进行IO操作
int  FD_ISSET(int fd, fd_set *set);
// 添加fd到set中
void FD_SET(int fd, fd_set *set);
// 用来清空fd_set，每次调用select前都要先调用FD_ZERO
void FD_ZERO(fd_set *set);
```

- `select`把`fd`分为三组：

  1. `readfds`：需要进行读操作的文件描述符
  2. `writefds`：需要进行写操作的文件描述符
  3. `exceptfds`：需要进行异常事件处理的文件描述符

  > 如果对应参数为`NULL`则说明该类事件不需要监听

  `select`返回后，原来传入的参数`readfds`等会被修改，只剩下可以进行对应IO操作的`fd`

- `select`可同时监听的文件描述符数量是通过`FS_SETSIZE`来限制的，在`Linux`系统中，该值为1024，当然我们可以增大这个值，但随着监听的文件描述符数量增加，`select`的效率会降低



###### 缺点

- 单个进程所打开的FD（文件描述符）是有限制的，通过`FD_SETSIZE`设置，默认1024

- 每次调用`select`，都需要把`fd`集合从用户态拷贝到内核态，这个开销在fd`很多`时会很大

  > 所有文件描述符都是在用户态被加入其文件描述符集合的，因此每次调用都需要将整个集合拷贝到内核态
  >
  > 因为是拷贝整个集体，因此很多没有就绪的`fd`也会被拷贝，这是没有必要的工作。并且由于每次调用`select`都会进行拷贝，一个`fd`可能会被拷贝多次

- 对`socket`扫描时是线性扫描，采用轮询的方法，效率较低（高并发时）

  > 因为`select`方法仅仅知道有IO事件发生，但是并不知道是哪几个流发生了，因此只能无差别的轮询所有流去寻找可操作的（查询fd对应的设备状态）

##### poll

```c
/**
 * poll只有一个pollfd数组，数组中的每个元素都表示一个需要监听IO操作事件的文件描述符。
 * events参数是我们需要关心的事件，
 * revents是所有内核监测到的事件
 */
struct pollfd {
        int fd; /* file descriptor */
        short events; /* requested events to watch */
        short revents; /* returned events witnessed */
    };

int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

###### 缺点

- 每次调用`poll`，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大

  > 与`select`同理，fd很多需要拷贝的就很多

- 对`socket`扫描时是线性扫描，采用轮询的方法，效率较低（高并发时）

  > 与`select`同理，只不过没有大小限制，因为其是基于链表实现的

  

##### epoll

```c
struct eventpoll {
    /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/
    struct rb_root  rbr;
    /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/
    struct list_head rdlist;
};

// 创建epoll实例，并把所有需要监听的事件放到epoll实例中的rbr中
int epoll_create(int size);
int epoll_create1(int flags);
// 往epoll实例中增删改要监测的文件描述符fd
// 每次注册新的事件到epoll句柄中时，会把fd拷贝进内核，在调用epoll_wait时无需拷贝，从而保证了每个fd在整个过程中只会被拷贝一次
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
// 用于阻塞的等待可以执行IO操作的文件描述符直到超时
// 这个方法实际上只需要检查rdlist里面是否有数据即可，有数据就返回，没数据就休眠直到超时结束
int epoll_wait(int epfd, struct epoll_event *events,
               int maxevents, int timeout);
int epoll_pwait(int epfd, struct epoll_event *events,
                int maxevents, int timeout,
                const sigset_t *sigmask);
```

- 只有`Linux`中有实现
- `epoll`则将整个文件描述符集合维护在内核态，每次添加文件描述符的时候（调用`epoll_ctl`）都需要执行一个系统调用
- `epoll`实际上是事件驱动，当有活动产生时，会自动触发epoll回调函数通知`epoll`文件描述符，然后内核将这些就绪的文件描述符放到`rdlist`中等待`epoll_wait`调用后被处理。因此不会像`select`和`poll`一样有无差别轮询的问题，时间复杂度为O(1)



###### ET-edge-triggered模式（高速模式）

ET模式下，`read`一个fd的时候一定要把它的`buffer`读光，也就是说一直读到`read`的返回值小于请求值，或者 遇到`EAGAIN`错误。还有一个特点是，`epoll`使用“事件”的就绪通知方式，通过`epoll_ctl`注册fd，一旦该fd就绪，内核就会采用类似`callback`的回调机制来激活该fd，`epoll_wait`便可以收到通知。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2018110814591325.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhYWlrdWFpY2h1YW4=,size_16,color_FFFFFF,t_70)

###### LT-level-triggered模式（默认模式）

LT模式下，只要这个fd还有数据可读，每次 `epoll_wait`都会返回它的事件，提醒用户程序去操作，而在ET（边缘触发）模式中，它只会提示一次，直到下次再有数据流入之前都不会再提示了，无 论fd中是否还有数据可读。



###### epoll为什么要有ET触发模式？

如果采用LT模式的话，系统中一旦有大量不需要读写的就绪文件描述符，它们每次调用`epoll_wait`都会返回，这样**会大大降低处理程序检索自己关心的就绪文件描述符的效率**。而采用ET这种边沿触发模式的话，当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。

如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用`epoll_wait`时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比LT效率高，系统不会充斥大量你不关心的就绪文件描述符

###### 优点

- 没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）；
- 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；即`Epoll`最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，`Epoll`的效率就会远远高于`select`和`poll`。
- 内存拷贝，利用`mmap`()文件映射内存加速与内核空间的消息传递；即`epoll`使用`mmap`减少复制开销。



## 虚拟文件系统VFS

虚拟文件系统（`Virtual File System`，简称`VFS`）是`Linux`内核的子系统之一，`VFS`是一个抽象层，其向上提供了统一的文件访问接口，而向下则兼容了各种不同的文件系统。它为用户程序提供文件和文件系统操作的统一接口，屏蔽不同文件系统的差异和操作细节。借助`VFS`可以直接使用`open()`、`read()`、`write()`这样的系统调用操作文件，而无须考虑具体的文件系统和实际的存储介质。

举个例子，`Linux`用户程序可以通过`read()` 来读取`ext3`、`NFS`、`XFS`等文件系统的文件，也可以读取存储在`SSD`、`HDD`等不同存储介质的文件，无须考虑不同文件系统或者不同存储介质的差异。

通过`VFS`系统，`Linux`提供了通用的系统调用，可以跨越不同文件系统和介质之间执行，极大简化了用户访问不同文件系统的过程。另一方面，新的文件系统、新类型的存储介质，可以无须编译的情况下，动态加载到`Linux`中。

"一切皆文件"是`Linux`的基本哲学之一，不仅是普通的文件，包括目录、字符设备、块设备、套接字等，都可以以文件的方式被对待。实现这一行为的基础，正是`Linux`的虚拟文件系统机制。



### 基本原理

`VFS`之所以能够衔接各种各样的文件系统，是因为它抽象了一个通用的文件系统模型，定义了通用文件系统都支持的、概念上的接口。新的文件系统只要支持并实现这些接口，并注册到`Linux`内核中，即可安装和使用。

![img](https://pic1.zhimg.com/80/v2-37194483aa2f80dbb59b6526835fcfac_720w.jpg)



![image-20211008195833343](https://tva1.sinaimg.cn/large/008i3skNgy1gv85ivvzw1j60u00u8q5z02.jpg)

