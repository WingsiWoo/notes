# 操作系统

仅针对Linux~

## IO

### 文件描述符

在`Linux`下，万物皆文件，而为了让一个应用程序对应上一个文件，就产生了文件描述符（`File Descriptor`，简称`fd`）。当应用程序请求内核打开/新建一个文件时，内核会返回一个文件描述符用于对应这个打开/新建的文件，其fd本质上就是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于`UNIX`、`Linux`这样的操作系统。

#### fd唯一性

每个进程都维护着一个自己专属的文件描述符表，因此进程+文件描述符可以确定唯一一个文件，而同一个文件在不同进程中的fd可能是不一样的。



### 多路IO复用

#### 基本概念

##### 什么是多路IO复用？

IO多路复用是一种同步IO模型，实现一个线程可以监视多个文件句柄；一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；没有文件句柄就绪时会阻塞应用程序，交出cpu。多路是指网络连接，复用指的是同一个线程

##### 为什么要采用多路IO复用？

没有该机制时，只有`BIO`和`NIO`（阻塞IO和非阻塞IO）。采用阻塞IO的进程在发出IO请求后，会一直阻塞等待，直到IO返回成功或失败；非阻塞IO的轮询操作十分浪费CPU资源



#### 三种实现方式

|                |                       select                       |                     poll                     |                            epoll                             |
| :------------: | :------------------------------------------------: | :------------------------------------------: | :----------------------------------------------------------: |
|  **数据结构**  |                       bitmap                       |                     数组                     |                            红黑树                            |
| **最大连接数** |                        1024                        |                     不限                     |                             不限                             |
|   **fd拷贝**   |                每次调用select时拷贝                |              每次调用poll时拷贝              |    fd首次调用epoll_ctl时拷贝，每次调用epoll_wait时不拷贝     |
|  **工作效率**  |                      轮询O(n)                      |                   轮询O(n)                   |                           回调O(1)                           |
|  **主要开销**  |            内核判断是否有文件描述符就绪            |         内核判断是否有文件描述符就绪         | 如果存在很多短期活跃连接，系统调用开销可能会大于select和poll |
|  **轮询问题**  | select和poll都需要不断轮询所有的fd集合直到设备就绪 |        期间可能需要睡眠和唤醒多次交替        |                调用epoll_wait不断轮询就绪链表                |
|    **适用**    |    当监测的fd数量较小，且各个fd都很活跃的情况下    | 当监测的fd数量较小，且各个fd都很活跃的情况下 |      当监听的fd数量较多，且单位时间仅部分fd活跃的情况下      |

> `select`和`poll`需要把所有的`fd`从用户态复制到内核态，然后在内核态状态下轮询这些`fd`去寻找已经就绪的，会造成巨大的资源浪费，因此能处理的并发连接少很多
>
> 而`epoll`则把它们拆分成三部分：
>
> 1. 调用`epoll_create`创建`epoll`对象（在`epoll`文件系统中给这个句柄分配资源）
> 2. 调用`epoll_ctl`向`epoll`对象中添加`fd`（此时会把这些`fd`复制到内核态中）
> 3. 调用`epoll_wait`收集就绪的`fd`



##### select

```c
// select的调用会阻塞到有文件描述符fd可以进行IO操作或被信号打断或者超时才会返回
int select(int nfds, fd_set *readfds, fd_set *writefds,
                fd_set *exceptfds, struct timeval *timeout);

// 把fd从set中移除
void FD_CLR(int fd, fd_set *set);
// 检测一个文件描述符是否在组中，用该方法来检测一次select调用之后有哪些文件描述符可以进行IO操作
int  FD_ISSET(int fd, fd_set *set);
// 添加fd到set中
void FD_SET(int fd, fd_set *set);
// 用来清空fd_set，每次调用select前都要先调用FD_ZERO
void FD_ZERO(fd_set *set);
```

- `select`把`fd`分为三组：

  1. `readfds`：需要进行读操作的文件描述符
  2. `writefds`：需要进行写操作的文件描述符
  3. `exceptfds`：需要进行异常事件处理的文件描述符

  > 如果对应参数为`NULL`则说明该类事件不需要监听

  `select`返回后，原来传入的参数`readfds`等会被修改，只剩下可以进行对应IO操作的`fd`

- `select`可同时监听的文件描述符数量是通过`FS_SETSIZE`来限制的，在`Linux`系统中，该值为1024，当然我们可以增大这个值，但随着监听的文件描述符数量增加，`select`的效率会降低



###### 缺点

- 单个进程所打开的FD（文件描述符）是有限制的，通过`FD_SETSIZE`设置，默认1024

- 每次调用`select`，都需要把`fd`集合从用户态拷贝到内核态，这个开销在fd`很多`时会很大

  > 所有文件描述符都是在用户态被加入其文件描述符集合的，因此每次调用都需要将整个集合拷贝到内核态
  >
  > 因为是拷贝整个集体，因此很多没有就绪的`fd`也会被拷贝，这是没有必要的工作。并且由于每次调用`select`都会进行拷贝，一个`fd`可能会被拷贝多次

- 对`socket`扫描时是线性扫描，采用轮询的方法，效率较低（高并发时）

  > 因为`select`方法仅仅知道有IO事件发生，但是并不知道是哪几个流发生了，因此只能无差别的轮询所有流去寻找可操作的（查询fd对应的设备状态）

##### poll

```c
/**
 * poll只有一个pollfd数组，数组中的每个元素都表示一个需要监听IO操作事件的文件描述符。
 * events参数是我们需要关心的事件，
 * revents是所有内核监测到的事件
 */
struct pollfd {
        int fd; /* file descriptor */
        short events; /* requested events to watch */
        short revents; /* returned events witnessed */
    };

int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

###### 缺点

- 每次调用`poll`，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大

  > 与`select`同理，fd很多需要拷贝的就很多

- 对`socket`扫描时是线性扫描，采用轮询的方法，效率较低（高并发时）

  > 与`select`同理，只不过没有大小限制，因为其是基于链表实现的

  

##### epoll

```c
struct eventpoll {
    /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/
    struct rb_root  rbr;
    /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/
    struct list_head rdlist;
};

// 创建epoll实例，并把所有需要监听的事件放到epoll实例中的rbr中
int epoll_create(int size);
int epoll_create1(int flags);
// 往epoll实例中增删改要监测的文件描述符fd
// 每次注册新的事件到epoll句柄中时，会把fd拷贝进内核，在调用epoll_wait时无需拷贝，从而保证了每个fd在整个过程中只会被拷贝一次
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
// 用于阻塞的等待可以执行IO操作的文件描述符直到超时
// 这个方法实际上只需要检查rdlist里面是否有数据即可，有数据就返回，没数据就休眠直到超时结束
int epoll_wait(int epfd, struct epoll_event *events,
               int maxevents, int timeout);
int epoll_pwait(int epfd, struct epoll_event *events,
                int maxevents, int timeout,
                const sigset_t *sigmask);
```

- 只有`Linux`中有实现
- `epoll`则将整个文件描述符集合维护在内核态，每次添加文件描述符的时候（调用`epoll_ctl`）都需要执行一个系统调用
- `epoll`实际上是事件驱动，当有活动产生时，会自动触发epoll回调函数通知`epoll`文件描述符，然后内核将这些就绪的文件描述符放到`rdlist`中等待`epoll_wait`调用后被处理。因此不会像`select`和`poll`一样有无差别轮询的问题，时间复杂度为O(1)



###### ET-edge-triggered模式（高速模式）

ET模式下，`read`一个fd的时候一定要把它的`buffer`读光，也就是说一直读到`read`的返回值小于请求值，或者 遇到`EAGAIN`错误。还有一个特点是，`epoll`使用“事件”的就绪通知方式，通过`epoll_ctl`注册fd，一旦该fd就绪，内核就会采用类似`callback`的回调机制来激活该fd，`epoll_wait`便可以收到通知。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2018110814591325.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhYWlrdWFpY2h1YW4=,size_16,color_FFFFFF,t_70)

###### LT-level-triggered模式（默认模式）

LT模式下，只要这个fd还有数据可读，每次 `epoll_wait`都会返回它的事件，提醒用户程序去操作，而在ET（边缘触发）模式中，它只会提示一次，直到下次再有数据流入之前都不会再提示了，无 论fd中是否还有数据可读。



###### epoll为什么要有ET触发模式？

如果采用LT模式的话，系统中一旦有大量不需要读写的就绪文件描述符，它们每次调用`epoll_wait`都会返回，这样**会大大降低处理程序检索自己关心的就绪文件描述符的效率**。而采用ET这种边沿触发模式的话，当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。

如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用`epoll_wait`时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比LT效率高，系统不会充斥大量你不关心的就绪文件描述符

###### 优点

- 没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）；
- 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；即`Epoll`最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，`Epoll`的效率就会远远高于`select`和`poll`。
- 内存拷贝，利用`mmap`()文件映射内存加速与内核空间的消息传递；即`epoll`使用`mmap`减少复制开销。



## 虚拟文件系统VFS

虚拟文件系统（`Virtual File System`，简称`VFS`）是`Linux`内核的子系统之一，`VFS`是一个抽象层，其向上提供了统一的文件访问接口，而向下则兼容了各种不同的文件系统。它为用户程序提供文件和文件系统操作的统一接口，屏蔽不同文件系统的差异和操作细节。借助`VFS`可以直接使用`open()`、`read()`、`write()`这样的系统调用操作文件，而无须考虑具体的文件系统和实际的存储介质。

举个例子，`Linux`用户程序可以通过`read()` 来读取`ext3`、`NFS`、`XFS`等文件系统的文件，也可以读取存储在`SSD`、`HDD`等不同存储介质的文件，无须考虑不同文件系统或者不同存储介质的差异。

通过`VFS`系统，`Linux`提供了通用的系统调用，可以跨越不同文件系统和介质之间执行，极大简化了用户访问不同文件系统的过程。另一方面，新的文件系统、新类型的存储介质，可以无须编译的情况下，动态加载到`Linux`中。

"一切皆文件"是`Linux`的基本哲学之一，不仅是普通的文件，包括目录、字符设备、块设备、套接字等，都可以以文件的方式被对待。实现这一行为的基础，正是`Linux`的虚拟文件系统机制。



### 基本原理

`VFS`之所以能够衔接各种各样的文件系统，是因为它抽象了一个通用的文件系统模型，定义了通用文件系统都支持的、概念上的接口。新的文件系统只要支持并实现这些接口，并注册到`Linux`内核中，即可安装和使用。

![img](https://pic1.zhimg.com/80/v2-37194483aa2f80dbb59b6526835fcfac_720w.jpg)



![image-20211008195833343](https://tva1.sinaimg.cn/large/008i3skNgy1gv85ivvzw1j60u00u8q5z02.jpg)



## 内存分布

### 两级保护机制

![image-20211009215511056](https://tva1.sinaimg.cn/large/008i3skNgy1gv9eij2bvmj60jg0cvmxm02.jpg)

`Linux`使用两级保护机制：0级供内核使用，3级供用户程序使用，每个进程有各自的私有用户空间（0～3G），这个空间对系统中的其他进程是不可见的，最高的1GB字节虚拟内核空间则为所有进程以及内核所共享。

当一个程序被编译为一个可执行文件时，就已经被划分为图示几段：

|    段名     |                    存储内容                     |    存储属性    | 分配方式 |
| :---------: | :---------------------------------------------: | :------------: | :------: |
| 代码段.text |              存放可执行程序的指令               | 存储态和运行态 |   静态   |
| 数据段.data |    存放已初始化的非零全局变量和静态局部变量     | 存储态和运行态 |   静态   |
|  BSS段.bss  | 存放未初始化或初始化为0的全局变量和静态局部变量 | 存储态和运行态 |   静态   |
|   堆Heap    |                        -                        |   只有运行态   |   动态   |
|   栈Stack   |        存放函数局部变量、参数以及返回值         |   只有运行态   |   静态   |



### 为什么要分段？

CPU是个自动化程度极高的芯片，只要给它第一个指令的初始地址，它就能执行本条指令并获取下一条指令的地址。（获取方式：下一条指令的起始地址=当前指令的起始地址+当前指令大小）。

这种获取方式也就意味着程序中的指令是挨着存放在一块的（即使两条指令中间因为为了对齐而存在空隙，但是仅仅只是物理上将其断开了，可以使用jmp指令跳过这些空隙以保持指令在逻辑上连续）。

因此为了让程序内指令接连不断的执行，把所有指令排列在一起形成一片连续的指令区域，就是代码段。

数据和代码分开存放的好处：

1. 可以为它们赋予不同属性，如若数据本身需要修改，其就应该有可写的属性，但程序中的代码是不能被修改的，应该具备只读的属性。
2. 提高CPU内部缓存的命中率：程序的局部性原理，CPU内部有针对数据和针对指令的两种缓存机制
3. 节省内存：程序中存在一些只读的部分，不会改变，比如代码，当一个程序的多个副本同时运行时，没必要在内存中同时存有多个相同的代码段，只需要把这一个代码段进行共享即可



### 堆与栈

|              |                              栈                              |                              堆                              |
| :----------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| **管理方式** |              编译器自动管理，无需程序员手工控制              |      堆空间的申请释放工作由程序员控制，容易产生内存泄漏      |
| **空间大小** | 是一块连续的内存区域，栈顶的地址和栈的最大容量是系统规定好的 | 是不连续的内存区域，系统使用链表来存储空闲内存地址，堆的可用大小得看内存空闲情况 |
| **增长方向** |                       从高地址向低地址                       |               从低地址向高地址（链表遍历方向）               |
| **分配方式** |           栈的分配和释放由编译器完成，无需手工完成           | 堆都是由程序员手动调用malloc函数动态分配内存，再调用free函数释放内存的 |
| **分配效率** | 栈是操作系统提供的数据结构，计算机底层会对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行 |       堆由C函数库提供，实现机制很复杂，效率比栈低很多        |
|   **优点**   | 效率高，不会产生内存碎片；内存管理简单（无需手动分配和回收）；有更好的空间局部性 |      如果系统当前空闲内存比较大，堆可用的内存也会比较大      |
|   **缺点**   |                   用户能从栈获得的空间较小                   | 效率低下，会产生内存碎片；需要手动分配和回收（需要花费额外的CPU）； |



### 内存管理

由于栈的分配和释放都是由编译器完成的，因此一般说的内存管理就是堆的内存管理。



#### 动态分配与静态分配

动态分配指需要程序员手动进行分配和释放内存，通过`malloc`申请内存，通过`free`释放，如果只申请不释放可能会造成内存泄漏

静态分配指在编译时就已经决定好了需要分配多少内存，由编译器进行分配，在进程结束后会由系统释放





## 零拷贝

### 什么是零拷贝？

零拷贝（`Zero-copy`）技术，狭义上是指计算机执行操作时，`CPU`不需要先将数据从某处内存复制到另一个特定区域；但实际中很难真正做到应用场景下的完全零复制，广义上讲可以说是减少冗余拷贝的I/O优化技术。



### 为什么需要零拷贝？

在没有零拷贝技术前，I/O过程中至少需要两次拷贝，这两次拷贝都会一直占用`CPU`，以`read()`调用为例：

1. 当用户进程调用`read`，`CPU`会从用户态转换为内核态，向`CPU`发送IO请求
2. `CPU`收到IO请求后，会把数据放入到磁盘控制器的内部缓冲区中，然后产生中断信号
3. `CPU`收到中断信号后，阻塞当前进程，然后把磁盘控制器缓冲区中的数据拷贝到内核缓冲区中
4. `CPU`再把内核缓冲区内的数据拷贝到用户空间中，并且这两次拷贝期间`CPU`无法执行其他任务
5. 两次拷贝完成后，`read`调用完成，`CPU`从内核态转换为用户态

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/I_O%20%E4%B8%AD%E6%96%AD.png)



### 实现技术

#### DMA（直接内存访问）技术

DMA（Direct Memory Access），就是在进行IO设备和内存的数据传输时，搬运数据的工作不再由CPU来完成，而是交给DMA控制器来完成，这样在数据搬运期间CPU就可以去完成其他工作。

> 但数据传输过程CPU仍然是必不可少的，因为要传输什么数据，从哪里传输到哪里都需要CPU来指挥，DMA并不具有这样的功能

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png)

1. 用户进程调用`read()`方法，`CPU`从用户态切换为内核态，向操作系统发出IO请求，请求读取数据到自己的内存缓冲区中，并阻塞进程
2. 操作系统收到IO请求后，把IO请求发送到`DMA`，然后唤醒进程使`CPU`继续执行任务
3. `DMA`把收到的IO请求进一步发送给磁盘
4. 磁盘收到IO请求后，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向`DMA`发送中断信号
5. `DMA`收到中断信号后，把磁盘缓冲区中的数据拷贝到内核缓冲区中，**本过程不占用`CPU`**
6. `DMA`读取足够多数据后发送中断信号给CPU
7. `CPU`收到中断信号后就会阻塞当前进程，把数据从内核拷贝到用户空间。至此，`read()`调用完成，`CPU`从内核态切换为用户态



##### DMA的应用

可参考https://www.yisu.com/zixun/459515.html



##### 存在问题

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)

以文件传输为例，需要进行一次`read()`调用和一次`write()`调用。

从图中看出，需要拷贝4次，其中两次是`DMA`拷贝，两次是`CPU`拷贝；`CPU`要在用户态和内核态之间来回切换4次。

> - *第一次拷贝*，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 `DMA` 搬运的。
> - *第二次拷贝*，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 `CPU` 完成的。
> - *第三次拷贝*，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 `socket` 的缓冲区里，这个过程依然还是由 `CPU` 搬运的。
> - *第四次拷贝*，把内核的 `socket` 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 `DMA` 搬运的。
>
>  
>
> - *第一次切换*，是`read()`调用开始，`CPU`从用户态切换到内核态
> - *第二次切换*，是`read()`调用完成，`CPU`从内核态转换回用户态
> - *第三次切换*，是`write()`调用开始，`CPU`从用户态切换到内核态
> - *第四次切换*，是`write()`调用结束，`CPU`从内核态转换回用户态

==所以，要想提高文件传输的性能，就要减少**用户态与内核态的上下文切换**和**内存拷贝**的次数==



###### 如何优化？

- **减少用户态与内核态的上下文切换的次数**

  读取磁盘数据时，之所以要发生上下文切换，这是因为用户态没有权限操作磁盘或网卡，因此需要切换到内核态去使用操作系统提供的系统调用函数来操作。而一次系统调用就必然会发生2次上下文切换：任务开始时，从用户态切换到内核态；任务完成后，从内核态切换为用户态。上图就是因为存在`read()`和`write()`两次系统调用，所以切换了4次。

  ==所以，要想减少上下文切换的次数，就要减少系统调用的次数==

- **减少内存拷贝的次数**

  先看看数据转移的位置：

  ![image-20211018154925613](https://tva1.sinaimg.cn/large/008i3skNgy1gvjiiovbz8j60io03b74i02.jpg)

  在文件传输的应用场景中，在用户空间并不会对数据进行再加工（即把文件原封不动的写回到内存），所以数据实际上可以不用搬运到用户空间，因此用户缓冲区是没有必要存在的



#### mmap+write

这种方式主要是减少内存拷贝的次数，为了减少把数据从内核缓冲区拷贝到用户的缓冲区这里，使用`mmap()`替换`read()`系统调用函数

`mmap()` 系统调用函数会直接把内核缓冲区里的数据「**映射**」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

<img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20+%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:67%;" />

- 应用进程调用了 `mmap()` 后，`DMA` 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核**「共享」**这个缓冲区；
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 `socket` 缓冲区中，这一切都发生在内核态，由 `CPU` 来搬运数据；
- 最后，把内核的 `socket` 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 `DMA` 搬运的。

> 减少了一次拷贝（`mmap()`1次+`write()`2次）
>
> ==3次拷贝，上下文切换仍有4次==，因为仍然有2次系统调用



#### sendfile

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`，函数形式如下：

```c
#include <sys/socket.h>
// out_fd-目的端文件描述符，in_fd-源端文件描述符
// offset-源端偏移量，count-复制数据的长度
// ssize_t实际复制数据的长度
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

首先，它可以替代前面的 `read()` 和 `write()` 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。

其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 `socket` 缓冲区里，不再拷贝到用户态，这样就==只有 2 次上下文切换，和 3 次数据拷贝==。如下图：

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png)

但是这还不是真正的零拷贝技术，如果网卡支持 `SG-DMA（The Scatter-Gather Direct Memory Access）`技术（和普通的 `DMA` 有所不同），我们可以进一步减少通过 `CPU` 把内核缓冲区里的数据拷贝到 `socket` 缓冲区的过程（这样就只进行了2次数据拷贝）。



#### 写时复制

写时复制是计算机编程中的一种优化策略，它的基本思想是这样的：如果有多个应用程序需要同时访问同一块数据，那么可以为这些应用程序分配指向这块数据的指针，在每一个应用程序看来，它们都拥有这块数据的一份数据拷贝，当其中一个应用程序需要对自己的这份数据拷贝进行修改的时候，就需要将数据真正地拷贝到该应用程序的地址空间中去，也就是说，该应用程序拥有了一份真正的私有数据拷贝，这样做是为了避免该应用程序对这块数据做的更改被其他应用程序看到。

这个过程对于应用程序来说是透明的，**如果应用程序永远不会对所访问的这块数据进行任何更改，那么就永远不需要将数据拷贝到应用程序自己的地址空间中去。**

这也是写时复制的最主要的优点。

写时复制的实现**需要 `MMU` 的支持**，`MMU` 需要知晓进程地址空间中哪些特殊的页面是只读的，当需要往这些页面中写数据的时候，`MMU` 就会发出一个异常给操作系统内核，操作系统内核就会分配新的物理存储空间，即将被写入数据的页面需要与新的物理存储位置相对应。

写时复制的最大好处就是可以节约内存。

不过对于操作系统内核来说，写时复制增加了其处理过程的复杂性。



#### 共享缓冲区

用户态和内核态共享缓冲区，目前仍不成熟

