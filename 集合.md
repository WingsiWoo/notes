# 集合

## ArrayList

`ArrayList`实现了`List`接口，是顺序容器，即元素存放的数据与放进去的顺序相同，允许放入`null`元素，底层通过**数组实现**。除该类未实现同步外，其余跟`Vector`大致相同。每个`ArrayList`都有一个容量(`capacity`)，表示底层数组的实际大小，容器内存储元素的个数不能多于当前容量。当向容器中添加元素时，如果容量不足，容器会自动增大底层数组的大小。前面已经提过，Java泛型只是编译器提供的语法糖，所以这里的数组是一个Object数组，以便能够容纳任何类型的对象。

`size(),` `isEmpty()`, `get(),` `set()`方法均能在常数时间内完成，`add()`方法的时间开销跟插入位置有关，`addAll()`方法的时间开销跟添加元素的个数成正比。其余方法大都是线性时间。

为追求效率，`ArrayList`没有实现同步(synchronized)，如果需要多个线程并发访问，用户可以手动同步，也可使用`Vector`替代。



### 属性分析

```java
// 默认的初始容量
private static final int DEFAULT_CAPACITY = 10;

private static final Object[] EMPTY_ELEMENTDATA = {};

private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};

// 用于存储数据的数组
transient Object[] elementData; 

// 数组大小
private int size;
```

1. `EMPTY_ELEMENTDATA`和`DEFAULTCAPACITY_EMPTY_ELEMENTDATA`（以下简称为`EE`和`DEE`）

   这两个实际上都是共享空数组，只是名字不一样而已。

   - 调用无参构造器创建的实例是DEE，调用有参构造器（参数为0）创建的实例是EE

   - 在`add(E e)`方法中，会先判断`elementData == DEE`，如果为`true`，说明这个`ArrayList`是调用无参构造器创建的，那么就会直接扩容到默认的最小初始容量`DEFAULT_CAPACITY（10）`

     如果不为`true`，说明这个`ArrayList`是调用有参构造器创建的空实例，它的容量增长是这样的：1→2→3→4→5→6→7→8→9→13（9→13的增长规律在之后讲解`grow()`方法时细🔐）

   - `EE`的存在是为了优化创建`ArrayList`空实例时产生不必要的空数组：在添加第一个元素前所有`ArrayList`空实例都指向`EE`这个空数组，节省了大量的内存空间

   - `DEE`是为了确保无参构造器创建的实例在添加第一个元素时，最小的容量为默认大小10，避免出现像`EE`那样容量缓慢增长需要多次扩容的情况。针对有参无参的构造在扩容时做区分走不通的扩容逻辑，优化性能。

   - `Java8`中才添加了`DEE`，在`Java7`及之前之有EE`表示`空数组

2. `ArrayList`底层中使用`Object[]`数组来存储数据。

   **为什么使用transient修饰？**

   首先要说明`transient`的作用：  `transient`用来表示一个域不是该对象序行化的一部分，当一个对象被序行化的时候，`transient`修饰的变量的值是不包括在序行化的表示中的。也就是说，`ArrayList`序列化后是没有`elementData`的。

   **那这样序列化后数据岂不是丢失了？**

   其实`ArrayList`中提供了两个方法专门用来对`elementData`序列化和反序列化：`writeObject()`和`readObject()`。

   > `ArrayList`在序列化的时候会调用`writeObject`，直接将`size`和`element`写入`ObjectOutputStream`；
   >
   > 反序列化时调用`readObject`，从`ObjectInputStream`获取`size`和`element`，再恢复到`elementData`。

   **为什么对于elementData不直接序列化而要使用上述的方法？**

   `elementData`是一个缓存数组，它通常会预留一些空间来存储新的元素，也就是说`elementData`中的部分空间可能是没有实际存储元素的，如果采用上述两个方法进行序列化，就可以保证只实例化那些实际存储的元素，而不是整个数组，从而节省空间和时间。



### 自动扩容

#### 检查是否需要扩容–调整minCapacity的值

在调用`add(E e)`方法插入元素时，会先调用`ensureCapacityInternal(size + 1)`来检查是否需要给数据扩容。

```java
// minCapacity即size+1
private void ensureCapacityInternal(int minCapacity) {
  	ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));
}

private static int calculateCapacity(Object[] elementData, int minCapacity) {
    if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {
      return Math.max(DEFAULT_CAPACITY, minCapacity);
    }
    return minCapacity;
}

private void ensureExplicitCapacity(int minCapacity) {
    modCount++;

    // overflow-conscious code
    if (minCapacity - elementData.length > 0)
      grow(minCapacity);
}
```

1. 在`calculateCapacity`方法中，会先判断`elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA`，如果为`true`，就返回`DEFAULT_CAPACITY`和`minCapacity`中的较大者，否则直接返回`minCapacity`

   > 所以前面说调用`add(E e)`方法添加第一个元素时，对`DEE`会直接扩容到10（因为此时`minCapacity`为1，一定比`DEFAULT_CAPACITY`小）；而对`EE`只会扩大1（直接返回`minCapacity`）

2. `ensureExplicitCapacity`中使用到的`modCount`是继承自`AbstractList`的成员变量，它记录着集合修改的次数，也就是说每次`add`或`remove`等时都会`modCount++`。

   在对一个集合对象进行迭代时，并不会写限制对集合操作的方法。但如果在迭代过程中调用`add`、`remove`这种会影响集合数据的方法，可能会影响迭代中的数据混乱从而引起错误。我们希望提供一种快失败机制，尽最大可能去抛出异常去避免这样的情况出现。

   因此使用了`modCount`，迭代器中有一个属性为`expectedModCount`，它的初始值就是`modCount`。在调用像`add`、`remove`这类方法前会先调用`checkForComodification()`来检查`expectedModCount`和`modCount`是否相等，如果不相等则抛出`ConcurrentModificationException`异常

   > 如果没`checkForComodification`去检查`expectedModCount`与`modCount`相等，这个程序肯定会报`ArrayIndexOutOfBoundsException`，这显然是不符合逻辑的。

3. 如果最小需要空间`minCapacity > elementData`的长度，说明需要扩容，调用`grow`方法扩容到`minCapacity`



#### 扩容

数组每次扩容会扩展成原容量的**1.5倍**，然后再根据`minCapacity`的值去做相关操作。所以前面提及到的9→13就是9+9/2=13

```java
private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

private void grow(int minCapacity) {
    // 扩容前数组的长度
    int oldCapacity = elementData.length;
   // 新容量为旧容量的1.5倍
    int newCapacity = oldCapacity + (oldCapacity >> 1);
  // 如果扩容后仍然比所需空间小，就把数组长度直接设置为需要的长度
    if (newCapacity - minCapacity < 0)
        newCapacity = minCapacity;
  // 判断数组长度是否溢出
    if (newCapacity - MAX_ARRAY_SIZE > 0)
        newCapacity = hugeCapacity(minCapacity);
    // 如果扩容后空间足够了，就创建新的数组就可以了
    elementData = Arrays.copyOf(elementData, newCapacity);
}

// 用于处理数组长度溢出的情况
private static int hugeCapacity(int minCapacity) {
    if (minCapacity < 0) // overflow
        throw new OutOfMemoryError();
  // 如果所需空间也溢出则新数组长度为MAX_VALUE，否则为默认的最大的数组长度
    return (minCapacity > MAX_ARRAY_SIZE) ?
        Integer.MAX_VALUE :
        MAX_ARRAY_SIZE;
}
```

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwkhttt6l7j30u01d1wkt.jpg" alt="image-20211119153122874" style="zoom:67%;" />



### remove

`remove()`方法也有两个版本，一个是`remove(int index)`删除指定位置的元素，另一个是`remove(Object o)`删除第一个满足`o.equals(elementData[index])`的元素。删除操作是`add()`操作的逆过程，需要将删除点之后的元素向前移动一个位置。**需要注意的是为了让GC起作用，必须显式的为最后一个位置赋`null`值。**

```java
public E remove(int index) {
    rangeCheck(index);
    modCount++;
    E oldValue = elementData(index);
    int numMoved = size - index - 1;
    if (numMoved > 0)
        System.arraycopy(elementData, index+1, elementData, index, numMoved);
    elementData[--size] = null; //清除该位置的引用，让GC起作用
    return oldValue;
}
    
```

> 关于Java GC这里需要特别说明一下，**有了垃圾收集器并不意味着一定不会有内存泄漏**。对象能否被GC的依据是是否还有引用指向它，上面代码中如果不手动赋`null`值，除非对应的位置被其他元素覆盖，否则原来的对象就一直不会被回收。



### toArray异常

`ArrayList`提供两个转换成数组的方法

```java
Object[] toArray()
<T> T[] toArray(T[] a)
```

如果调用`toArray()`方法可能会抛出`ClassCastException`异常，但调用`toArray(T[] a)`则不会。这是因为`toArray()`方法返回`Object[]`数组，将`Object[]`转换成其他类型可能会引起异常，因为Java不支持向下转型



## Stack&Queue

**为什么现在官方不推荐使用Stack实现栈？**

Stack继承了Vector类，一点是现在官方也不推荐使用Vector。

另一点是Vector是动态数组，这意味着它可以在数组中任意位置添加和删除元素，Stack继承了Vector中所有的公有方法，而栈只能对栈顶元素进行操作，因此这样的设计破坏了对栈的数据结构的封装。



### Queue

`Queue`接口继承自`Collection`接口，除了最基本的`Collection`的方法之外，它还支持额外的*`insertion`*, *`extraction`*和*`inspection`*操作。这里有两组格式，共6个方法，一组是抛出异常的实现；另外一组是返回值的实现(没有则返回`null`)。

|         | Throws exception | Returns special value |
| :-----: | :--------------: | :-------------------: |
| Insert  |      add(e)      |       offer(e)        |
| Remove  |     remove()     |        poll()         |
| Examine |    element()     |        peek()         |



### Deque

`Deque`是"double ended queue", 表示双向的队列，英文读作"deck". Deque 继承自 Queue接口，除了支持Queue的方法之外，还支持`insert`, `remove`和`examine`操作，由于Deque是双向的，所以可以对队列的头和尾都进行操作，它同时也支持两组格式，一组是抛出异常的实现；另外一组是返回值的实现(没有则返回null)。共12个方法如下:

|         | First Element - Head |               | Last Element - Tail |               |
| :-----: | :------------------: | :-----------: | :-----------------: | :-----------: |
|         |   Throws exception   | Special value |  Throws exception   | Special value |
| Insert  |     addFirst(e)      | offerFirst(e) |     addLast(e)      | offerLast(e)  |
| Remove  |    removeFirst()     |  pollFirst()  |    removeLast()     |  pollLast()   |
| Examine |      getFirst()      |  peekFirst()  |      getLast()      |  peekLast()   |



#### ArrayDeque

`ArrayDeque`和`LinkedList`是`Deque`的两个通用实现，官方更推荐使用`AarryDeque`用作栈和队列

`ArrayDeque`底层使用数组实现，为了能满足可以在数组两端同时插入或删除元素的需求，该数组应该是一个**循环数组**，也就是说数组的任何一点都可能被看作起点或者终点。`ArrayDeque`是非线程安全的，当多个线程同时使用的时候，需要程序员手动同步；另外，该容器不允许放入`null`元素。

![ArrayDeque_base.png](https://pdai.tech/_images/collection/ArrayDeque_base.png)

> **`head`指向首端第一个有效元素，`tail`指向尾端第一个可以插入元素的空位**。因为是循环数组，所以`head`不一定总等于0，`tail`也不一定总是比`head`大。



##### 构造方法

1. `ArrayDeque`的空参构造方法默认创建大小为16的数组

   ```java
   public ArrayDeque() {
       elements = new Object[16];
   }
   ```

2. `ArrayDeque`的含参构造方法都会先调用一个`calculateSize(int numElements)`方法来求实际需要创建的数组大小

   - 如果`numElements < MIN_INITIAL_CAPACITY`，则该方法会返回`MIN_INITIAL_CAPACITY`，创建一个长度为8的数组
   - 如果`numsElements >= MIN_INITIAL_CAPACITY`，则该方法会返回大于`numsElements`的最接近的2的n次方

   ```java
   private static int calculateSize(int numElements) {
      // 最小容量，为8
       int initialCapacity = MIN_INITIAL_CAPACITY;
       // 算出大于numElements且不小于8的最接近的2的n次方
       if (numElements >= initialCapacity) {
           initialCapacity = numElements;
           initialCapacity |= (initialCapacity >>>  1);
           initialCapacity |= (initialCapacity >>>  2);
           initialCapacity |= (initialCapacity >>>  4);
           initialCapacity |= (initialCapacity >>>  8);
           initialCapacity |= (initialCapacity >>> 16);
           initialCapacity++;
   
           if (initialCapacity < 0)   // Too many elements, must back off
               initialCapacity >>>= 1;// Good luck allocating 2 ^ 30 elements
       }
      // 如果一开始的initialCapacity>numElements，即要求初始化的数组长度小于8，则实际上创建长度为8的数组（最小要求）
       return initialCapacity;
   }
   ```



##### 入队

1. `ArrayDeque`的入队有两种方法：从队头入队`addFirst(E e)`和从队尾入队`addLast(E e)`。

2. 使用取余来计算新元素的下标，`x & (len - 1) = x % len`，使用&的方式更快。

   `addFirst`中`x=head-1`，`addLast`中`x=tail+1`

3. 如果容量不够了，就扩大为两倍



##### 扩容

```java
private void doubleCapacity() {
 	 assert head == tail;
    // 头指针的位置
    int p = head;
    // 旧数组长度
    int n = elements.length;
    // 头指针离数组尾的距离
    int r = n - p; // number of elements to the right of p
    // 新长度为旧长度的两倍
    int newCapacity = n << 1;
    // 判断是否溢出
    if (newCapacity < 0)
        throw new IllegalStateException("Sorry, deque too big");
    // 新建新数组
    Object[] a = new Object[newCapacity];
    // 将旧数组head之后的元素拷贝到新数组中
    System.arraycopy(elements, p, a, 0, r);
    // 将旧数组下标0到head之间的元素拷贝到新数组中
    System.arraycopy(elements, 0, a, r, p);
    // 赋值为新数组
    elements = a;
    // head指向0，tail指向旧数组长度表示的位置
    head = 0;
    tail = n;
}
```

![img](https://pic3.zhimg.com/80/v2-387f8054f0d552eccf6a077506c1d512_720w.jpg)



## HashMap

### 属性分析

```java
// 默认的容量大小为16（map的大小都会初始化为2的n次方）
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16

// 最大的容量为2^30
static final int MAXIMUM_CAPACITY = 1 << 30;

// 默认的负载因子，当装载的数据达到容量的0.75就会进行rehash扩容
static final float DEFAULT_LOAD_FACTOR = 0.75f;

// JDK8新增，链表树化阈值，当桶中节点数大于该长度时，将链表转成红黑树存储
static final int TREEIFY_THRESHOLD = 8;

// JDK8新增，红黑树链化阈值，当同种节点数小于该长度，将红黑树转成链表存储
static final int UNTREEIFY_THRESHOLD = 6;

// 最小树化阈值，当桶中所有值都超过该值时才会进行树化
static final int MIN_TREEIFY_CAPACITY = 64;

// 哈希桶数组，其长度一定是2的n次方
transient Node<K,V>[] table;

//  数据转换成set的另一种存储形式，主要用于迭代
transient Set<Map.Entry<K,V>> entrySet;

// 记录当前map的大小
transient int size;

// 与ArrayList中的作用一致-快失败机制
transient int modCount;

// 扩容阈值，当map中存储的键值对超过该值时就扩容为原来的两倍
int threshold;

// 当前的负载因子，可通过这个计算出扩容阈值：threshold = loadFactor * table.length
final float loadFactor;
```

1. `TREEIFY_THRESHOLD`、`UNTREEIFY_THRESHOLD`、`MIN_TREEIFY_CAPACITY`这几个与红黑树相关的类常量为JDK8新增的。

   JDK7的`HashMap`底层数据结构为**数组+链表**，JDK8的底层数据结构为**数组+链表+红黑树**，在JDK7及以前无论数据有多少，一个桶中的数据都是用链表来存储的，所以当数据量过大时，花在寻找数据（遍历链表）的时间就非常大

2. 负载因子=0.75：在理想情况下，使用随机哈希吗，节点出现的频率在`hash`桶中遵循泊松分布。负载因子为0.75时，桶中元素达到8个的概率为0.00000006，也就是说每个碰撞位置的链表长度超过8个是几乎不可能的。（因为由链表转换成红黑树的操作是十分耗费性能的，所以要在保证`HashMap`查询效率的同时尽可能减少树化的概率）

3. 为什么`HashMap`的容量一定要是2的幂？

   `HashMap` 采用这种非常规设计，主要是**为了在取模和扩容时做优化，同时为了减少hash碰撞**，`HashMap` 定位哈希桶索引位置时，也加入了高位参与运算的过程。

   长度16或者其他2的幂，`Length-1`的值是所有二进制位全为1，这种情况下，`index`的结果等同于`HashCode`后几位的值。只要输入的`HashCode`本身分布均匀，`Hash`算法的结果就是均匀的。

   如果不为16或者2的幂，那么，经过`hash`算法之后，有些`index`结果出现的概率会更大，而有些`index`则永远步会出现。即不符合`hash`算法的j均匀分布的原则。



### 确定哈希桶数组索引位置-hash

```java
//经过两步操作最后得到的才是我们用来确定位置的hash值
    static final int hash(Object key) {
        int h;
        // h = key.hashCode() 为第一步 取hashCode值
        // h ^ (h >>> 16)  为第二步 高位参与运算
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }
//确定hash值之后的操作就是确定在哈希桶中的位置了
//下面是 put() 方法中的一行代码，n为哈希桶数组长度，hash为前一步确定的hash值
// 相当于取余，只不过位与运算的速度更快
    p = tab[i = (n - 1) & hash]
```

![img](https://upload-images.jianshu.io/upload_images/18499914-08b5602de447673d.png?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp)



### 扩容resize

```java
final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        //第一部分：排除异常情况，扩容
        if (oldCap > 0) {
           // 如果HashMap的容量已经到达最大容量了，不会再库容
            if (oldCap >= MAXIMUM_CAPACITY) {
               // 增大扩容阈值，保证之后基本都不会再尝试进行扩容了
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
          // 如果扩大到两倍后没有溢出且大于默认的初始化容量
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
               // 新的扩容阈值为原来的两倍
                newThr = oldThr << 1; // double threshold
        }
        //第二部分：设置阈值
        else if (oldThr > 0) // initial capacity was placed in threshold
           // 如果原来的thredshold大于0则将容量设为原来的thredshold
			// 在第一次带参数初始化时候会有这种情况
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
          // 在默认无参数初始化会有这种情况
           // 默认初始化最小容量为16
            newCap = DEFAULT_INITIAL_CAPACITY;
           // 扩容阈值=负载因子*容量=12
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
  		// 初始化新的扩容阈值
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
  
// 第三部分：移动数据到新map中
```

![image-20211119215033509](https://tva1.sinaimg.cn/large/008i3skNgy1gwksscftukj31b20tajxy.jpg)

```java
// 第三部分：移动数据到新map中  
@SuppressWarnings({"rawtypes","unchecked"})
    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    //第三部分：旧数据保存在新数组里面
    if (oldTab != null) {
        //遍历旧数组
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                oldTab[j] = null;
                 // 如果没有后续节点，则用取余操作来确定索引位置
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                //如果是红黑树，需要进行树拆分然后映射
                else if (e instanceof TreeNode)
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                // 如果是多个节点的链表，将原链表拆分为两个链表
                else { // preserve order
                   // 数组低位的头和尾
                    Node<K,V> loHead = null, loTail = null;
                   // 数组高位的头和尾
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    do {
                        next = e.next;
                        // 如果e.hash & oldCap == 0则当前节点不需要移位，直接映射到新数组对应相同索引的位置
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        } 
                        // 原索引 + oldCap
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                  
                    // 遍历完桶中所有节点后才把拆分完成的两个链表关联到新表中
                     // 链表1存于原索引
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    // 链表2存于原索引加上原hash桶长度的偏移量
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
```

![image-20211120104920633](https://tva1.sinaimg.cn/large/008i3skNgy1gwlfamlc0wj30u00w7jvz.jpg)

![img](https://pic4.zhimg.com/80/v2-3b60c7eae5642485e9a7fa2943aa753b_720w.jpg)



**e.hash & oldCap == 0为什么可以判断当前节点是否需要移位, 而不是再次计算hash**

仍然是原始长度为16举例:

```text
 old:
 10: 0000 1010
 oldCap-1=16-1=15: 0000 1111
 &: 000[0] 1010 
 
 new:
 10: 0000 1010
newCap-1=16*2-1= 31: 0001 1111
 &: 000[1] 1010 
```

从上面的示例可以很轻易的看出, 两次`indexFor()`的差别只是第二次参与位于比第一次左边有一位从0变为1, 而这个变化的1刚好是`oldCap`, 那么只需要判断原`key`的`hash`这个位上是否为1: 若是1, 则需要移动至`oldCap + i`的槽位, 若为0, 则不需要移动;

这也是`HashMap`的长度必须保证是2的倍数的原因, 正因为这种环环相扣的设计, `HashMap.loadFactor`的选值是3/4（0.75）就能理解了, `table.length * 3/4`可以被优化为`(table.length >> 2) << 2) - (table.length >> 2) == table.length - (table.lenght >> 2)`, （乘除运算优化为位运算）JAVA的位运算比乘除的效率更高, 所以取3/4在保证`hash`冲突小的情况下兼顾了效率;



#### JDK7中的resize死循环问题

在JDK1.7及以前的版本，如果在并发环境中使用`HashMap`保存数据，有可能会产生两个问题：死循环和数据丢失。

产生死循环问题是因为JDK1.7及以前的版本中，`HashMap`扩容采用的是头插入（导致结点顺序可能会颠倒），1.8做的改进是采用尾插法，所以不会造成死循环的问题，但是仍然无法解决数据丢失问题，所以JDK8中的`HashMap`仍然是线程不安全的。

下面是JDK7中扩容时可能会产生死循环问题的部分代码

```java
void transfer(Entry[] newTable, boolean rehash) {
        int newCapacity = newTable.length;
        for (Entry<K,V> e : table) {
            while(null != e) {
               // 暂存节点e原来的下一个结点
                Entry<K,V> next = e.next;     
               // 重新计算在新表中的hash值
                if (rehash) {
                    e.hash = null == e.key ? 0 : hash(e.key);
                }
                int i = indexFor(e.hash, newCapacity);
               // 节点e的下一个结点指向新数组中对应链表的头结点（头插）
                e.next = newTable[i];
               // 更新新数组中存储的头结点为e
                newTable[i] = e;
               // 遍历下一个结点
                e = next;
            }
        }
    }
```

下面还原一下JDK7中死循环问题的产生过程

1. 扩容前的HashMap如下

   ![image-20211120112741856](https://tva1.sinaimg.cn/large/008i3skNgy1gwlgej2pwuj30m80d6750.jpg)

2. 当试图插入第四个节点时，由于已经达到扩容阈值，所以会进行扩容操作

   ![image-20211120112848823](https://tva1.sinaimg.cn/large/008i3skNgy1gwlgfotgi1j30ok0iedh3.jpg)

   ![image-20211120112939272](https://tva1.sinaimg.cn/large/008i3skNgy1gwlggk7dcsj30ng0igmyd.jpg)

   ![image-20211120113033655](https://tva1.sinaimg.cn/large/008i3skNgy1gwlghigj7vj30ju0hswfi.jpg)

   可以看到扩容结束后ab结点的顺序调换了

3. 第二点中的扩容在单线程环境中是没有问题的，但如果是在多线程环境下呢？

   假设线程1和线程2都要进行扩容操作，线程2获取到原来的a节点后，由线程1完成了整个扩容操作。

   线程1完成后线程2会继续进行扩容。

   （1）线程二开始执行, 获取A节点的`next`节点, `a.next = null`;

   （2） 接着执行 `a.next = newTable[i]`; 因为这时候`newTable[i]`已经是B节点了, 并且`b.next = a`; 那么我们把`newTable[i]`赋值给`a.next`后, 就会线程`a-b-a`这样的环形链表了

   （3） 因为第三步的`a.next`已经是`null`, 所以C节点就丢失了

   （4） 之后如果想要查找位于桶1中的数据时就会陷入死循环了

   ![image-20211120114128522](https://tva1.sinaimg.cn/large/008i3skNgy1gwlgsv6clyj30h40hmdgm.jpg)



**JDK8的改进：**

1. JDK8使用尾插法，所以不会改变结点的顺序，就不会产生倒排问题
2. 而且JDK8是在遍历完所有节点之后, 才对形成的两个链表进行关联`table`的, 所以不会像JAVA7一般形成A-B-A问题了
3. 但是如果并发了, JAVA的`HashMap`还是没有解决丢数据的问题, 但是不和JAVA7一般有数据倒排以及死循环的问题了



### put

```java
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
   // 表为空，进行扩容操作
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
   // 对应的桶中没有数据，直接作为头结点放到桶中（其实就是没有发生碰撞）
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
  
    // 把新节点插入到链表或树中
    else {
        Node<K,V> e; K k;
       // 判断是否传入同一个key
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
       // 传入新的key
       // 插入到红黑树中
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
       // 插入到链表中
        else {
            for (int binCount = 0; ; ++binCount) {
               // 如果没有后续结点
                if ((e = p.next) == null) {
                   // 插入到链表的最后
                    p.next = newNode(hash, key, value, null);
                   // 树化
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
              // 使用hash值、内存、equals同时判断key与结点e的key是否相同
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
      
      // 如果该key已经存在于map中，则进行替换并返回旧值
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
  // 用于快失败机制
    ++modCount;
  // 如果已经达到扩容阈值，则调用resize进行扩容
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```

![image-20211120150927434](https://tva1.sinaimg.cn/large/008i3skNgy1gwlmta9z0sj30u011ddll.jpg)



### 红黑树

#### 红黑树的引入

> 有了二叉搜索树，为什么还要引入平衡二叉树？

- 极端情况下，二叉搜索树可能会退化成一条链，这样搜索的时间复杂度就会退化为O(n)
- 引入平衡二叉树，通过旋转操作保证二叉树的平衡性，可以保证最坏时间复杂度也在O(log~2~n)内

> 有了平衡二叉树，为什么还要引入红黑树？

- AVL树要求高度平衡，这就意味着几乎每次插入/删除都需要对树进行旋转调整，而旋转调整是一个非常费时的操作
- 在需要频繁进行插入/删除的场景下，AVL树的效率大打折扣
- 红黑树通过牺牲严格的平衡，换取少量的旋转操作：红黑树的插入最多两次旋转就可以解决，红黑树的删除最多三次旋转就可以解决
- 红黑树的红黑规则，可以保证最坏时间复杂度也在O(log~2~n)内



#### 红黑规则

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190409222157219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQ0NTQ1Mzg=,size_16,color_FFFFFF,t_70)

- 节点不是红色就是黑色，根节点一定是黑色的
- 红黑树的叶子结点并非传统的叶子结点，它的叶子结点是一个空结点，并且为黑色
- 同一路径上不存在连续的红色节点➡️一个节点为红色，其两个孩子节点一定都是黑色
- 任意一结点到每个叶子结点的路径都包含数量相同的黑结点➡️黑色完美平衡



#### 红黑树的插入

##### 1. 红黑树为空

直接把插入节点设为根节点，黑色



##### 2. 插入节点已存在

把插入节点设为与树中相同节点的颜色，然后更新树中节点的值为插入节点



##### 3. 插入节点的父节点为红色

###### 3.1 uncle节点存在并且为红色

1. 插入节点X设为红色
2. 把`parent`节点和`uncle`节点设为黑色
3. 把`grand parent`节点设为红色（**黑红红➡️红黑红**）
4. 把`grand parent`作为插入节点X，重复以上步骤继续向上改变颜色
5. 直到根节点结束，记住根节点要恢复为黑色

![image-20220113161226586](https://tva1.sinaimg.cn/large/008i3skNgy1gyc43hi8lyj30m70iv76s.jpg)

> 总体思路即把黑红红变为红黑红，这是唯一一种会增加黑色节点层数的插入场景
>
> 由变色的过程可以得知，红黑树是自底向上生成的，而AVL树是自顶向下生成的



###### 3.2 uncle节点不存在或为黑色，并且插入节点为父节点的左孩子

1. 把父节点设为黑色
2. 把祖父节点设为红色
3. 把祖父节点作为旋转支点，进行右旋（如果父节点为祖父节点的左孩子则右旋，如果为右孩子则左旋）

![image-20220113162741807](https://tva1.sinaimg.cn/large/008i3skNgy1gyc4jbefztj30to081jso.jpg)

> 这种情况无需向上调整

###### 3.3 uncle节点不存在或为黑色，并且插入节点为父节点的右孩子

对父节点进行左旋即可调整为情况3.2进行后续操作



#### 红黑树删除

##### 删除节点无子节点

直接删除即可

##### 删除节点只有一个子节点

用子节点替换删除节点后进行调整

##### 删除节点有两个子节点

用后继结点（大于删除结点的最小结点，即右子树的最左节点）替换删除结点



##### 1. 替换节点为红色节点

由于替换结点是红色，删除也了不会影响红黑树的平衡，只要把替换结点的颜色设为删除的结点的颜色即可重新平衡。

##### 2. 替换节点为黑色节点

如下图要删除节点P：

1. 替换节点为R
2. 把替换节点R的兄弟节点S设为黑色
3. 删除节点P设为红色
4. 以删除节点P为支点进行左旋
5. 得到情况

![img](https://upload-images.jianshu.io/upload_images/2392382-1e4c3388491b588f.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

###### 2.1 替换节点是其父节点的左孩子



### 线程不安全问题

1. 多线程环境下扩容导致的死循环

   JDK1.7的HashMap采用头插法插入元素，在多线程环境下可能会导致环形链表的出现，导致死循环。JDK1.8采用尾插法插入元素，解决了这个问题

2. 多线程的put可能会导致元素丢失

   多线程环境下，假设线程AB要插入的key的hash值是一样的（实际上发生了哈希碰撞），但是它们都以为该位置没有元素，可以直接插入而不是去重新hash，就会导致前一个插入的key被后一个覆盖

3. put和get并发时，有可能会导致get返回空（实际上该key对应的值不为空）

   假设线程A尝试获取key对应的值，先hash计算找到了该key对应的存放位置。此时线程B对map进行了resize操作，导致该键值对的位置发生了变化。之后线程A仍然到原来的位置获取值就会得到null



### LinkedHashMap

`LinkedHashMap`是`HashMap`的直接子类，两者唯一的区别是`LinkedHashMap`在`HashMap`的基础上，采用双向链表的形式将所有`entry`连接起来，从而保证了元素的迭代顺序和插入顺序相同，即在`HashMap`的基础上实现了数据有序。



## HashSet

`HashSet`是对`HashMap`的简单包装，利用`HashMap`键不可重复且无需的特性来实现`HashSet`的无序不重复的功能



## WeekHashMap

`WeekHashMap`里的`Entry`可能会被GC自动回收，即使程序员没有手动调用`remove()`或者`clear()`方法

`WeakHashMap`的这个特点特别适用于需要缓存的场景。在缓存场景下，由于内存是有限的，不能缓存所有对象；对象缓存命中可以提高系统效率，但缓存MISS也不会造成错误，因为可以通过计算重新得到。

要明白 `WeakHashMap`的工作原理，还需要引入一个概念 : **弱引用(WeakReference)**。

我们都知道Java中内存是通过GC自动管理的，GC会在程序运行过程中自动判断哪些对象是可以被回收的，并在合适的时机进行内存释放。GC判断某个对象是否可被回收的依据是，**是否有有效的引用指向该对象**。如果没有有效引用指向该对象(基本意味着不存在访问该对象的方式)，那么该对象就是可回收的。这里的**有效引用** 并不包括**弱引用**。也就是说，**虽然弱引用可以用来访问对象，但进行垃圾回收时弱引用并不会被考虑在内，仅有弱引用指向的对象仍然会被GC回收**。

`WeakHashMap` 内部是通过弱引用来管理`entry`的，弱引用的特性对应到 `WeakHashMap` 上意味着什么呢？**将一对`key, value`放入到 `WeakHashMap` 里并不能避免该`key`值被GC回收，除非在WeakHashMap之外还有对该`key`的强引用**。



## ConcurrentHashMap

### 实现原理

#### JDK1.7

JDK1.7中的ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成的。即把原来HashMap中的table（HashEntry数组）分为多个小组，一个小组为一个Segment。

Segment继承了ReentrantLock，即可重入锁。当线程想要访问数据时需要先获取其所在Segment的锁

#### JDK1.8

JDK1.8中取消了Segment数组结构，而仅仅采用跟HashMap一样的基本的数据结构（数组+链表+红黑树），并通过CAS+synchronized（自旋重试64次后升级为synchronized，阻塞获取）的方式实现了更低粒度的加锁，提高了ConcurrentHashMap的并发度



### put

#### JDK1.7

1. 尝试获取Segment的锁，一开始时自旋获取，重试64次后升级为阻塞获取
2. 获取到锁后通过计算hash定位到HashEntry所在位置
3. 遍历HashEntry，如果存在相同的key则覆盖
4. 如果不存在相同的key说明是新增数据，新建一个HashEntry并加入到Segment中，并判断是否需要扩容
5. 释放Segment的锁

#### JDK1.8

1. 根据key计算hash值

2. 判断覆盖还是插入

3. 如果是插入，则定位到Node，拿到头节点head

   - head == null，则通过CAS方式直接加入
   - head.hash = MOVED == -1，说明有其他线程正在扩容，参与一起进行扩容
   - 如果都不满足，则使用synchronized锁住head节点，然后遍历插入
   - 判断插入后是否需要进行变形

   

### get
